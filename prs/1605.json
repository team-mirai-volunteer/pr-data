{
  "basic_info": {
    "url": "https://api.github.com/repos/team-mirai/policy/pulls/1605",
    "id": 2541894235,
    "node_id": "PR_kwDOOqTJvM6Xgjpb",
    "html_url": "https://github.com/team-mirai/policy/pull/1605",
    "diff_url": "https://github.com/team-mirai/policy/pull/1605.diff",
    "patch_url": "https://github.com/team-mirai/policy/pull/1605.patch",
    "issue_url": "https://api.github.com/repos/team-mirai/policy/issues/1605",
    "number": 1605,
    "state": "open",
    "locked": false,
    "title": "Add PR section analyzer script to identify which PRs modify the same sections",
    "user": {
      "login": "devin-ai-integration[bot]",
      "id": 158243242,
      "node_id": "BOT_kgDOCW6Zqg",
      "avatar_url": "https://avatars.githubusercontent.com/in/811515?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D",
      "html_url": "https://github.com/apps/devin-ai-integration",
      "followers_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "# PR Section Analyzer and Education PR Analyzer\n\nThis PR adds scripts to analyze pull requests in the policy repository and extract the specific sections of the manifest that are being modified. The scripts can identify which PRs modify the same sections of the manifest, making it easier to track related changes.\n\n## Features\n\n- Extracts markdown section hierarchies from policy documents\n- Identifies which sections are modified by each PR\n- Groups PRs by the sections they modify\n- Handles Japanese section formatting (ステップ１, ステップ２, etc.)\n- Works without relying on LLM technology\n- Supports both single PR analysis and batch processing\n- Outputs results in text or JSON format\n\n## Implementation Details\n\nThe scripts use regex patterns to identify markdown headings and Japanese section numbering patterns. They build a section hierarchy tree and map line numbers from PR diffs to the corresponding sections.\n\nTwo main scripts are included:\n- `pr_section_analyzer_final.py`: Analyzes any PR to identify modified sections\n- `education_pr_analyzer.py`: Specifically analyzes PRs with the education label (\"教育\")\n\n## Usage\n\n### PR Section Analyzer\n\n```bash\n# Analyze a specific PR\npython pr_section_analyzer_final.py --pr 1533\n\n# Analyze all PRs (limited to 100 by default)\npython pr_section_analyzer_final.py --all\n\n# Analyze all PRs with a custom limit\npython pr_section_analyzer_final.py --all --limit 50\n\n# Output results in JSON format\npython pr_section_analyzer_final.py --pr 1533 --format json\n\n# Save results to a file\npython pr_section_analyzer_final.py --all --output report.txt\n```\n\n### Education PR Analyzer\n\n```bash\n# Analyze education-labeled PRs (limited to 20 by default)\npython education_pr_analyzer.py\n\n# Analyze with a custom limit\npython education_pr_analyzer.py --limit 10\n\n# Save results to a file\npython education_pr_analyzer.py --output education_report.md\n```\n\n## Example Output\n\nThe education PR analyzer generates a comprehensive report showing:\n\n1. Which PRs modify the same sections:\n\n```markdown\n### 11_ステップ１教育.md\n#### ４）貧困世帯の子どもたち・保護者の皆様を支援するため、データとAIを駆使し、プッシュ型の支援を実現します > 現状認識・課題分析\nこのセクションを変更するPR:\n- PR #1462: [SAISにおける科学技術コンテスト参加支援の明記](https://github.com/team-mirai/policy/pull/1462)\n- PR #1460: [教育政策における個別最適化とプライバシー保護の強化（匿名ユーザー提案）](https://github.com/team-mirai/policy/pull/1460)\n```\n\n2. Which sections are modified by each PR:\n\n```markdown\n### PR #1335: 教育政策の改善案：AI家庭教師の対象拡大とプッシュ型支援の強化（すださんご提案）\n- 11_ステップ１教育.md: １．教育 > ビジョン\n- 11_ステップ１教育.md: ４）貧困世帯の子どもたち・保護者の皆様を支援するため、データとAIを駆使し、プッシュ型の支援を実現します > 現状認識・課題分析\n- 11_ステップ１教育.md: １）すべての子どもに「専属のAI家庭教師」を届けます > 現状認識・課題分析\n```\n\nA sample report is included in the PR: `education_pr_report_final.md`\n\n## Link to Devin run\nhttps://app.devin.ai/sessions/4d02a46b92954a4d8978a5f005ddc511\n\nRequested by: NISHIO Hirokazu (nishio.hirokazu@gmail.com)\n",
    "created_at": "2025-05-24T18:45:15Z",
    "updated_at": "2025-05-24T19:54:12Z",
    "closed_at": null,
    "merged_at": null,
    "merge_commit_sha": "ef00c0c9d11aa38e8687f677f2a3753baa1efa4d",
    "assignee": null,
    "assignees": [],
    "requested_reviewers": [],
    "requested_teams": [],
    "labels": [],
    "milestone": null,
    "draft": true,
    "commits_url": "https://api.github.com/repos/team-mirai/policy/pulls/1605/commits",
    "review_comments_url": "https://api.github.com/repos/team-mirai/policy/pulls/1605/comments",
    "review_comment_url": "https://api.github.com/repos/team-mirai/policy/pulls/comments{/number}",
    "comments_url": "https://api.github.com/repos/team-mirai/policy/issues/1605/comments",
    "statuses_url": "https://api.github.com/repos/team-mirai/policy/statuses/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
    "head": {
      "label": "team-mirai:devin/1748111368-pr-section-analyzer",
      "ref": "devin/1748111368-pr-section-analyzer",
      "sha": "1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
      "user": {
        "login": "team-mirai",
        "id": 210232249,
        "node_id": "O_kgDODIfjuQ",
        "avatar_url": "https://avatars.githubusercontent.com/u/210232249?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/team-mirai",
        "html_url": "https://github.com/team-mirai",
        "followers_url": "https://api.github.com/users/team-mirai/followers",
        "following_url": "https://api.github.com/users/team-mirai/following{/other_user}",
        "gists_url": "https://api.github.com/users/team-mirai/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/team-mirai/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/team-mirai/subscriptions",
        "organizations_url": "https://api.github.com/users/team-mirai/orgs",
        "repos_url": "https://api.github.com/users/team-mirai/repos",
        "events_url": "https://api.github.com/users/team-mirai/events{/privacy}",
        "received_events_url": "https://api.github.com/users/team-mirai/received_events",
        "type": "Organization",
        "user_view_type": "public",
        "site_admin": false
      },
      "repo": {
        "id": 983878076,
        "node_id": "R_kgDOOqTJvA",
        "name": "policy",
        "full_name": "team-mirai/policy",
        "private": false,
        "owner": {
          "login": "team-mirai",
          "id": 210232249,
          "node_id": "O_kgDODIfjuQ",
          "avatar_url": "https://avatars.githubusercontent.com/u/210232249?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/team-mirai",
          "html_url": "https://github.com/team-mirai",
          "followers_url": "https://api.github.com/users/team-mirai/followers",
          "following_url": "https://api.github.com/users/team-mirai/following{/other_user}",
          "gists_url": "https://api.github.com/users/team-mirai/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/team-mirai/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/team-mirai/subscriptions",
          "organizations_url": "https://api.github.com/users/team-mirai/orgs",
          "repos_url": "https://api.github.com/users/team-mirai/repos",
          "events_url": "https://api.github.com/users/team-mirai/events{/privacy}",
          "received_events_url": "https://api.github.com/users/team-mirai/received_events",
          "type": "Organization",
          "user_view_type": "public",
          "site_admin": false
        },
        "html_url": "https://github.com/team-mirai/policy",
        "description": null,
        "fork": false,
        "url": "https://api.github.com/repos/team-mirai/policy",
        "forks_url": "https://api.github.com/repos/team-mirai/policy/forks",
        "keys_url": "https://api.github.com/repos/team-mirai/policy/keys{/key_id}",
        "collaborators_url": "https://api.github.com/repos/team-mirai/policy/collaborators{/collaborator}",
        "teams_url": "https://api.github.com/repos/team-mirai/policy/teams",
        "hooks_url": "https://api.github.com/repos/team-mirai/policy/hooks",
        "issue_events_url": "https://api.github.com/repos/team-mirai/policy/issues/events{/number}",
        "events_url": "https://api.github.com/repos/team-mirai/policy/events",
        "assignees_url": "https://api.github.com/repos/team-mirai/policy/assignees{/user}",
        "branches_url": "https://api.github.com/repos/team-mirai/policy/branches{/branch}",
        "tags_url": "https://api.github.com/repos/team-mirai/policy/tags",
        "blobs_url": "https://api.github.com/repos/team-mirai/policy/git/blobs{/sha}",
        "git_tags_url": "https://api.github.com/repos/team-mirai/policy/git/tags{/sha}",
        "git_refs_url": "https://api.github.com/repos/team-mirai/policy/git/refs{/sha}",
        "trees_url": "https://api.github.com/repos/team-mirai/policy/git/trees{/sha}",
        "statuses_url": "https://api.github.com/repos/team-mirai/policy/statuses/{sha}",
        "languages_url": "https://api.github.com/repos/team-mirai/policy/languages",
        "stargazers_url": "https://api.github.com/repos/team-mirai/policy/stargazers",
        "contributors_url": "https://api.github.com/repos/team-mirai/policy/contributors",
        "subscribers_url": "https://api.github.com/repos/team-mirai/policy/subscribers",
        "subscription_url": "https://api.github.com/repos/team-mirai/policy/subscription",
        "commits_url": "https://api.github.com/repos/team-mirai/policy/commits{/sha}",
        "git_commits_url": "https://api.github.com/repos/team-mirai/policy/git/commits{/sha}",
        "comments_url": "https://api.github.com/repos/team-mirai/policy/comments{/number}",
        "issue_comment_url": "https://api.github.com/repos/team-mirai/policy/issues/comments{/number}",
        "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/{+path}",
        "compare_url": "https://api.github.com/repos/team-mirai/policy/compare/{base}...{head}",
        "merges_url": "https://api.github.com/repos/team-mirai/policy/merges",
        "archive_url": "https://api.github.com/repos/team-mirai/policy/{archive_format}{/ref}",
        "downloads_url": "https://api.github.com/repos/team-mirai/policy/downloads",
        "issues_url": "https://api.github.com/repos/team-mirai/policy/issues{/number}",
        "pulls_url": "https://api.github.com/repos/team-mirai/policy/pulls{/number}",
        "milestones_url": "https://api.github.com/repos/team-mirai/policy/milestones{/number}",
        "notifications_url": "https://api.github.com/repos/team-mirai/policy/notifications{?since,all,participating}",
        "labels_url": "https://api.github.com/repos/team-mirai/policy/labels{/name}",
        "releases_url": "https://api.github.com/repos/team-mirai/policy/releases{/id}",
        "deployments_url": "https://api.github.com/repos/team-mirai/policy/deployments",
        "created_at": "2025-05-15T04:00:23Z",
        "updated_at": "2025-05-24T15:59:18Z",
        "pushed_at": "2025-05-24T19:57:49Z",
        "git_url": "git://github.com/team-mirai/policy.git",
        "ssh_url": "git@github.com:team-mirai/policy.git",
        "clone_url": "https://github.com/team-mirai/policy.git",
        "svn_url": "https://github.com/team-mirai/policy",
        "homepage": null,
        "size": 6044,
        "stargazers_count": 129,
        "watchers_count": 129,
        "language": "TypeScript",
        "has_issues": true,
        "has_projects": true,
        "has_downloads": true,
        "has_wiki": true,
        "has_pages": false,
        "has_discussions": false,
        "forks_count": 38,
        "mirror_url": null,
        "archived": false,
        "disabled": false,
        "open_issues_count": 1553,
        "license": {
          "key": "cc-by-4.0",
          "name": "Creative Commons Attribution 4.0 International",
          "spdx_id": "CC-BY-4.0",
          "url": "https://api.github.com/licenses/cc-by-4.0",
          "node_id": "MDc6TGljZW5zZTI1"
        },
        "allow_forking": true,
        "is_template": false,
        "web_commit_signoff_required": false,
        "topics": [],
        "visibility": "public",
        "forks": 38,
        "open_issues": 1553,
        "watchers": 129,
        "default_branch": "main"
      }
    },
    "base": {
      "label": "team-mirai:main",
      "ref": "main",
      "sha": "172c39b3b74735a08df9da768d5042bd12222794",
      "user": {
        "login": "team-mirai",
        "id": 210232249,
        "node_id": "O_kgDODIfjuQ",
        "avatar_url": "https://avatars.githubusercontent.com/u/210232249?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/team-mirai",
        "html_url": "https://github.com/team-mirai",
        "followers_url": "https://api.github.com/users/team-mirai/followers",
        "following_url": "https://api.github.com/users/team-mirai/following{/other_user}",
        "gists_url": "https://api.github.com/users/team-mirai/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/team-mirai/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/team-mirai/subscriptions",
        "organizations_url": "https://api.github.com/users/team-mirai/orgs",
        "repos_url": "https://api.github.com/users/team-mirai/repos",
        "events_url": "https://api.github.com/users/team-mirai/events{/privacy}",
        "received_events_url": "https://api.github.com/users/team-mirai/received_events",
        "type": "Organization",
        "user_view_type": "public",
        "site_admin": false
      },
      "repo": {
        "id": 983878076,
        "node_id": "R_kgDOOqTJvA",
        "name": "policy",
        "full_name": "team-mirai/policy",
        "private": false,
        "owner": {
          "login": "team-mirai",
          "id": 210232249,
          "node_id": "O_kgDODIfjuQ",
          "avatar_url": "https://avatars.githubusercontent.com/u/210232249?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/team-mirai",
          "html_url": "https://github.com/team-mirai",
          "followers_url": "https://api.github.com/users/team-mirai/followers",
          "following_url": "https://api.github.com/users/team-mirai/following{/other_user}",
          "gists_url": "https://api.github.com/users/team-mirai/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/team-mirai/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/team-mirai/subscriptions",
          "organizations_url": "https://api.github.com/users/team-mirai/orgs",
          "repos_url": "https://api.github.com/users/team-mirai/repos",
          "events_url": "https://api.github.com/users/team-mirai/events{/privacy}",
          "received_events_url": "https://api.github.com/users/team-mirai/received_events",
          "type": "Organization",
          "user_view_type": "public",
          "site_admin": false
        },
        "html_url": "https://github.com/team-mirai/policy",
        "description": null,
        "fork": false,
        "url": "https://api.github.com/repos/team-mirai/policy",
        "forks_url": "https://api.github.com/repos/team-mirai/policy/forks",
        "keys_url": "https://api.github.com/repos/team-mirai/policy/keys{/key_id}",
        "collaborators_url": "https://api.github.com/repos/team-mirai/policy/collaborators{/collaborator}",
        "teams_url": "https://api.github.com/repos/team-mirai/policy/teams",
        "hooks_url": "https://api.github.com/repos/team-mirai/policy/hooks",
        "issue_events_url": "https://api.github.com/repos/team-mirai/policy/issues/events{/number}",
        "events_url": "https://api.github.com/repos/team-mirai/policy/events",
        "assignees_url": "https://api.github.com/repos/team-mirai/policy/assignees{/user}",
        "branches_url": "https://api.github.com/repos/team-mirai/policy/branches{/branch}",
        "tags_url": "https://api.github.com/repos/team-mirai/policy/tags",
        "blobs_url": "https://api.github.com/repos/team-mirai/policy/git/blobs{/sha}",
        "git_tags_url": "https://api.github.com/repos/team-mirai/policy/git/tags{/sha}",
        "git_refs_url": "https://api.github.com/repos/team-mirai/policy/git/refs{/sha}",
        "trees_url": "https://api.github.com/repos/team-mirai/policy/git/trees{/sha}",
        "statuses_url": "https://api.github.com/repos/team-mirai/policy/statuses/{sha}",
        "languages_url": "https://api.github.com/repos/team-mirai/policy/languages",
        "stargazers_url": "https://api.github.com/repos/team-mirai/policy/stargazers",
        "contributors_url": "https://api.github.com/repos/team-mirai/policy/contributors",
        "subscribers_url": "https://api.github.com/repos/team-mirai/policy/subscribers",
        "subscription_url": "https://api.github.com/repos/team-mirai/policy/subscription",
        "commits_url": "https://api.github.com/repos/team-mirai/policy/commits{/sha}",
        "git_commits_url": "https://api.github.com/repos/team-mirai/policy/git/commits{/sha}",
        "comments_url": "https://api.github.com/repos/team-mirai/policy/comments{/number}",
        "issue_comment_url": "https://api.github.com/repos/team-mirai/policy/issues/comments{/number}",
        "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/{+path}",
        "compare_url": "https://api.github.com/repos/team-mirai/policy/compare/{base}...{head}",
        "merges_url": "https://api.github.com/repos/team-mirai/policy/merges",
        "archive_url": "https://api.github.com/repos/team-mirai/policy/{archive_format}{/ref}",
        "downloads_url": "https://api.github.com/repos/team-mirai/policy/downloads",
        "issues_url": "https://api.github.com/repos/team-mirai/policy/issues{/number}",
        "pulls_url": "https://api.github.com/repos/team-mirai/policy/pulls{/number}",
        "milestones_url": "https://api.github.com/repos/team-mirai/policy/milestones{/number}",
        "notifications_url": "https://api.github.com/repos/team-mirai/policy/notifications{?since,all,participating}",
        "labels_url": "https://api.github.com/repos/team-mirai/policy/labels{/name}",
        "releases_url": "https://api.github.com/repos/team-mirai/policy/releases{/id}",
        "deployments_url": "https://api.github.com/repos/team-mirai/policy/deployments",
        "created_at": "2025-05-15T04:00:23Z",
        "updated_at": "2025-05-24T15:59:18Z",
        "pushed_at": "2025-05-24T19:57:49Z",
        "git_url": "git://github.com/team-mirai/policy.git",
        "ssh_url": "git@github.com:team-mirai/policy.git",
        "clone_url": "https://github.com/team-mirai/policy.git",
        "svn_url": "https://github.com/team-mirai/policy",
        "homepage": null,
        "size": 6044,
        "stargazers_count": 129,
        "watchers_count": 129,
        "language": "TypeScript",
        "has_issues": true,
        "has_projects": true,
        "has_downloads": true,
        "has_wiki": true,
        "has_pages": false,
        "has_discussions": false,
        "forks_count": 38,
        "mirror_url": null,
        "archived": false,
        "disabled": false,
        "open_issues_count": 1553,
        "license": {
          "key": "cc-by-4.0",
          "name": "Creative Commons Attribution 4.0 International",
          "spdx_id": "CC-BY-4.0",
          "url": "https://api.github.com/licenses/cc-by-4.0",
          "node_id": "MDc6TGljZW5zZTI1"
        },
        "allow_forking": true,
        "is_template": false,
        "web_commit_signoff_required": false,
        "topics": [],
        "visibility": "public",
        "forks": 38,
        "open_issues": 1553,
        "watchers": 129,
        "default_branch": "main"
      }
    },
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/team-mirai/policy/pulls/1605"
      },
      "html": {
        "href": "https://github.com/team-mirai/policy/pull/1605"
      },
      "issue": {
        "href": "https://api.github.com/repos/team-mirai/policy/issues/1605"
      },
      "comments": {
        "href": "https://api.github.com/repos/team-mirai/policy/issues/1605/comments"
      },
      "review_comments": {
        "href": "https://api.github.com/repos/team-mirai/policy/pulls/1605/comments"
      },
      "review_comment": {
        "href": "https://api.github.com/repos/team-mirai/policy/pulls/comments{/number}"
      },
      "commits": {
        "href": "https://api.github.com/repos/team-mirai/policy/pulls/1605/commits"
      },
      "statuses": {
        "href": "https://api.github.com/repos/team-mirai/policy/statuses/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35"
      }
    },
    "author_association": "CONTRIBUTOR",
    "auto_merge": null,
    "active_lock_reason": null,
    "merged": false,
    "mergeable": true,
    "rebaseable": true,
    "mergeable_state": "clean",
    "merged_by": null,
    "comments": 2,
    "review_comments": 0,
    "maintainer_can_modify": false,
    "commits": 4,
    "additions": 2047,
    "deletions": 0,
    "changed_files": 6
  },
  "state": "open",
  "updated_at": "2025-05-24T19:54:12Z",
  "comments": [
    {
      "url": "https://api.github.com/repos/team-mirai/policy/issues/comments/2906972687",
      "html_url": "https://github.com/team-mirai/policy/pull/1605#issuecomment-2906972687",
      "issue_url": "https://api.github.com/repos/team-mirai/policy/issues/1605",
      "id": 2906972687,
      "node_id": "IC_kwDOOqTJvM6tROIP",
      "user": {
        "login": "devin-ai-integration[bot]",
        "id": 158243242,
        "node_id": "BOT_kgDOCW6Zqg",
        "avatar_url": "https://avatars.githubusercontent.com/in/811515?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D",
        "html_url": "https://github.com/apps/devin-ai-integration",
        "followers_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/followers",
        "following_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/following{/other_user}",
        "gists_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/subscriptions",
        "organizations_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/orgs",
        "repos_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/repos",
        "events_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/events{/privacy}",
        "received_events_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/received_events",
        "type": "Bot",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2025-05-24T18:45:19Z",
      "updated_at": "2025-05-24T18:45:19Z",
      "author_association": "CONTRIBUTOR",
      "body": "### 🤖 Devin AI Engineer\n\nI'll be helping with this pull request! Here's what you should know:\n\n✅ I will automatically:\n- Address comments on this PR. Add '(aside)' to your comment to have me ignore it.\n- Look at CI failures and help fix them\n\nNote: I can only respond to comments from users who have write access to this repository.\n\n⚙️ Control Options:\n- [ ] Disable automatic comment and CI monitoring\n",
      "reactions": {
        "url": "https://api.github.com/repos/team-mirai/policy/issues/comments/2906972687/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": {
        "id": 811515,
        "client_id": "Iv1.fffb955bc006997f",
        "slug": "devin-ai-integration",
        "node_id": "A_kwHOCQk3ds4ADGH7",
        "owner": {
          "login": "usacognition",
          "id": 151598966,
          "node_id": "O_kgDOCQk3dg",
          "avatar_url": "https://avatars.githubusercontent.com/u/151598966?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/usacognition",
          "html_url": "https://github.com/usacognition",
          "followers_url": "https://api.github.com/users/usacognition/followers",
          "following_url": "https://api.github.com/users/usacognition/following{/other_user}",
          "gists_url": "https://api.github.com/users/usacognition/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/usacognition/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/usacognition/subscriptions",
          "organizations_url": "https://api.github.com/users/usacognition/orgs",
          "repos_url": "https://api.github.com/users/usacognition/repos",
          "events_url": "https://api.github.com/users/usacognition/events{/privacy}",
          "received_events_url": "https://api.github.com/users/usacognition/received_events",
          "type": "Organization",
          "user_view_type": "public",
          "site_admin": false
        },
        "name": "Devin.ai Integration",
        "description": "This Github App integration allows Devin (devin.ai) to have limited authenticated access to your organization's code repositories.",
        "external_url": "https://devin.ai",
        "html_url": "https://github.com/apps/devin-ai-integration",
        "created_at": "2024-01-31T03:16:04Z",
        "updated_at": "2025-05-14T06:48:07Z",
        "permissions": {
          "actions": "read",
          "checks": "read",
          "contents": "write",
          "deployments": "read",
          "discussions": "write",
          "emails": "read",
          "issues": "write",
          "members": "read",
          "metadata": "read",
          "packages": "read",
          "pages": "read",
          "pull_requests": "write",
          "repository_advisories": "read",
          "repository_hooks": "read",
          "repository_projects": "read",
          "statuses": "read",
          "vulnerability_alerts": "read",
          "workflows": "write"
        },
        "events": [
          "check_run",
          "issue_comment",
          "pull_request",
          "pull_request_review",
          "pull_request_review_comment",
          "repository",
          "status"
        ]
      }
    },
    {
      "url": "https://api.github.com/repos/team-mirai/policy/issues/comments/2906997703",
      "html_url": "https://github.com/team-mirai/policy/pull/1605#issuecomment-2906997703",
      "issue_url": "https://api.github.com/repos/team-mirai/policy/issues/1605",
      "id": 2906997703,
      "node_id": "IC_kwDOOqTJvM6tRUPH",
      "user": {
        "login": "nishio",
        "id": 315198,
        "node_id": "MDQ6VXNlcjMxNTE5OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/315198?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/nishio",
        "html_url": "https://github.com/nishio",
        "followers_url": "https://api.github.com/users/nishio/followers",
        "following_url": "https://api.github.com/users/nishio/following{/other_user}",
        "gists_url": "https://api.github.com/users/nishio/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/nishio/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/nishio/subscriptions",
        "organizations_url": "https://api.github.com/users/nishio/orgs",
        "repos_url": "https://api.github.com/users/nishio/repos",
        "events_url": "https://api.github.com/users/nishio/events{/privacy}",
        "received_events_url": "https://api.github.com/users/nishio/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2025-05-24T19:54:12Z",
      "updated_at": "2025-05-24T19:54:12Z",
      "author_association": "NONE",
      "body": "これは後でrandom repoに移動します",
      "reactions": {
        "url": "https://api.github.com/repos/team-mirai/policy/issues/comments/2906997703/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "performed_via_github_app": null
    }
  ],
  "review_comments": [],
  "files": [
    {
      "sha": "85c4313def35c635569fe7fd0e331fc7d0eaf486",
      "filename": "education_pr_analyzer.py",
      "status": "added",
      "additions": 373,
      "deletions": 0,
      "changes": 373,
      "blob_url": "https://github.com/team-mirai/policy/blob/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/education_pr_analyzer.py",
      "raw_url": "https://github.com/team-mirai/policy/raw/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/education_pr_analyzer.py",
      "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/education_pr_analyzer.py?ref=1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
      "patch": "@@ -0,0 +1,373 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Script to analyze PRs with the education label and generate a Markdown report\n+of which sections are modified by each PR.\n+\"\"\"\n+\n+import argparse\n+import json\n+import os\n+import re\n+import subprocess\n+import sys\n+from collections import defaultdict\n+from datetime import datetime\n+\n+def run_command(command):\n+    \"\"\"Run a shell command and return the output.\"\"\"\n+    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n+    if result.returncode != 0:\n+        print(f\"Error running command: {command}\")\n+        print(f\"Error: {result.stderr}\")\n+        return \"\"\n+    return result.stdout.strip()\n+\n+def get_education_prs():\n+    \"\"\"Get all PRs with the education label.\"\"\"\n+    open_prs_cmd = 'gh pr list --label \"教育\" --state open --json number,title,url'\n+    open_prs_output = run_command(open_prs_cmd)\n+    \n+    closed_prs_cmd = 'gh pr list --label \"教育\" --state closed --json number,title,url'\n+    closed_prs_output = run_command(closed_prs_cmd)\n+    \n+    open_prs = json.loads(open_prs_output) if open_prs_output else []\n+    closed_prs = json.loads(closed_prs_output) if closed_prs_output else []\n+    \n+    all_prs = open_prs + closed_prs\n+    \n+    return all_prs\n+\n+def extract_markdown_sections(file_path):\n+    \"\"\"Extract markdown sections from a file with improved Japanese section handling.\"\"\"\n+    if not os.path.exists(file_path):\n+        print(f\"File not found: {file_path}\")\n+        return {}\n+    \n+    with open(file_path, 'r', encoding='utf-8') as f:\n+        content = f.read()\n+    \n+    print(f\"Extracting sections from file with {len(content.split('\\n'))} lines\")\n+    \n+    headings = []\n+    section_content = {}\n+    current_section_start = None\n+    \n+    for i, line in enumerate(content.split('\\n')):\n+        heading_match = re.match(r'^(#+)\\s+(.+)$', line)\n+        \n+        jp_section_match = None\n+        if not heading_match:\n+            jp_section_match = re.match(r'^([０-９]+）|\\d+）)\\s*(.+)$', line)\n+        \n+        jp_policy_match = None\n+        if not heading_match and not jp_section_match:\n+            jp_policy_match = re.match(r'^###\\s+(現状認識・課題分析|政策概要)$', line)\n+        \n+        if heading_match:\n+            level = len(heading_match.group(1))\n+            title = heading_match.group(2).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found heading at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_section_match:\n+            level = 2\n+            title = jp_section_match.group(0).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_policy_match:\n+            level = 3\n+            title = jp_policy_match.group(1).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP policy section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+    \n+    if current_section_start is not None and len(content.split('\\n')) > 0:\n+        section_content[current_section_start] = (current_section_start, len(content.split('\\n')))\n+    \n+    sections = {}\n+    \n+    sorted_headings = sorted(headings, key=lambda x: x[0])\n+    \n+    for i, (line_num, level, title) in enumerate(sorted_headings):\n+        parent_sections = []\n+        current_parent_level = 0\n+        \n+        for j in range(i-1, -1, -1):\n+            prev_line, prev_level, prev_title = sorted_headings[j]\n+            \n+            if prev_level < level and prev_level > current_parent_level:\n+                parent_sections.insert(0, prev_title)\n+                current_parent_level = prev_level\n+                \n+                if prev_level == 1:\n+                    break\n+        \n+        section_path = \" > \".join(parent_sections + [title])\n+        \n+        content_range = section_content.get(line_num, (line_num, line_num))\n+        \n+        sections[line_num] = {\n+            \"level\": level,\n+            \"title\": title,\n+            \"path\": section_path,\n+            \"parent_sections\": parent_sections,\n+            \"start_line\": content_range[0],\n+            \"end_line\": content_range[1]\n+        }\n+    \n+    return sections\n+\n+def find_section_for_line(sections, line_number):\n+    \"\"\"Find the section that contains a specific line.\"\"\"\n+    for section_line, section_info in sections.items():\n+        if section_info[\"start_line\"] <= line_number <= section_info[\"end_line\"]:\n+            return section_info\n+    \n+    section_lines = sorted(sections.keys())\n+    for i, section_line in enumerate(section_lines):\n+        if section_line > line_number:\n+            if i > 0:\n+                return sections[section_lines[i-1]]\n+            break\n+    \n+    if section_lines:\n+        return sections[section_lines[-1]]\n+    \n+    return None\n+\n+def get_file_diff(pr_number, file_path):\n+    \"\"\"Get the diff for a specific file in a PR using git commands.\"\"\"\n+    command = f\"gh pr view {pr_number} --json headRefName\"\n+    output = run_command(command)\n+    try:\n+        data = json.loads(output)\n+        branch_name = data.get('headRefName')\n+        if not branch_name:\n+            print(f\"Could not get branch name for PR #{pr_number}\")\n+            return \"\"\n+        \n+        fetch_cmd = f\"git fetch origin {branch_name}\"\n+        run_command(fetch_cmd)\n+        \n+        diff_cmd = f\"git diff main..origin/{branch_name} -- '{file_path}'\"\n+        return run_command(diff_cmd)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON for PR branch: {output}\")\n+        return \"\"\n+\n+def extract_line_numbers_from_diff(diff_text):\n+    \"\"\"Extract line numbers from a diff.\"\"\"\n+    line_numbers = []\n+    current_line = None\n+    \n+    for line in diff_text.split('\\n'):\n+        if line.startswith('@@'):\n+            match = re.search(r'@@ -\\d+(?:,\\d+)? \\+(\\d+)(?:,(\\d+))? @@', line)\n+            if match:\n+                current_line = int(match.group(1))\n+                line_count = int(match.group(2)) if match.group(2) else 1\n+        elif current_line is not None:\n+            if line.startswith('+') and not line.startswith('+++'):\n+                line_numbers.append(current_line)\n+                current_line += 1\n+            elif line.startswith('-'):\n+                pass\n+            else:\n+                current_line += 1\n+    \n+    return line_numbers\n+\n+def get_pr_details(pr_number):\n+    \"\"\"Get details for a specific PR.\"\"\"\n+    cmd = f'gh pr view {pr_number} --json number,title,url,body'\n+    output = run_command(cmd)\n+    if not output:\n+        return None\n+    \n+    pr_data = json.loads(output)\n+    return pr_data\n+\n+def get_pr_files(pr_number):\n+    \"\"\"Get the files changed in a PR.\"\"\"\n+    cmd = f'gh pr view {pr_number} --json files'\n+    output = run_command(cmd)\n+    if not output:\n+        return []\n+    \n+    pr_data = json.loads(output)\n+    return [file['path'] for file in pr_data.get('files', [])]\n+\n+def analyze_pr(pr_number):\n+    \"\"\"Analyze a PR and extract the sections it modifies.\"\"\"\n+    pr_details = get_pr_details(pr_number)\n+    if not pr_details:\n+        print(f\"Failed to get details for PR #{pr_number}\")\n+        return None\n+    \n+    pr_files = get_pr_files(pr_number)\n+    if not pr_files:\n+        print(f\"No files found for PR #{pr_number}\")\n+        return None\n+    \n+    education_files = [f for f in pr_files if f.endswith('.md') and (\n+        f.startswith('11_ステップ１教育') or \n+        f.startswith('21_ステップ２教育') or \n+        f.startswith('32_ステップ３教育')\n+    )]\n+    \n+    if not education_files:\n+        print(f\"No education files found for PR #{pr_number}\")\n+        return None\n+    \n+    results = []\n+    \n+    for file_path in education_files:\n+        print(f\"Processing file: {file_path}\")\n+        full_path = os.path.join(os.getcwd(), file_path)\n+        \n+        if not os.path.exists(full_path):\n+            print(f\"Warning: File {full_path} does not exist, skipping\")\n+            continue\n+        \n+        file_diff = get_file_diff(pr_number, file_path)\n+        \n+        if not file_diff:\n+            print(f\"No diff found for file {file_path}\")\n+            continue\n+        \n+        print(f\"Found diff for file {file_path}, length: {len(file_diff)}\")\n+        \n+        line_numbers = extract_line_numbers_from_diff(file_diff)\n+        \n+        if not line_numbers:\n+            print(f\"No line numbers found in diff for file {file_path}\")\n+            continue\n+        \n+        print(f\"Affected line numbers: {line_numbers[:10]}...\")\n+        \n+        sections = extract_markdown_sections(full_path)\n+        print(f\"Found {len(sections)} sections in {file_path}\")\n+        \n+        if len(sections) > 0:\n+            print(f\"Section titles: {[s['title'] for s in sections.values()][:5]}\")\n+        \n+        affected_sections = set()\n+        for line_number in line_numbers:\n+            section = find_section_for_line(sections, line_number)\n+            if section:\n+                print(f\"Line {line_number} is in section: {section['title']}\")\n+                affected_sections.add((section['title'], section['path']))\n+        \n+        for section_title, section_path in affected_sections:\n+            results.append({\n+                'pr_number': pr_number,\n+                'pr_title': pr_details.get('title', ''),\n+                'pr_url': pr_details.get('url', ''),\n+                'file': file_path,\n+                'section': section_title,\n+                'section_path': section_path,\n+                'changes': [line for line in file_diff.split('\\n') if line.startswith('+') and not line.startswith('+++')]\n+            })\n+    \n+    print(f\"Found {len(results)} affected sections\")\n+    return results\n+\n+def generate_markdown_report(pr_analyses):\n+    \"\"\"Generate a Markdown report from the PR analyses.\"\"\"\n+    if not pr_analyses:\n+        return \"# 教育関連PRの分析\\n\\n分析対象のPRが見つかりませんでした。\"\n+    \n+    sections_to_prs = defaultdict(list)\n+    for pr in pr_analyses:\n+        for result in pr.get(\"results\", []):\n+            section_key = f\"{result['file']}: {result['section_path']}\"\n+            sections_to_prs[section_key].append({\n+                \"number\": result['pr_number'],\n+                \"title\": result['pr_title'],\n+                \"url\": result['pr_url']\n+            })\n+    \n+    report = \"# 教育関連PRの分析\\n\\n\"\n+    report += f\"分析日時: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n+    \n+    report += \"## セクション別PR一覧\\n\\n\"\n+    for section_key, prs in sections_to_prs.items():\n+        if len(prs) > 1:  # Only show sections with multiple PRs\n+            file_path, section_path = section_key.split(\": \", 1)\n+            report += f\"### {file_path}\\n\"\n+            report += f\"#### {section_path}\\n\"\n+            report += \"このセクションを変更するPR:\\n\"\n+            for pr in prs:\n+                report += f\"- PR #{pr['number']}: [{pr['title']}]({pr['url']})\\n\"\n+            report += \"\\n\"\n+    \n+    report += \"## PR別変更セクション一覧\\n\\n\"\n+    pr_to_sections = defaultdict(list)\n+    for pr in pr_analyses:\n+        for result in pr.get(\"results\", []):\n+            pr_key = f\"{result['pr_number']}:{result['pr_title']}\"\n+            section_info = {\n+                'file': result['file'],\n+                'section_path': result['section_path']\n+            }\n+            pr_to_sections[pr_key].append(section_info)\n+    \n+    for pr_key, sections in sorted(pr_to_sections.items()):\n+        pr_number, pr_title = pr_key.split(\":\", 1)\n+        report += f\"### PR #{pr_number}: {pr_title}\\n\"\n+        \n+        for section in sections:\n+            report += f\"- {section['file']}: {section['section_path']}\\n\"\n+        \n+        report += \"\\n\"\n+    \n+    return report\n+\n+def main():\n+    parser = argparse.ArgumentParser(description=\"Analyze PRs with the education label\")\n+    parser.add_argument(\"--output\", help=\"Output file for the Markdown report\")\n+    parser.add_argument(\"--limit\", type=int, default=20, help=\"Limit the number of PRs to analyze\")\n+    args = parser.parse_args()\n+    \n+    education_prs = get_education_prs()\n+    print(f\"Found {len(education_prs)} PRs with the education label\")\n+    \n+    education_prs = education_prs[:args.limit]\n+    \n+    pr_analyses = []\n+    for pr in education_prs:\n+        pr_number = pr[\"number\"]\n+        print(f\"\\nAnalyzing PR #{pr_number}: {pr['title']}\")\n+        results = analyze_pr(pr_number)\n+        if results:\n+            pr_analyses.append({\n+                \"pr_number\": pr_number,\n+                \"pr_title\": pr['title'],\n+                \"pr_url\": pr['url'],\n+                \"results\": results\n+            })\n+    \n+    report = generate_markdown_report(pr_analyses)\n+    \n+    if args.output:\n+        with open(args.output, \"w\", encoding=\"utf-8\") as f:\n+            f.write(report)\n+        print(f\"Report written to {args.output}\")\n+    else:\n+        print(\"\\n\" + \"=\"*80 + \"\\n\")\n+        print(report)\n+\n+if __name__ == \"__main__\":\n+    main()"
    },
    {
      "sha": "1c16ea171d0118085d39a0f00f0c57dc549979af",
      "filename": "education_pr_report_final.md",
      "status": "added",
      "additions": 88,
      "deletions": 0,
      "changes": 88,
      "blob_url": "https://github.com/team-mirai/policy/blob/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/education_pr_report_final.md",
      "raw_url": "https://github.com/team-mirai/policy/raw/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/education_pr_report_final.md",
      "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/education_pr_report_final.md?ref=1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
      "patch": "@@ -0,0 +1,88 @@\n+# 教育関連PRの分析\n+\n+分析日時: 2025-05-24 19:13:06\n+\n+## セクション別PR一覧\n+\n+### 32_ステップ３教育.md\n+#### ３）子どもたちの好奇心と「はじめる力」を育むための教育に投資します > 現状認識・課題分析\n+このセクションを変更するPR:\n+- PR #1475: [政策案「２．教育」への子どもの権利尊重に関する新規提案 (IR)](https://github.com/team-mirai/policy/pull/1475)\n+- PR #1307: [公設民営型施設の事例記述を最新情報に基づき更新](https://github.com/team-mirai/policy/pull/1307)\n+\n+### 11_ステップ１教育.md\n+#### ４）貧困世帯の子どもたち・保護者の皆様を支援するため、データとAIを駆使し、プッシュ型の支援を実現します > 現状認識・課題分析\n+このセクションを変更するPR:\n+- PR #1462: [SAISにおける科学技術コンテスト参加支援の明記](https://github.com/team-mirai/policy/pull/1462)\n+- PR #1460: [教育政策における個別最適化とプライバシー保護の強化（匿名ユーザー提案）](https://github.com/team-mirai/policy/pull/1460)\n+- PR #1431: [教育方針改善：専科制導入と教員の役割分担によるAI活用促進と働き方改革](https://github.com/team-mirai/policy/pull/1431)\n+- PR #1385: [mayabreaさん提案：小学校における生成AI利用の安全技術要件を強化](https://github.com/team-mirai/policy/pull/1385)\n+- PR #1383: [教育政策案の改善：既存教育アプリの迅速な導入による学習機会の拡充（Himawaruwaruさん提案）](https://github.com/team-mirai/policy/pull/1383)\n+- PR #1338: [教員の働き方改革に関する記述を更新（千葉さん提案）](https://github.com/team-mirai/policy/pull/1338)\n+- PR #1335: [教育政策の改善案：AI家庭教師の対象拡大とプッシュ型支援の強化（すださんご提案）](https://github.com/team-mirai/policy/pull/1335)\n+- PR #1316: [ムライ氏提案：実践的スキル育成と社会ニーズを反映した教育へのAI活用強化](https://github.com/team-mirai/policy/pull/1316)\n+\n+### 11_ステップ１教育.md\n+#### １．教育 > ビジョン\n+このセクションを変更するPR:\n+- PR #1460: [教育政策における個別最適化とプライバシー保護の強化（匿名ユーザー提案）](https://github.com/team-mirai/policy/pull/1460)\n+- PR #1335: [教育政策の改善案：AI家庭教師の対象拡大とプッシュ型支援の強化（すださんご提案）](https://github.com/team-mirai/policy/pull/1335)\n+- PR #1316: [ムライ氏提案：実践的スキル育成と社会ニーズを反映した教育へのAI活用強化](https://github.com/team-mirai/policy/pull/1316)\n+\n+### 11_ステップ１教育.md\n+#### １）すべての子どもに「専属のAI家庭教師」を届けます > 現状認識・課題分析\n+このセクションを変更するPR:\n+- PR #1431: [教育方針改善：専科制導入と教員の役割分担によるAI活用促進と働き方改革](https://github.com/team-mirai/policy/pull/1431)\n+- PR #1383: [教育政策案の改善：既存教育アプリの迅速な導入による学習機会の拡充（Himawaruwaruさん提案）](https://github.com/team-mirai/policy/pull/1383)\n+- PR #1335: [教育政策の改善案：AI家庭教師の対象拡大とプッシュ型支援の強化（すださんご提案）](https://github.com/team-mirai/policy/pull/1335)\n+\n+### 11_ステップ１教育.md\n+#### ３）：AI＆ITによる教員の働き方改革を進める > 現状認識・課題分析\n+このセクションを変更するPR:\n+- PR #1431: [教育方針改善：専科制導入と教員の役割分担によるAI活用促進と働き方改革](https://github.com/team-mirai/policy/pull/1431)\n+- PR #1338: [教員の働き方改革に関する記述を更新（千葉さん提案）](https://github.com/team-mirai/policy/pull/1338)\n+\n+## PR別変更セクション一覧\n+\n+### PR #1307: 公設民営型施設の事例記述を最新情報に基づき更新\n+- 32_ステップ３教育.md: ３）子どもたちの好奇心と「はじめる力」を育むための教育に投資します > 現状認識・課題分析\n+\n+### PR #1316: ムライ氏提案：実践的スキル育成と社会ニーズを反映した教育へのAI活用強化\n+- 11_ステップ１教育.md: １．教育 > ビジョン\n+- 11_ステップ１教育.md: ４）貧困世帯の子どもたち・保護者の皆様を支援するため、データとAIを駆使し、プッシュ型の支援を実現します > 現状認識・課題分析\n+\n+### PR #1335: 教育政策の改善案：AI家庭教師の対象拡大とプッシュ型支援の強化（すださんご提案）\n+- 11_ステップ１教育.md: １．教育 > ビジョン\n+- 11_ステップ１教育.md: ４）貧困世帯の子どもたち・保護者の皆様を支援するため、データとAIを駆使し、プッシュ型の支援を実現します > 現状認識・課題分析\n+- 11_ステップ１教育.md: １）すべての子どもに「専属のAI家庭教師」を届けます > 現状認識・課題分析\n+- 11_ステップ１教育.md: １．教育 > １）すべての子どもに「専属のAI家庭教師」を届けます\n+\n+### PR #1338: 教員の働き方改革に関する記述を更新（千葉さん提案）\n+- 11_ステップ１教育.md: ４）貧困世帯の子どもたち・保護者の皆様を支援するため、データとAIを駆使し、プッシュ型の支援を実現します > 現状認識・課題分析\n+- 11_ステップ１教育.md: ３）：AI＆ITによる教員の働き方改革を進める > 現状認識・課題分析\n+\n+### PR #1383: 教育政策案の改善：既存教育アプリの迅速な導入による学習機会の拡充（Himawaruwaruさん提案）\n+- 11_ステップ１教育.md: １）すべての子どもに「専属のAI家庭教師」を届けます > 現状認識・課題分析\n+- 11_ステップ１教育.md: ４）貧困世帯の子どもたち・保護者の皆様を支援するため、データとAIを駆使し、プッシュ型の支援を実現します > 現状認識・課題分析\n+\n+### PR #1385: mayabreaさん提案：小学校における生成AI利用の安全技術要件を強化\n+- 11_ステップ１教育.md: ２）子どものAIリテラシーを育み、AIと共生する未来を切り拓きます > 現状認識・課題分析\n+- 11_ステップ１教育.md: ４）貧困世帯の子どもたち・保護者の皆様を支援するため、データとAIを駆使し、プッシュ型の支援を実現します > 現状認識・課題分析\n+\n+### PR #1431: 教育方針改善：専科制導入と教員の役割分担によるAI活用促進と働き方改革\n+- 11_ステップ１教育.md: １）すべての子どもに「専属のAI家庭教師」を届けます > 現状認識・課題分析\n+- 11_ステップ１教育.md: ３）：AI＆ITによる教員の働き方改革を進める > 現状認識・課題分析\n+- 11_ステップ１教育.md: ４）貧困世帯の子どもたち・保護者の皆様を支援するため、データとAIを駆使し、プッシュ型の支援を実現します > 現状認識・課題分析\n+\n+### PR #1460: 教育政策における個別最適化とプライバシー保護の強化（匿名ユーザー提案）\n+- 11_ステップ１教育.md: １．教育 > ビジョン\n+- 11_ステップ１教育.md: ４）貧困世帯の子どもたち・保護者の皆様を支援するため、データとAIを駆使し、プッシュ型の支援を実現します > 現状認識・課題分析\n+\n+### PR #1462: SAISにおける科学技術コンテスト参加支援の明記\n+- 11_ステップ１教育.md: ４）貧困世帯の子どもたち・保護者の皆様を支援するため、データとAIを駆使し、プッシュ型の支援を実現します > 現状認識・課題分析\n+- 11_ステップ１教育.md: １．教育 > ３）：AI＆ITによる教員の働き方改革を進める\n+\n+### PR #1475: 政策案「２．教育」への子どもの権利尊重に関する新規提案 (IR)\n+- 32_ステップ３教育.md: ３）子どもたちの好奇心と「はじめる力」を育むための教育に投資します > 現状認識・課題分析\n+- 32_ステップ３教育.md: １）政府全体で、教育への投資予算を確保します > 現状認識・課題分析\n+"
    },
    {
      "sha": "c98dacfacf04cbebb19abf181a0f40a44efd24f7",
      "filename": "pr_section_analyzer.py",
      "status": "added",
      "additions": 420,
      "deletions": 0,
      "changes": 420,
      "blob_url": "https://github.com/team-mirai/policy/blob/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer.py",
      "raw_url": "https://github.com/team-mirai/policy/raw/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer.py",
      "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/pr_section_analyzer.py?ref=1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
      "patch": "@@ -0,0 +1,420 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Script to analyze PRs in the policy repository and extract the specific sections \n+of the manifest that are being modified. This script can identify which PRs modify\n+the same sections of the manifest.\n+\"\"\"\n+\n+import json\n+import os\n+import re\n+import subprocess\n+import sys\n+import argparse\n+from collections import defaultdict\n+\n+def run_command(command):\n+    \"\"\"Run a shell command and return the output.\"\"\"\n+    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n+    if result.returncode != 0:\n+        print(f\"Error running command: {command}\")\n+        print(f\"Error: {result.stderr}\")\n+        return \"\"\n+    return result.stdout.strip()\n+\n+def get_pr_list(limit=100):\n+    \"\"\"Get a list of PRs from the repository.\"\"\"\n+    command = f\"gh pr list --limit {limit} --json number,title,headRefName,state,url\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR list: {output}\")\n+        return []\n+\n+def get_pr_details(pr_number):\n+    \"\"\"Get detailed information about a PR.\"\"\"\n+    command = f\"gh pr view {pr_number} --json number,title,headRefName,state,url,body\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR details: {output}\")\n+        return {}\n+\n+def get_pr_files(pr_number):\n+    \"\"\"Get the list of files changed in a PR.\"\"\"\n+    command = f\"gh pr view {pr_number} --json files\"\n+    output = run_command(command)\n+    try:\n+        data = json.loads(output)\n+        return data.get(\"files\", [])\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR files: {output}\")\n+        return []\n+\n+def get_pr_diff(pr_number):\n+    \"\"\"Get the diff for a PR.\"\"\"\n+    command = f\"git fetch origin pull/{pr_number}/head:pr-{pr_number}-temp && git diff main..pr-{pr_number}-temp\"\n+    diff = run_command(command)\n+    if not diff:\n+        print(f\"DEBUG: Failed to get diff using git fetch/diff, trying alternative method\")\n+        command = f\"git fetch && git diff origin/main...origin/pr-{pr_number}-head\"\n+        diff = run_command(command)\n+    return diff\n+\n+def extract_markdown_sections(file_path):\n+    \"\"\"Extract the markdown sections from a file with improved Japanese section handling.\"\"\"\n+    if not os.path.exists(file_path):\n+        print(f\"File not found: {file_path}\")\n+        return {}\n+    \n+    with open(file_path, 'r', encoding='utf-8') as f:\n+        content = f.read()\n+    \n+    headings = []\n+    section_content = {}\n+    current_section_start = None\n+    \n+    for i, line in enumerate(content.split('\\n')):\n+        heading_match = re.match(r'^(#+)\\s+(.+)$', line)\n+        \n+        jp_section_match = None\n+        if not heading_match:\n+            jp_section_match = re.match(r'^([０-９]+）|\\d+）)\\s*(.+)$', line)\n+        \n+        if heading_match:\n+            level = len(heading_match.group(1))\n+            title = heading_match.group(2).strip()\n+            headings.append((i+1, level, title))\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_section_match:\n+            level = 3\n+            title = jp_section_match.group(0).strip()\n+            headings.append((i+1, level, title))\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+    \n+    if current_section_start is not None and len(content.split('\\n')) > 0:\n+        section_content[current_section_start] = (current_section_start, len(content.split('\\n')))\n+    \n+    sections = {}\n+    for i, (line_num, level, title) in enumerate(headings):\n+        parent_sections = []\n+        for prev_line, prev_level, prev_title in reversed(headings[:i]):\n+            if prev_level < level:\n+                parent_sections.insert(0, prev_title)\n+                if prev_level == 1:  # Stop at top level\n+                    break\n+        \n+        section_path = \" > \".join(parent_sections + [title])\n+        \n+        content_range = section_content.get(line_num, (line_num, line_num))\n+        \n+        sections[line_num] = {\n+            \"level\": level,\n+            \"title\": title,\n+            \"path\": section_path,\n+            \"parent_sections\": parent_sections,\n+            \"start_line\": content_range[0],\n+            \"end_line\": content_range[1]\n+        }\n+    \n+    return sections\n+\n+def find_section_for_line(sections, line_number):\n+    \"\"\"Find the section that contains a specific line.\"\"\"\n+    for section_line, section_info in sections.items():\n+        if section_info[\"start_line\"] <= line_number <= section_info[\"end_line\"]:\n+            return section_info\n+    \n+    section_lines = sorted(sections.keys())\n+    for i, section_line in enumerate(section_lines):\n+        if section_line > line_number:\n+            if i > 0:\n+                return sections[section_lines[i-1]]\n+            break\n+    \n+    if section_lines:\n+        return sections[section_lines[-1]]\n+    \n+    return None\n+\n+def parse_diff_hunks(diff_text):\n+    \"\"\"Parse the diff hunks to extract line numbers and changes with improved accuracy.\"\"\"\n+    hunks = []\n+    current_file = None\n+    in_file_header = False\n+    \n+    lines = diff_text.split('\\n')\n+    i = 0\n+    \n+    while i < len(lines):\n+        line = lines[i]\n+        \n+        if line.startswith('diff --git'):\n+            in_file_header = True\n+            match = re.search(r'b/(.+?)(\"?)$', line)\n+            if match:\n+                current_file = match.group(1)\n+                if current_file.endswith('\"'):\n+                    current_file = current_file[:-1]\n+                current_file = current_file.replace('\\\\', '')\n+                print(f\"DEBUG: Extracted file path: {current_file}\")\n+        elif line.startswith('+++'):\n+            in_file_header = False\n+            match = re.search(r'\\+\\+\\+ b/(.+?)(\"?)$', line)\n+            if match:\n+                new_file = match.group(1)\n+                if new_file.endswith('\"'):\n+                    new_file = new_file[:-1]\n+                new_file = new_file.replace('\\\\', '')\n+                print(f\"DEBUG: Updated file path from +++ line: {new_file}\")\n+                current_file = new_file\n+        elif line.startswith('@@') and not in_file_header:\n+            match = re.search(r'@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@', line)\n+            if match and current_file:\n+                old_start = int(match.group(1))\n+                old_count = int(match.group(2)) if match.group(2) else 1\n+                new_start = int(match.group(3))\n+                new_count = int(match.group(4)) if match.group(4) else 1\n+                \n+                hunk = {\n+                    'file': current_file,\n+                    'old_start': old_start,\n+                    'old_count': old_count,\n+                    'new_start': new_start,\n+                    'new_count': new_count,\n+                    'changes': [],\n+                    'context_before': [],\n+                    'context_after': []\n+                }\n+                \n+                j = i + 1\n+                while j < len(lines) and j < i + 4 and not lines[j].startswith(('+', '-', '@@')):\n+                    hunk['context_before'].append(lines[j])\n+                    j += 1\n+                \n+                current_old_line = old_start\n+                current_new_line = new_start\n+                \n+                j = i + 1\n+                while j < len(lines) and not lines[j].startswith('@@'):\n+                    if lines[j].startswith('+'):\n+                        hunk['changes'].append({\n+                            'type': 'add',\n+                            'line_num': current_new_line,\n+                            'content': lines[j][1:]\n+                        })\n+                        current_new_line += 1\n+                    elif lines[j].startswith('-'):\n+                        hunk['changes'].append({\n+                            'type': 'remove',\n+                            'line_num': current_old_line,\n+                            'content': lines[j][1:]\n+                        })\n+                        current_old_line += 1\n+                    else:\n+                        current_old_line += 1\n+                        current_new_line += 1\n+                    j += 1\n+                \n+                hunks.append(hunk)\n+        \n+        i += 1\n+    \n+    return hunks\n+\n+def analyze_pr(pr_number):\n+    \"\"\"Analyze a PR and extract the sections being modified.\"\"\"\n+    pr_details = get_pr_details(pr_number)\n+    diff_text = get_pr_diff(pr_number)\n+    \n+    print(f\"DEBUG: Analyzing PR #{pr_number}\")\n+    \n+    if not diff_text:\n+        print(\"DEBUG: No diff text found\")\n+        return None\n+    \n+    print(f\"DEBUG: Diff text length: {len(diff_text)} characters\")\n+    print(f\"DEBUG: First 100 chars of diff: {diff_text[:100]}\")\n+    \n+    hunks = parse_diff_hunks(diff_text)\n+    print(f\"DEBUG: Found {len(hunks)} hunks in diff\")\n+    \n+    results = []\n+    for i, hunk in enumerate(hunks):\n+        file_path = hunk['file']\n+        print(f\"DEBUG: Hunk {i+1} - File: {file_path}\")\n+        \n+        is_markdown = False\n+        if file_path.endswith('.md'):\n+            is_markdown = True\n+        elif file_path.lower().endswith('.md\"'):\n+            is_markdown = True\n+            file_path = file_path[:-1]  # Remove trailing quote\n+        elif '.' in file_path and file_path.split('.')[-1].lower() == 'md':\n+            is_markdown = True\n+        \n+        if not is_markdown:\n+            print(f\"DEBUG: Skipping non-markdown file: {file_path}\")\n+            continue\n+        \n+        full_path = os.path.join(os.getcwd(), file_path)\n+        print(f\"DEBUG: Full path: {full_path}\")\n+        \n+        if not os.path.exists(full_path):\n+            print(f\"Warning: File {full_path} does not exist, skipping\")\n+            continue\n+        \n+        sections = extract_markdown_sections(full_path)\n+        print(f\"DEBUG: Found {len(sections)} sections in {file_path}\")\n+        if len(sections) > 0:\n+            print(f\"DEBUG: Section titles: {[s['title'] for s in sections.values()][:5]}\")\n+        \n+        affected_sections = set()\n+        for j, change in enumerate(hunk['changes']):\n+            line_number = change['line_num']\n+            print(f\"DEBUG: Change {j+1} - Line: {line_number}, Content: {change['content'][:30]}...\")\n+            \n+            section = find_section_for_line(sections, line_number)\n+            \n+            if section:\n+                print(f\"DEBUG: Found section: {section['title']}\")\n+                affected_sections.add((section['title'], section['path']))\n+            else:\n+                print(f\"DEBUG: No section found for line {line_number}\")\n+        \n+        for section_title, section_path in affected_sections:\n+            results.append({\n+                'pr_number': pr_number,\n+                'pr_title': pr_details.get('title', ''),\n+                'pr_url': pr_details.get('url', ''),\n+                'file': file_path,\n+                'section': section_title,\n+                'section_path': section_path,\n+                'changes': [c['content'] for c in hunk['changes']]\n+            })\n+    \n+    print(f\"DEBUG: Found {len(results)} affected sections\")\n+    return results\n+\n+def analyze_all_prs(limit=100):\n+    \"\"\"Analyze all PRs and group them by the sections they modify.\"\"\"\n+    prs = get_pr_list(limit)\n+    \n+    all_results = []\n+    section_to_prs = defaultdict(list)\n+    \n+    for pr in prs:\n+        pr_number = pr['number']\n+        print(f\"Analyzing PR #{pr_number}: {pr['title']}\")\n+        \n+        results = analyze_pr(pr_number)\n+        if results:\n+            all_results.extend(results)\n+            \n+            for result in results:\n+                section_key = f\"{result['file']}:{result['section_path']}\"\n+                section_to_prs[section_key].append({\n+                    'pr_number': pr_number,\n+                    'pr_title': pr['title'],\n+                    'pr_url': pr['url']\n+                })\n+    \n+    return all_results, section_to_prs\n+\n+def generate_report(all_results, section_to_prs):\n+    \"\"\"Generate a report of the analysis results.\"\"\"\n+    report = []\n+    \n+    report.append(\"# PRs Grouped by Section\\n\")\n+    \n+    for section_key, prs in sorted(section_to_prs.items()):\n+        if len(prs) > 1:  # Only show sections with multiple PRs\n+            file_path, section_path = section_key.split(':', 1)\n+            report.append(f\"## {file_path}\\n\")\n+            report.append(f\"### {section_path}\\n\")\n+            report.append(\"PRs modifying this section:\\n\")\n+            \n+            for pr in prs:\n+                report.append(f\"- PR #{pr['pr_number']}: {pr['pr_title']} ({pr['pr_url']})\\n\")\n+            \n+            report.append(\"\\n\")\n+    \n+    report.append(\"# Sections Modified by Each PR\\n\")\n+    \n+    pr_to_sections = defaultdict(list)\n+    for result in all_results:\n+        pr_key = f\"{result['pr_number']}:{result['pr_title']}\"\n+        section_info = {\n+            'file': result['file'],\n+            'section_path': result['section_path']\n+        }\n+        pr_to_sections[pr_key].append(section_info)\n+    \n+    for pr_key, sections in sorted(pr_to_sections.items()):\n+        pr_number, pr_title = pr_key.split(':', 1)\n+        report.append(f\"## PR #{pr_number}: {pr_title}\\n\")\n+        \n+        for section in sections:\n+            report.append(f\"- {section['file']}: {section['section_path']}\\n\")\n+        \n+        report.append(\"\\n\")\n+    \n+    return \"\".join(report)\n+\n+def main():\n+    \"\"\"Main function with improved command-line interface.\"\"\"\n+    parser = argparse.ArgumentParser(description='Analyze PRs in the policy repository and extract modified sections.')\n+    group = parser.add_mutually_exclusive_group(required=True)\n+    group.add_argument('--pr', type=str, help='PR number to analyze')\n+    group.add_argument('--all', action='store_true', help='Analyze all PRs')\n+    parser.add_argument('--limit', type=int, default=100, help='Limit the number of PRs to analyze (default: 100)')\n+    parser.add_argument('--output', type=str, help='Output file for the report (default: stdout)')\n+    parser.add_argument('--format', choices=['text', 'json'], default='text', help='Output format (default: text)')\n+    \n+    args = parser.parse_args()\n+    \n+    if args.pr:\n+        results = analyze_pr(args.pr)\n+        \n+        if results:\n+            if args.format == 'json':\n+                output = json.dumps(results, indent=2, ensure_ascii=False)\n+            else:\n+                output = f\"\\nAnalysis of PR #{args.pr}:\\n\"\n+                for result in results:\n+                    output += f\"\\nFile: {result['file']}\\n\"\n+                    output += f\"Section: {result['section_path']}\\n\"\n+                    output += \"Changes:\\n\"\n+                    for change in result['changes']:\n+                        output += f\"  {change}\\n\"\n+        else:\n+            output = f\"No markdown sections found in PR #{args.pr}\"\n+    else:  # --all\n+        all_results, section_to_prs = analyze_all_prs(args.limit)\n+        \n+        if args.format == 'json':\n+            output = json.dumps({\n+                'results': all_results,\n+                'section_to_prs': {k: v for k, v in section_to_prs.items()}\n+            }, indent=2, ensure_ascii=False)\n+        else:\n+            output = generate_report(all_results, section_to_prs)\n+    \n+    if args.output:\n+        with open(args.output, 'w', encoding='utf-8') as f:\n+            f.write(output)\n+        print(f\"Report written to {args.output}\")\n+    else:\n+        print(output)\n+\n+if __name__ == \"__main__\":\n+    main()"
    },
    {
      "sha": "a1a964cd88275570738c78f209c59f246f07a092",
      "filename": "pr_section_analyzer_final.py",
      "status": "added",
      "additions": 390,
      "deletions": 0,
      "changes": 390,
      "blob_url": "https://github.com/team-mirai/policy/blob/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer_final.py",
      "raw_url": "https://github.com/team-mirai/policy/raw/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer_final.py",
      "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/pr_section_analyzer_final.py?ref=1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
      "patch": "@@ -0,0 +1,390 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Script to analyze PRs in the policy repository and extract the specific sections \n+of the manifest that are being modified. This script can identify which PRs modify\n+the same sections of the manifest.\n+\"\"\"\n+\n+import json\n+import os\n+import re\n+import subprocess\n+import sys\n+import argparse\n+from collections import defaultdict\n+\n+def run_command(command):\n+    \"\"\"Run a shell command and return the output.\"\"\"\n+    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n+    if result.returncode != 0:\n+        print(f\"Error running command: {command}\")\n+        print(f\"Error: {result.stderr}\")\n+        return \"\"\n+    return result.stdout.strip()\n+\n+def get_pr_list(limit=100):\n+    \"\"\"Get a list of PRs from the repository.\"\"\"\n+    command = f\"gh pr list --limit {limit} --json number,title,headRefName,state,url\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR list: {output}\")\n+        return []\n+\n+def get_pr_details(pr_number):\n+    \"\"\"Get detailed information about a PR.\"\"\"\n+    command = f\"gh pr view {pr_number} --json number,title,headRefName,state,url,body\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR details: {output}\")\n+        return {}\n+\n+def get_pr_files(pr_number):\n+    \"\"\"Get the list of files changed in a PR using GitHub API.\"\"\"\n+    command = f\"gh pr view {pr_number} --json files\"\n+    output = run_command(command)\n+    try:\n+        data = json.loads(output)\n+        files = [file.get('path') for file in data.get('files', [])]\n+        print(f\"Changed files from GitHub API: {files}\")\n+        return files\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR files: {output}\")\n+        return []\n+\n+def get_file_diff(pr_number, file_path):\n+    \"\"\"Get the diff for a specific file in a PR.\"\"\"\n+    command = f\"gh pr view {pr_number} --json headRefName\"\n+    output = run_command(command)\n+    try:\n+        data = json.loads(output)\n+        branch_name = data.get('headRefName')\n+        if not branch_name:\n+            print(f\"Could not get branch name for PR #{pr_number}\")\n+            return \"\"\n+        \n+        fetch_cmd = f\"git fetch origin {branch_name}\"\n+        run_command(fetch_cmd)\n+        \n+        diff_cmd = f\"git diff main..origin/{branch_name} -- '{file_path}'\"\n+        return run_command(diff_cmd)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON for PR branch: {output}\")\n+        return \"\"\n+\n+def extract_markdown_sections(file_path):\n+    \"\"\"Extract the markdown sections from a file with improved Japanese section handling.\"\"\"\n+    if not os.path.exists(file_path):\n+        print(f\"File not found: {file_path}\")\n+        return {}\n+    \n+    with open(file_path, 'r', encoding='utf-8') as f:\n+        content = f.read()\n+    \n+    print(f\"Extracting sections from file with {len(content.split('\\n'))} lines\")\n+    \n+    headings = []\n+    section_content = {}\n+    current_section_start = None\n+    \n+    for i, line in enumerate(content.split('\\n')):\n+        heading_match = re.match(r'^(#+)\\s+(.+)$', line)\n+        \n+        jp_section_match = None\n+        if not heading_match:\n+            jp_section_match = re.match(r'^([０-９]+）|\\d+）)\\s*(.+)$', line)\n+        \n+        jp_policy_match = None\n+        if not heading_match and not jp_section_match:\n+            jp_policy_match = re.match(r'^###\\s+(現状認識・課題分析|政策概要)$', line)\n+        \n+        if heading_match:\n+            level = len(heading_match.group(1))\n+            title = heading_match.group(2).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found heading at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_section_match:\n+            level = 2\n+            title = jp_section_match.group(0).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_policy_match:\n+            level = 3\n+            title = jp_policy_match.group(1).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP policy section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+    \n+    if current_section_start is not None and len(content.split('\\n')) > 0:\n+        section_content[current_section_start] = (current_section_start, len(content.split('\\n')))\n+    \n+    sections = {}\n+    \n+    sorted_headings = sorted(headings, key=lambda x: x[0])\n+    \n+    for i, (line_num, level, title) in enumerate(sorted_headings):\n+        parent_sections = []\n+        current_parent_level = 0\n+        \n+        for j in range(i-1, -1, -1):\n+            prev_line, prev_level, prev_title = sorted_headings[j]\n+            \n+            if prev_level < level and prev_level > current_parent_level:\n+                parent_sections.insert(0, prev_title)\n+                current_parent_level = prev_level\n+                \n+                if prev_level == 1:\n+                    break\n+        \n+        section_path = \" > \".join(parent_sections + [title])\n+        \n+        content_range = section_content.get(line_num, (line_num, line_num))\n+        \n+        sections[line_num] = {\n+            \"level\": level,\n+            \"title\": title,\n+            \"path\": section_path,\n+            \"parent_sections\": parent_sections,\n+            \"start_line\": content_range[0],\n+            \"end_line\": content_range[1]\n+        }\n+    \n+    return sections\n+\n+def find_section_for_line(sections, line_number):\n+    \"\"\"Find the section that contains a specific line.\"\"\"\n+    for section_line, section_info in sections.items():\n+        if section_info[\"start_line\"] <= line_number <= section_info[\"end_line\"]:\n+            return section_info\n+    \n+    section_lines = sorted(sections.keys())\n+    for i, section_line in enumerate(section_lines):\n+        if section_line > line_number:\n+            if i > 0:\n+                return sections[section_lines[i-1]]\n+            break\n+    \n+    if section_lines:\n+        return sections[section_lines[-1]]\n+    \n+    return None\n+\n+def extract_line_numbers_from_diff(diff_text):\n+    \"\"\"Extract line numbers from a diff.\"\"\"\n+    line_numbers = []\n+    current_line = None\n+    \n+    for line in diff_text.split('\\n'):\n+        if line.startswith('@@'):\n+            match = re.search(r'@@ -\\d+(?:,\\d+)? \\+(\\d+)(?:,(\\d+))? @@', line)\n+            if match:\n+                current_line = int(match.group(1))\n+                line_count = int(match.group(2)) if match.group(2) else 1\n+        elif current_line is not None:\n+            if line.startswith('+') and not line.startswith('+++'):\n+                line_numbers.append(current_line)\n+                current_line += 1\n+            elif line.startswith('-'):\n+                pass\n+            else:\n+                current_line += 1\n+    \n+    return line_numbers\n+\n+def analyze_pr(pr_number):\n+    \"\"\"Analyze a PR and extract the sections being modified.\"\"\"\n+    pr_details = get_pr_details(pr_number)\n+    changed_files = get_pr_files(pr_number)\n+    \n+    print(f\"Analyzing PR #{pr_number}\")\n+    \n+    if not changed_files:\n+        print(\"No changed files found\")\n+        return None\n+    \n+    results = []\n+    \n+    for file_path in changed_files:\n+        if not file_path.endswith('.md'):\n+            print(f\"Skipping non-markdown file: {file_path}\")\n+            continue\n+        \n+        print(f\"Processing file: {file_path}\")\n+        full_path = os.path.join(os.getcwd(), file_path)\n+        \n+        if not os.path.exists(full_path):\n+            print(f\"Warning: File {full_path} does not exist, skipping\")\n+            continue\n+        \n+        file_diff = get_file_diff(pr_number, file_path)\n+        \n+        if not file_diff:\n+            print(f\"No diff found for file {file_path}\")\n+            continue\n+        \n+        print(f\"Found diff for file {file_path}, length: {len(file_diff)}\")\n+        \n+        line_numbers = extract_line_numbers_from_diff(file_diff)\n+        \n+        if not line_numbers:\n+            print(f\"No line numbers found in diff for file {file_path}\")\n+            continue\n+        \n+        print(f\"Affected line numbers: {line_numbers[:10]}...\")\n+        \n+        sections = extract_markdown_sections(full_path)\n+        print(f\"Found {len(sections)} sections in {file_path}\")\n+        \n+        if len(sections) > 0:\n+            print(f\"Section titles: {[s['title'] for s in sections.values()][:5]}\")\n+        \n+        affected_sections = set()\n+        for line_number in line_numbers:\n+            section = find_section_for_line(sections, line_number)\n+            if section:\n+                print(f\"Line {line_number} is in section: {section['title']}\")\n+                affected_sections.add((section['title'], section['path']))\n+        \n+        for section_title, section_path in affected_sections:\n+            results.append({\n+                'pr_number': pr_number,\n+                'pr_title': pr_details.get('title', ''),\n+                'pr_url': pr_details.get('url', ''),\n+                'file': file_path,\n+                'section': section_title,\n+                'section_path': section_path,\n+                'changes': [line for line in file_diff.split('\\n') if line.startswith('+') and not line.startswith('+++')]\n+            })\n+    \n+    print(f\"Found {len(results)} affected sections\")\n+    return results\n+\n+def analyze_all_prs(limit=100):\n+    \"\"\"Analyze all PRs and group them by the sections they modify.\"\"\"\n+    prs = get_pr_list(limit)\n+    \n+    all_results = []\n+    section_to_prs = defaultdict(list)\n+    \n+    for pr in prs:\n+        pr_number = pr['number']\n+        print(f\"Analyzing PR #{pr_number}: {pr['title']}\")\n+        \n+        results = analyze_pr(pr_number)\n+        if results:\n+            all_results.extend(results)\n+            \n+            for result in results:\n+                section_key = f\"{result['file']}:{result['section_path']}\"\n+                section_to_prs[section_key].append({\n+                    'pr_number': pr_number,\n+                    'pr_title': pr['title'],\n+                    'pr_url': pr['url']\n+                })\n+    \n+    return all_results, section_to_prs\n+\n+def generate_report(all_results, section_to_prs):\n+    \"\"\"Generate a report of the analysis results.\"\"\"\n+    report = []\n+    \n+    report.append(\"# PRs Grouped by Section\\n\")\n+    \n+    for section_key, prs in sorted(section_to_prs.items()):\n+        if len(prs) > 1:  # Only show sections with multiple PRs\n+            file_path, section_path = section_key.split(':', 1)\n+            report.append(f\"## {file_path}\\n\")\n+            report.append(f\"### {section_path}\\n\")\n+            report.append(\"PRs modifying this section:\\n\")\n+            \n+            for pr in prs:\n+                report.append(f\"- PR #{pr['pr_number']}: {pr['pr_title']} ({pr['pr_url']})\\n\")\n+            \n+            report.append(\"\\n\")\n+    \n+    report.append(\"# Sections Modified by Each PR\\n\")\n+    \n+    pr_to_sections = defaultdict(list)\n+    for result in all_results:\n+        pr_key = f\"{result['pr_number']}:{result['pr_title']}\"\n+        section_info = {\n+            'file': result['file'],\n+            'section_path': result['section_path']\n+        }\n+        pr_to_sections[pr_key].append(section_info)\n+    \n+    for pr_key, sections in sorted(pr_to_sections.items()):\n+        pr_number, pr_title = pr_key.split(':', 1)\n+        report.append(f\"## PR #{pr_number}: {pr_title}\\n\")\n+        \n+        for section in sections:\n+            report.append(f\"- {section['file']}: {section['section_path']}\\n\")\n+        \n+        report.append(\"\\n\")\n+    \n+    return \"\".join(report)\n+\n+def main():\n+    \"\"\"Main function with improved command-line interface.\"\"\"\n+    parser = argparse.ArgumentParser(description='Analyze PRs in the policy repository and extract modified sections.')\n+    group = parser.add_mutually_exclusive_group(required=True)\n+    group.add_argument('--pr', type=str, help='PR number to analyze')\n+    group.add_argument('--all', action='store_true', help='Analyze all PRs')\n+    parser.add_argument('--limit', type=int, default=100, help='Limit the number of PRs to analyze (default: 100)')\n+    parser.add_argument('--output', type=str, help='Output file for the report (default: stdout)')\n+    parser.add_argument('--format', choices=['text', 'json'], default='text', help='Output format (default: text)')\n+    \n+    args = parser.parse_args()\n+    \n+    if args.pr:\n+        results = analyze_pr(args.pr)\n+        \n+        if results:\n+            if args.format == 'json':\n+                output = json.dumps(results, indent=2, ensure_ascii=False)\n+            else:\n+                output = f\"\\nAnalysis of PR #{args.pr}:\\n\"\n+                for result in results:\n+                    output += f\"\\nFile: {result['file']}\\n\"\n+                    output += f\"Section: {result['section_path']}\\n\"\n+                    output += \"Changes:\\n\"\n+                    for change in result['changes']:\n+                        output += f\"  {change}\\n\"\n+        else:\n+            output = f\"No markdown sections found in PR #{args.pr}\"\n+    else:  # --all\n+        all_results, section_to_prs = analyze_all_prs(args.limit)\n+        \n+        if args.format == 'json':\n+            output = json.dumps({\n+                'results': all_results,\n+                'section_to_prs': {k: v for k, v in section_to_prs.items()}\n+            }, indent=2, ensure_ascii=False)\n+        else:\n+            output = generate_report(all_results, section_to_prs)\n+    \n+    if args.output:\n+        with open(args.output, 'w', encoding='utf-8') as f:\n+            f.write(output)\n+        print(f\"Report written to {args.output}\")\n+    else:\n+        print(output)\n+\n+if __name__ == \"__main__\":\n+    main()"
    },
    {
      "sha": "29dd289128d3622917579d7305d95eea19622db4",
      "filename": "pr_section_analyzer_new.py",
      "status": "added",
      "additions": 395,
      "deletions": 0,
      "changes": 395,
      "blob_url": "https://github.com/team-mirai/policy/blob/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer_new.py",
      "raw_url": "https://github.com/team-mirai/policy/raw/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer_new.py",
      "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/pr_section_analyzer_new.py?ref=1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
      "patch": "@@ -0,0 +1,395 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Script to analyze PRs in the policy repository and extract the specific sections \n+of the manifest that are being modified. This script can identify which PRs modify\n+the same sections of the manifest.\n+\"\"\"\n+\n+import json\n+import os\n+import re\n+import subprocess\n+import sys\n+import argparse\n+from collections import defaultdict\n+\n+def run_command(command):\n+    \"\"\"Run a shell command and return the output.\"\"\"\n+    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n+    if result.returncode != 0:\n+        print(f\"Error running command: {command}\")\n+        print(f\"Error: {result.stderr}\")\n+        return \"\"\n+    return result.stdout.strip()\n+\n+def get_pr_list(limit=100):\n+    \"\"\"Get a list of PRs from the repository.\"\"\"\n+    command = f\"gh pr list --limit {limit} --json number,title,headRefName,state,url\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR list: {output}\")\n+        return []\n+\n+def get_pr_details(pr_number):\n+    \"\"\"Get detailed information about a PR.\"\"\"\n+    command = f\"gh pr view {pr_number} --json number,title,headRefName,state,url,body\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR details: {output}\")\n+        return {}\n+\n+def get_pr_changed_files(pr_number):\n+    \"\"\"Get the list of files changed in a PR using git directly.\"\"\"\n+    fetch_cmd = f\"git fetch origin pull/{pr_number}/head:pr-{pr_number}-temp\"\n+    run_command(fetch_cmd)\n+    \n+    files_cmd = f\"git ls-tree -r --name-only pr-{pr_number}-temp\"\n+    files_output = run_command(files_cmd)\n+    \n+    if not files_output:\n+        print(f\"No changed files found for PR #{pr_number}\")\n+        return []\n+    \n+    all_files = files_output.split('\\n')\n+    \n+    diff_cmd = f\"git diff --name-only main..pr-{pr_number}-temp\"\n+    diff_output = run_command(diff_cmd)\n+    \n+    if not diff_output:\n+        print(f\"No changed files found in diff for PR #{pr_number}\")\n+        return []\n+    \n+    changed_files_escaped = diff_output.split('\\n')\n+    \n+    changed_files = []\n+    for file_path in all_files:\n+        for escaped_path in changed_files_escaped:\n+            if escaped_path.startswith('\"') and escaped_path.endswith('\"'):\n+                escaped_path = escaped_path[1:-1]\n+            \n+            try:\n+                decoded_path = bytes(escaped_path, 'utf-8').decode('unicode_escape')\n+                if decoded_path == file_path or escaped_path == file_path:\n+                    changed_files.append(file_path)\n+                    break\n+            except:\n+                if escaped_path == file_path:\n+                    changed_files.append(file_path)\n+                    break\n+    \n+    print(f\"Changed files (actual): {changed_files}\")\n+    return changed_files\n+\n+def get_file_diff(pr_number, file_path):\n+    \"\"\"Get the diff for a specific file in a PR.\"\"\"\n+    command = f\"git diff main..pr-{pr_number}-temp -- '{file_path}'\"\n+    return run_command(command)\n+\n+def extract_markdown_sections(file_path):\n+    \"\"\"Extract the markdown sections from a file with improved Japanese section handling.\"\"\"\n+    if not os.path.exists(file_path):\n+        print(f\"File not found: {file_path}\")\n+        return {}\n+    \n+    with open(file_path, 'r', encoding='utf-8') as f:\n+        content = f.read()\n+    \n+    print(f\"Extracting sections from file with {len(content.split('\\n'))} lines\")\n+    \n+    headings = []\n+    section_content = {}\n+    current_section_start = None\n+    \n+    for i, line in enumerate(content.split('\\n')):\n+        heading_match = re.match(r'^(#+)\\s+(.+)$', line)\n+        \n+        jp_section_match = None\n+        if not heading_match:\n+            jp_section_match = re.match(r'^([０-９]+）|\\d+）)\\s*(.+)$', line)\n+        \n+        jp_policy_match = None\n+        if not heading_match and not jp_section_match:\n+            jp_policy_match = re.match(r'^###\\s+(現状認識・課題分析|政策概要)$', line)\n+        \n+        if heading_match:\n+            level = len(heading_match.group(1))\n+            title = heading_match.group(2).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found heading at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_section_match:\n+            level = 3\n+            title = jp_section_match.group(0).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_policy_match:\n+            level = 3\n+            title = jp_policy_match.group(1).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP policy section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+    \n+    if current_section_start is not None and len(content.split('\\n')) > 0:\n+        section_content[current_section_start] = (current_section_start, len(content.split('\\n')))\n+    \n+    sections = {}\n+    for i, (line_num, level, title) in enumerate(headings):\n+        parent_sections = []\n+        for prev_line, prev_level, prev_title in reversed(headings[:i]):\n+            if prev_level < level:\n+                parent_sections.insert(0, prev_title)\n+                if prev_level == 1:  # Stop at top level\n+                    break\n+        \n+        section_path = \" > \".join(parent_sections + [title])\n+        \n+        content_range = section_content.get(line_num, (line_num, line_num))\n+        \n+        sections[line_num] = {\n+            \"level\": level,\n+            \"title\": title,\n+            \"path\": section_path,\n+            \"parent_sections\": parent_sections,\n+            \"start_line\": content_range[0],\n+            \"end_line\": content_range[1]\n+        }\n+    \n+    return sections\n+\n+def find_section_for_line(sections, line_number):\n+    \"\"\"Find the section that contains a specific line.\"\"\"\n+    for section_line, section_info in sections.items():\n+        if section_info[\"start_line\"] <= line_number <= section_info[\"end_line\"]:\n+            return section_info\n+    \n+    section_lines = sorted(sections.keys())\n+    for i, section_line in enumerate(section_lines):\n+        if section_line > line_number:\n+            if i > 0:\n+                return sections[section_lines[i-1]]\n+            break\n+    \n+    if section_lines:\n+        return sections[section_lines[-1]]\n+    \n+    return None\n+\n+def extract_line_numbers_from_diff(diff_text):\n+    \"\"\"Extract line numbers from a diff.\"\"\"\n+    line_numbers = []\n+    current_line = None\n+    \n+    for line in diff_text.split('\\n'):\n+        if line.startswith('@@'):\n+            match = re.search(r'@@ -\\d+(?:,\\d+)? \\+(\\d+)(?:,(\\d+))? @@', line)\n+            if match:\n+                current_line = int(match.group(1))\n+                line_count = int(match.group(2)) if match.group(2) else 1\n+        elif current_line is not None:\n+            if line.startswith('+') and not line.startswith('+++'):\n+                line_numbers.append(current_line)\n+                current_line += 1\n+            elif line.startswith('-'):\n+                pass\n+            else:\n+                current_line += 1\n+    \n+    return line_numbers\n+\n+def analyze_pr(pr_number):\n+    \"\"\"Analyze a PR and extract the sections being modified.\"\"\"\n+    pr_details = get_pr_details(pr_number)\n+    changed_files = get_pr_changed_files(pr_number)\n+    \n+    print(f\"Analyzing PR #{pr_number}\")\n+    \n+    if not changed_files:\n+        print(\"No changed files found\")\n+        return None\n+    \n+    results = []\n+    \n+    for file_path in changed_files:\n+        if not file_path.endswith('.md'):\n+            print(f\"Skipping non-markdown file: {file_path}\")\n+            continue\n+        \n+        print(f\"Processing file: {file_path}\")\n+        full_path = os.path.join(os.getcwd(), file_path)\n+        \n+        if not os.path.exists(full_path):\n+            print(f\"Warning: File {full_path} does not exist, skipping\")\n+            continue\n+        \n+        file_diff = get_file_diff(pr_number, file_path)\n+        \n+        if not file_diff:\n+            print(f\"No diff found for file {file_path}\")\n+            continue\n+        \n+        print(f\"Found diff for file {file_path}, length: {len(file_diff)}\")\n+        \n+        line_numbers = extract_line_numbers_from_diff(file_diff)\n+        \n+        if not line_numbers:\n+            print(f\"No line numbers found in diff for file {file_path}\")\n+            continue\n+        \n+        print(f\"Affected line numbers: {line_numbers[:10]}...\")\n+        \n+        sections = extract_markdown_sections(full_path)\n+        print(f\"Found {len(sections)} sections in {file_path}\")\n+        \n+        if len(sections) > 0:\n+            print(f\"Section titles: {[s['title'] for s in sections.values()][:5]}\")\n+        \n+        affected_sections = set()\n+        for line_number in line_numbers:\n+            section = find_section_for_line(sections, line_number)\n+            if section:\n+                print(f\"Line {line_number} is in section: {section['title']}\")\n+                affected_sections.add((section['title'], section['path']))\n+        \n+        for section_title, section_path in affected_sections:\n+            results.append({\n+                'pr_number': pr_number,\n+                'pr_title': pr_details.get('title', ''),\n+                'pr_url': pr_details.get('url', ''),\n+                'file': file_path,\n+                'section': section_title,\n+                'section_path': section_path,\n+                'changes': [line for line in file_diff.split('\\n') if line.startswith('+') and not line.startswith('+++')]\n+            })\n+    \n+    print(f\"Found {len(results)} affected sections\")\n+    return results\n+\n+def analyze_all_prs(limit=100):\n+    \"\"\"Analyze all PRs and group them by the sections they modify.\"\"\"\n+    prs = get_pr_list(limit)\n+    \n+    all_results = []\n+    section_to_prs = defaultdict(list)\n+    \n+    for pr in prs:\n+        pr_number = pr['number']\n+        print(f\"Analyzing PR #{pr_number}: {pr['title']}\")\n+        \n+        results = analyze_pr(pr_number)\n+        if results:\n+            all_results.extend(results)\n+            \n+            for result in results:\n+                section_key = f\"{result['file']}:{result['section_path']}\"\n+                section_to_prs[section_key].append({\n+                    'pr_number': pr_number,\n+                    'pr_title': pr['title'],\n+                    'pr_url': pr['url']\n+                })\n+    \n+    return all_results, section_to_prs\n+\n+def generate_report(all_results, section_to_prs):\n+    \"\"\"Generate a report of the analysis results.\"\"\"\n+    report = []\n+    \n+    report.append(\"# PRs Grouped by Section\\n\")\n+    \n+    for section_key, prs in sorted(section_to_prs.items()):\n+        if len(prs) > 1:  # Only show sections with multiple PRs\n+            file_path, section_path = section_key.split(':', 1)\n+            report.append(f\"## {file_path}\\n\")\n+            report.append(f\"### {section_path}\\n\")\n+            report.append(\"PRs modifying this section:\\n\")\n+            \n+            for pr in prs:\n+                report.append(f\"- PR #{pr['pr_number']}: {pr['pr_title']} ({pr['pr_url']})\\n\")\n+            \n+            report.append(\"\\n\")\n+    \n+    report.append(\"# Sections Modified by Each PR\\n\")\n+    \n+    pr_to_sections = defaultdict(list)\n+    for result in all_results:\n+        pr_key = f\"{result['pr_number']}:{result['pr_title']}\"\n+        section_info = {\n+            'file': result['file'],\n+            'section_path': result['section_path']\n+        }\n+        pr_to_sections[pr_key].append(section_info)\n+    \n+    for pr_key, sections in sorted(pr_to_sections.items()):\n+        pr_number, pr_title = pr_key.split(':', 1)\n+        report.append(f\"## PR #{pr_number}: {pr_title}\\n\")\n+        \n+        for section in sections:\n+            report.append(f\"- {section['file']}: {section['section_path']}\\n\")\n+        \n+        report.append(\"\\n\")\n+    \n+    return \"\".join(report)\n+\n+def main():\n+    \"\"\"Main function with improved command-line interface.\"\"\"\n+    parser = argparse.ArgumentParser(description='Analyze PRs in the policy repository and extract modified sections.')\n+    group = parser.add_mutually_exclusive_group(required=True)\n+    group.add_argument('--pr', type=str, help='PR number to analyze')\n+    group.add_argument('--all', action='store_true', help='Analyze all PRs')\n+    parser.add_argument('--limit', type=int, default=100, help='Limit the number of PRs to analyze (default: 100)')\n+    parser.add_argument('--output', type=str, help='Output file for the report (default: stdout)')\n+    parser.add_argument('--format', choices=['text', 'json'], default='text', help='Output format (default: text)')\n+    \n+    args = parser.parse_args()\n+    \n+    if args.pr:\n+        results = analyze_pr(args.pr)\n+        \n+        if results:\n+            if args.format == 'json':\n+                output = json.dumps(results, indent=2, ensure_ascii=False)\n+            else:\n+                output = f\"\\nAnalysis of PR #{args.pr}:\\n\"\n+                for result in results:\n+                    output += f\"\\nFile: {result['file']}\\n\"\n+                    output += f\"Section: {result['section_path']}\\n\"\n+                    output += \"Changes:\\n\"\n+                    for change in result['changes']:\n+                        output += f\"  {change}\\n\"\n+        else:\n+            output = f\"No markdown sections found in PR #{args.pr}\"\n+    else:  # --all\n+        all_results, section_to_prs = analyze_all_prs(args.limit)\n+        \n+        if args.format == 'json':\n+            output = json.dumps({\n+                'results': all_results,\n+                'section_to_prs': {k: v for k, v in section_to_prs.items()}\n+            }, indent=2, ensure_ascii=False)\n+        else:\n+            output = generate_report(all_results, section_to_prs)\n+    \n+    if args.output:\n+        with open(args.output, 'w', encoding='utf-8') as f:\n+            f.write(output)\n+        print(f\"Report written to {args.output}\")\n+    else:\n+        print(output)\n+\n+if __name__ == \"__main__\":\n+    main()"
    },
    {
      "sha": "a5f4987ed94c6ced36ef9c83baafad95ca4fe49e",
      "filename": "pr_section_analyzer_v2.py",
      "status": "added",
      "additions": 381,
      "deletions": 0,
      "changes": 381,
      "blob_url": "https://github.com/team-mirai/policy/blob/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer_v2.py",
      "raw_url": "https://github.com/team-mirai/policy/raw/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer_v2.py",
      "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/pr_section_analyzer_v2.py?ref=1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
      "patch": "@@ -0,0 +1,381 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Script to analyze PRs in the policy repository and extract the specific sections \n+of the manifest that are being modified. This script can identify which PRs modify\n+the same sections of the manifest.\n+\"\"\"\n+\n+import json\n+import os\n+import re\n+import subprocess\n+import sys\n+import argparse\n+from collections import defaultdict\n+\n+def run_command(command):\n+    \"\"\"Run a shell command and return the output.\"\"\"\n+    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n+    if result.returncode != 0:\n+        print(f\"Error running command: {command}\")\n+        print(f\"Error: {result.stderr}\")\n+        return \"\"\n+    return result.stdout.strip()\n+\n+def get_pr_list(limit=100):\n+    \"\"\"Get a list of PRs from the repository.\"\"\"\n+    command = f\"gh pr list --limit {limit} --json number,title,headRefName,state,url\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR list: {output}\")\n+        return []\n+\n+def get_pr_details(pr_number):\n+    \"\"\"Get detailed information about a PR.\"\"\"\n+    command = f\"gh pr view {pr_number} --json number,title,headRefName,state,url,body\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR details: {output}\")\n+        return {}\n+\n+def get_pr_files(pr_number):\n+    \"\"\"Get the list of files changed in a PR using GitHub API.\"\"\"\n+    command = f\"gh pr view {pr_number} --json files\"\n+    output = run_command(command)\n+    try:\n+        data = json.loads(output)\n+        files = [file.get('path') for file in data.get('files', [])]\n+        print(f\"Changed files from GitHub API: {files}\")\n+        return files\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR files: {output}\")\n+        return []\n+\n+def get_file_diff(pr_number, file_path):\n+    \"\"\"Get the diff for a specific file in a PR.\"\"\"\n+    command = f\"gh pr view {pr_number} --json headRefName\"\n+    output = run_command(command)\n+    try:\n+        data = json.loads(output)\n+        branch_name = data.get('headRefName')\n+        if not branch_name:\n+            print(f\"Could not get branch name for PR #{pr_number}\")\n+            return \"\"\n+        \n+        fetch_cmd = f\"git fetch origin {branch_name}\"\n+        run_command(fetch_cmd)\n+        \n+        diff_cmd = f\"git diff main..origin/{branch_name} -- '{file_path}'\"\n+        return run_command(diff_cmd)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON for PR branch: {output}\")\n+        return \"\"\n+\n+def extract_markdown_sections(file_path):\n+    \"\"\"Extract the markdown sections from a file with improved Japanese section handling.\"\"\"\n+    if not os.path.exists(file_path):\n+        print(f\"File not found: {file_path}\")\n+        return {}\n+    \n+    with open(file_path, 'r', encoding='utf-8') as f:\n+        content = f.read()\n+    \n+    print(f\"Extracting sections from file with {len(content.split('\\n'))} lines\")\n+    \n+    headings = []\n+    section_content = {}\n+    current_section_start = None\n+    \n+    for i, line in enumerate(content.split('\\n')):\n+        heading_match = re.match(r'^(#+)\\s+(.+)$', line)\n+        \n+        jp_section_match = None\n+        if not heading_match:\n+            jp_section_match = re.match(r'^([０-９]+）|\\d+）)\\s*(.+)$', line)\n+        \n+        jp_policy_match = None\n+        if not heading_match and not jp_section_match:\n+            jp_policy_match = re.match(r'^###\\s+(現状認識・課題分析|政策概要)$', line)\n+        \n+        if heading_match:\n+            level = len(heading_match.group(1))\n+            title = heading_match.group(2).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found heading at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_section_match:\n+            level = 3\n+            title = jp_section_match.group(0).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_policy_match:\n+            level = 3\n+            title = jp_policy_match.group(1).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP policy section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+    \n+    if current_section_start is not None and len(content.split('\\n')) > 0:\n+        section_content[current_section_start] = (current_section_start, len(content.split('\\n')))\n+    \n+    sections = {}\n+    for i, (line_num, level, title) in enumerate(headings):\n+        parent_sections = []\n+        for prev_line, prev_level, prev_title in reversed(headings[:i]):\n+            if prev_level < level:\n+                parent_sections.insert(0, prev_title)\n+                if prev_level == 1:  # Stop at top level\n+                    break\n+        \n+        section_path = \" > \".join(parent_sections + [title])\n+        \n+        content_range = section_content.get(line_num, (line_num, line_num))\n+        \n+        sections[line_num] = {\n+            \"level\": level,\n+            \"title\": title,\n+            \"path\": section_path,\n+            \"parent_sections\": parent_sections,\n+            \"start_line\": content_range[0],\n+            \"end_line\": content_range[1]\n+        }\n+    \n+    return sections\n+\n+def find_section_for_line(sections, line_number):\n+    \"\"\"Find the section that contains a specific line.\"\"\"\n+    for section_line, section_info in sections.items():\n+        if section_info[\"start_line\"] <= line_number <= section_info[\"end_line\"]:\n+            return section_info\n+    \n+    section_lines = sorted(sections.keys())\n+    for i, section_line in enumerate(section_lines):\n+        if section_line > line_number:\n+            if i > 0:\n+                return sections[section_lines[i-1]]\n+            break\n+    \n+    if section_lines:\n+        return sections[section_lines[-1]]\n+    \n+    return None\n+\n+def extract_line_numbers_from_diff(diff_text):\n+    \"\"\"Extract line numbers from a diff.\"\"\"\n+    line_numbers = []\n+    current_line = None\n+    \n+    for line in diff_text.split('\\n'):\n+        if line.startswith('@@'):\n+            match = re.search(r'@@ -\\d+(?:,\\d+)? \\+(\\d+)(?:,(\\d+))? @@', line)\n+            if match:\n+                current_line = int(match.group(1))\n+                line_count = int(match.group(2)) if match.group(2) else 1\n+        elif current_line is not None:\n+            if line.startswith('+') and not line.startswith('+++'):\n+                line_numbers.append(current_line)\n+                current_line += 1\n+            elif line.startswith('-'):\n+                pass\n+            else:\n+                current_line += 1\n+    \n+    return line_numbers\n+\n+def analyze_pr(pr_number):\n+    \"\"\"Analyze a PR and extract the sections being modified.\"\"\"\n+    pr_details = get_pr_details(pr_number)\n+    changed_files = get_pr_files(pr_number)\n+    \n+    print(f\"Analyzing PR #{pr_number}\")\n+    \n+    if not changed_files:\n+        print(\"No changed files found\")\n+        return None\n+    \n+    results = []\n+    \n+    for file_path in changed_files:\n+        if not file_path.endswith('.md'):\n+            print(f\"Skipping non-markdown file: {file_path}\")\n+            continue\n+        \n+        print(f\"Processing file: {file_path}\")\n+        full_path = os.path.join(os.getcwd(), file_path)\n+        \n+        if not os.path.exists(full_path):\n+            print(f\"Warning: File {full_path} does not exist, skipping\")\n+            continue\n+        \n+        file_diff = get_file_diff(pr_number, file_path)\n+        \n+        if not file_diff:\n+            print(f\"No diff found for file {file_path}\")\n+            continue\n+        \n+        print(f\"Found diff for file {file_path}, length: {len(file_diff)}\")\n+        \n+        line_numbers = extract_line_numbers_from_diff(file_diff)\n+        \n+        if not line_numbers:\n+            print(f\"No line numbers found in diff for file {file_path}\")\n+            continue\n+        \n+        print(f\"Affected line numbers: {line_numbers[:10]}...\")\n+        \n+        sections = extract_markdown_sections(full_path)\n+        print(f\"Found {len(sections)} sections in {file_path}\")\n+        \n+        if len(sections) > 0:\n+            print(f\"Section titles: {[s['title'] for s in sections.values()][:5]}\")\n+        \n+        affected_sections = set()\n+        for line_number in line_numbers:\n+            section = find_section_for_line(sections, line_number)\n+            if section:\n+                print(f\"Line {line_number} is in section: {section['title']}\")\n+                affected_sections.add((section['title'], section['path']))\n+        \n+        for section_title, section_path in affected_sections:\n+            results.append({\n+                'pr_number': pr_number,\n+                'pr_title': pr_details.get('title', ''),\n+                'pr_url': pr_details.get('url', ''),\n+                'file': file_path,\n+                'section': section_title,\n+                'section_path': section_path,\n+                'changes': [line for line in file_diff.split('\\n') if line.startswith('+') and not line.startswith('+++')]\n+            })\n+    \n+    print(f\"Found {len(results)} affected sections\")\n+    return results\n+\n+def analyze_all_prs(limit=100):\n+    \"\"\"Analyze all PRs and group them by the sections they modify.\"\"\"\n+    prs = get_pr_list(limit)\n+    \n+    all_results = []\n+    section_to_prs = defaultdict(list)\n+    \n+    for pr in prs:\n+        pr_number = pr['number']\n+        print(f\"Analyzing PR #{pr_number}: {pr['title']}\")\n+        \n+        results = analyze_pr(pr_number)\n+        if results:\n+            all_results.extend(results)\n+            \n+            for result in results:\n+                section_key = f\"{result['file']}:{result['section_path']}\"\n+                section_to_prs[section_key].append({\n+                    'pr_number': pr_number,\n+                    'pr_title': pr['title'],\n+                    'pr_url': pr['url']\n+                })\n+    \n+    return all_results, section_to_prs\n+\n+def generate_report(all_results, section_to_prs):\n+    \"\"\"Generate a report of the analysis results.\"\"\"\n+    report = []\n+    \n+    report.append(\"# PRs Grouped by Section\\n\")\n+    \n+    for section_key, prs in sorted(section_to_prs.items()):\n+        if len(prs) > 1:  # Only show sections with multiple PRs\n+            file_path, section_path = section_key.split(':', 1)\n+            report.append(f\"## {file_path}\\n\")\n+            report.append(f\"### {section_path}\\n\")\n+            report.append(\"PRs modifying this section:\\n\")\n+            \n+            for pr in prs:\n+                report.append(f\"- PR #{pr['pr_number']}: {pr['pr_title']} ({pr['pr_url']})\\n\")\n+            \n+            report.append(\"\\n\")\n+    \n+    report.append(\"# Sections Modified by Each PR\\n\")\n+    \n+    pr_to_sections = defaultdict(list)\n+    for result in all_results:\n+        pr_key = f\"{result['pr_number']}:{result['pr_title']}\"\n+        section_info = {\n+            'file': result['file'],\n+            'section_path': result['section_path']\n+        }\n+        pr_to_sections[pr_key].append(section_info)\n+    \n+    for pr_key, sections in sorted(pr_to_sections.items()):\n+        pr_number, pr_title = pr_key.split(':', 1)\n+        report.append(f\"## PR #{pr_number}: {pr_title}\\n\")\n+        \n+        for section in sections:\n+            report.append(f\"- {section['file']}: {section['section_path']}\\n\")\n+        \n+        report.append(\"\\n\")\n+    \n+    return \"\".join(report)\n+\n+def main():\n+    \"\"\"Main function with improved command-line interface.\"\"\"\n+    parser = argparse.ArgumentParser(description='Analyze PRs in the policy repository and extract modified sections.')\n+    group = parser.add_mutually_exclusive_group(required=True)\n+    group.add_argument('--pr', type=str, help='PR number to analyze')\n+    group.add_argument('--all', action='store_true', help='Analyze all PRs')\n+    parser.add_argument('--limit', type=int, default=100, help='Limit the number of PRs to analyze (default: 100)')\n+    parser.add_argument('--output', type=str, help='Output file for the report (default: stdout)')\n+    parser.add_argument('--format', choices=['text', 'json'], default='text', help='Output format (default: text)')\n+    \n+    args = parser.parse_args()\n+    \n+    if args.pr:\n+        results = analyze_pr(args.pr)\n+        \n+        if results:\n+            if args.format == 'json':\n+                output = json.dumps(results, indent=2, ensure_ascii=False)\n+            else:\n+                output = f\"\\nAnalysis of PR #{args.pr}:\\n\"\n+                for result in results:\n+                    output += f\"\\nFile: {result['file']}\\n\"\n+                    output += f\"Section: {result['section_path']}\\n\"\n+                    output += \"Changes:\\n\"\n+                    for change in result['changes']:\n+                        output += f\"  {change}\\n\"\n+        else:\n+            output = f\"No markdown sections found in PR #{args.pr}\"\n+    else:  # --all\n+        all_results, section_to_prs = analyze_all_prs(args.limit)\n+        \n+        if args.format == 'json':\n+            output = json.dumps({\n+                'results': all_results,\n+                'section_to_prs': {k: v for k, v in section_to_prs.items()}\n+            }, indent=2, ensure_ascii=False)\n+        else:\n+            output = generate_report(all_results, section_to_prs)\n+    \n+    if args.output:\n+        with open(args.output, 'w', encoding='utf-8') as f:\n+            f.write(output)\n+        print(f\"Report written to {args.output}\")\n+    else:\n+        print(output)\n+\n+if __name__ == \"__main__\":\n+    main()"
    }
  ]
}