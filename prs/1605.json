{
  "basic_info": {
    "url": "https://api.github.com/repos/team-mirai/policy/pulls/1605",
    "id": 2541894235,
    "node_id": "PR_kwDOOqTJvM6Xgjpb",
    "html_url": "https://github.com/team-mirai/policy/pull/1605",
    "diff_url": "https://github.com/team-mirai/policy/pull/1605.diff",
    "patch_url": "https://github.com/team-mirai/policy/pull/1605.patch",
    "issue_url": "https://api.github.com/repos/team-mirai/policy/issues/1605",
    "number": 1605,
    "state": "open",
    "locked": false,
    "title": "Add PR section analyzer script to identify which PRs modify the same sections",
    "user": {
      "login": "devin-ai-integration[bot]",
      "id": 158243242,
      "node_id": "BOT_kgDOCW6Zqg",
      "avatar_url": "https://avatars.githubusercontent.com/in/811515?v=4",
      "gravatar_id": "",
      "url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D",
      "html_url": "https://github.com/apps/devin-ai-integration",
      "followers_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/followers",
      "following_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/following{/other_user}",
      "gists_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/gists{/gist_id}",
      "starred_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/starred{/owner}{/repo}",
      "subscriptions_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/subscriptions",
      "organizations_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/orgs",
      "repos_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/repos",
      "events_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/events{/privacy}",
      "received_events_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/received_events",
      "type": "Bot",
      "user_view_type": "public",
      "site_admin": false
    },
    "body": "# PR Section Analyzer and Education PR Analyzer\n\nThis PR adds scripts to analyze pull requests in the policy repository and extract the specific sections of the manifest that are being modified. The scripts can identify which PRs modify the same sections of the manifest, making it easier to track related changes.\n\n## Features\n\n- Extracts markdown section hierarchies from policy documents\n- Identifies which sections are modified by each PR\n- Groups PRs by the sections they modify\n- Handles Japanese section formatting (ã‚¹ãƒ†ãƒƒãƒ—ï¼‘, ã‚¹ãƒ†ãƒƒãƒ—ï¼’, etc.)\n- Works without relying on LLM technology\n- Supports both single PR analysis and batch processing\n- Outputs results in text or JSON format\n\n## Implementation Details\n\nThe scripts use regex patterns to identify markdown headings and Japanese section numbering patterns. They build a section hierarchy tree and map line numbers from PR diffs to the corresponding sections.\n\nTwo main scripts are included:\n- `pr_section_analyzer_final.py`: Analyzes any PR to identify modified sections\n- `education_pr_analyzer.py`: Specifically analyzes PRs with the education label (\"æ•™è‚²\")\n\n## Usage\n\n### PR Section Analyzer\n\n```bash\n# Analyze a specific PR\npython pr_section_analyzer_final.py --pr 1533\n\n# Analyze all PRs (limited to 100 by default)\npython pr_section_analyzer_final.py --all\n\n# Analyze all PRs with a custom limit\npython pr_section_analyzer_final.py --all --limit 50\n\n# Output results in JSON format\npython pr_section_analyzer_final.py --pr 1533 --format json\n\n# Save results to a file\npython pr_section_analyzer_final.py --all --output report.txt\n```\n\n### Education PR Analyzer\n\n```bash\n# Analyze education-labeled PRs (limited to 20 by default)\npython education_pr_analyzer.py\n\n# Analyze with a custom limit\npython education_pr_analyzer.py --limit 10\n\n# Save results to a file\npython education_pr_analyzer.py --output education_report.md\n```\n\n## Example Output\n\nThe education PR analyzer generates a comprehensive report showing:\n\n1. Which PRs modify the same sections:\n\n```markdown\n### 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md\n#### ï¼”ï¼‰è²§å›°ä¸–å¸¯ã®å­ã©ã‚‚ãŸã¡ãƒ»ä¿è­·è€…ã®çš†æ§˜ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã¨AIã‚’é§†ä½¿ã—ã€ãƒ—ãƒƒã‚·ãƒ¥å‹ã®æ”¯æ´ã‚’å®Ÿç¾ã—ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\nã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¤‰æ›´ã™ã‚‹PR:\n- PR #1462: [SAISã«ãŠã‘ã‚‹ç§‘å­¦æŠ€è¡“ã‚³ãƒ³ãƒ†ã‚¹ãƒˆå‚åŠ æ”¯æ´ã®æ˜è¨˜](https://github.com/team-mirai/policy/pull/1462)\n- PR #1460: [æ•™è‚²æ”¿ç­–ã«ãŠã‘ã‚‹å€‹åˆ¥æœ€é©åŒ–ã¨ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ã®å¼·åŒ–ï¼ˆåŒ¿åãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆï¼‰](https://github.com/team-mirai/policy/pull/1460)\n```\n\n2. Which sections are modified by each PR:\n\n```markdown\n### PR #1335: æ•™è‚²æ”¿ç­–ã®æ”¹å–„æ¡ˆï¼šAIå®¶åº­æ•™å¸«ã®å¯¾è±¡æ‹¡å¤§ã¨ãƒ—ãƒƒã‚·ãƒ¥å‹æ”¯æ´ã®å¼·åŒ–ï¼ˆã™ã ã•ã‚“ã”ææ¡ˆï¼‰\n- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼‘ï¼æ•™è‚² > ãƒ“ã‚¸ãƒ§ãƒ³\n- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼”ï¼‰è²§å›°ä¸–å¸¯ã®å­ã©ã‚‚ãŸã¡ãƒ»ä¿è­·è€…ã®çš†æ§˜ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã¨AIã‚’é§†ä½¿ã—ã€ãƒ—ãƒƒã‚·ãƒ¥å‹ã®æ”¯æ´ã‚’å®Ÿç¾ã—ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼‘ï¼‰ã™ã¹ã¦ã®å­ã©ã‚‚ã«ã€Œå°‚å±ã®AIå®¶åº­æ•™å¸«ã€ã‚’å±Šã‘ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n```\n\nA sample report is included in the PR: `education_pr_report_final.md`\n\n## Link to Devin run\nhttps://app.devin.ai/sessions/4d02a46b92954a4d8978a5f005ddc511\n\nRequested by: NISHIO Hirokazu (nishio.hirokazu@gmail.com)\n",
    "created_at": "2025-05-24T18:45:15Z",
    "updated_at": "2025-05-24T19:54:12Z",
    "closed_at": null,
    "merged_at": null,
    "merge_commit_sha": "ef00c0c9d11aa38e8687f677f2a3753baa1efa4d",
    "assignee": null,
    "assignees": [],
    "requested_reviewers": [],
    "requested_teams": [],
    "labels": [],
    "milestone": null,
    "draft": true,
    "commits_url": "https://api.github.com/repos/team-mirai/policy/pulls/1605/commits",
    "review_comments_url": "https://api.github.com/repos/team-mirai/policy/pulls/1605/comments",
    "review_comment_url": "https://api.github.com/repos/team-mirai/policy/pulls/comments{/number}",
    "comments_url": "https://api.github.com/repos/team-mirai/policy/issues/1605/comments",
    "statuses_url": "https://api.github.com/repos/team-mirai/policy/statuses/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
    "head": {
      "label": "team-mirai:devin/1748111368-pr-section-analyzer",
      "ref": "devin/1748111368-pr-section-analyzer",
      "sha": "1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
      "user": {
        "login": "team-mirai",
        "id": 210232249,
        "node_id": "O_kgDODIfjuQ",
        "avatar_url": "https://avatars.githubusercontent.com/u/210232249?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/team-mirai",
        "html_url": "https://github.com/team-mirai",
        "followers_url": "https://api.github.com/users/team-mirai/followers",
        "following_url": "https://api.github.com/users/team-mirai/following{/other_user}",
        "gists_url": "https://api.github.com/users/team-mirai/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/team-mirai/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/team-mirai/subscriptions",
        "organizations_url": "https://api.github.com/users/team-mirai/orgs",
        "repos_url": "https://api.github.com/users/team-mirai/repos",
        "events_url": "https://api.github.com/users/team-mirai/events{/privacy}",
        "received_events_url": "https://api.github.com/users/team-mirai/received_events",
        "type": "Organization",
        "user_view_type": "public",
        "site_admin": false
      },
      "repo": {
        "id": 983878076,
        "node_id": "R_kgDOOqTJvA",
        "name": "policy",
        "full_name": "team-mirai/policy",
        "private": false,
        "owner": {
          "login": "team-mirai",
          "id": 210232249,
          "node_id": "O_kgDODIfjuQ",
          "avatar_url": "https://avatars.githubusercontent.com/u/210232249?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/team-mirai",
          "html_url": "https://github.com/team-mirai",
          "followers_url": "https://api.github.com/users/team-mirai/followers",
          "following_url": "https://api.github.com/users/team-mirai/following{/other_user}",
          "gists_url": "https://api.github.com/users/team-mirai/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/team-mirai/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/team-mirai/subscriptions",
          "organizations_url": "https://api.github.com/users/team-mirai/orgs",
          "repos_url": "https://api.github.com/users/team-mirai/repos",
          "events_url": "https://api.github.com/users/team-mirai/events{/privacy}",
          "received_events_url": "https://api.github.com/users/team-mirai/received_events",
          "type": "Organization",
          "user_view_type": "public",
          "site_admin": false
        },
        "html_url": "https://github.com/team-mirai/policy",
        "description": null,
        "fork": false,
        "url": "https://api.github.com/repos/team-mirai/policy",
        "forks_url": "https://api.github.com/repos/team-mirai/policy/forks",
        "keys_url": "https://api.github.com/repos/team-mirai/policy/keys{/key_id}",
        "collaborators_url": "https://api.github.com/repos/team-mirai/policy/collaborators{/collaborator}",
        "teams_url": "https://api.github.com/repos/team-mirai/policy/teams",
        "hooks_url": "https://api.github.com/repos/team-mirai/policy/hooks",
        "issue_events_url": "https://api.github.com/repos/team-mirai/policy/issues/events{/number}",
        "events_url": "https://api.github.com/repos/team-mirai/policy/events",
        "assignees_url": "https://api.github.com/repos/team-mirai/policy/assignees{/user}",
        "branches_url": "https://api.github.com/repos/team-mirai/policy/branches{/branch}",
        "tags_url": "https://api.github.com/repos/team-mirai/policy/tags",
        "blobs_url": "https://api.github.com/repos/team-mirai/policy/git/blobs{/sha}",
        "git_tags_url": "https://api.github.com/repos/team-mirai/policy/git/tags{/sha}",
        "git_refs_url": "https://api.github.com/repos/team-mirai/policy/git/refs{/sha}",
        "trees_url": "https://api.github.com/repos/team-mirai/policy/git/trees{/sha}",
        "statuses_url": "https://api.github.com/repos/team-mirai/policy/statuses/{sha}",
        "languages_url": "https://api.github.com/repos/team-mirai/policy/languages",
        "stargazers_url": "https://api.github.com/repos/team-mirai/policy/stargazers",
        "contributors_url": "https://api.github.com/repos/team-mirai/policy/contributors",
        "subscribers_url": "https://api.github.com/repos/team-mirai/policy/subscribers",
        "subscription_url": "https://api.github.com/repos/team-mirai/policy/subscription",
        "commits_url": "https://api.github.com/repos/team-mirai/policy/commits{/sha}",
        "git_commits_url": "https://api.github.com/repos/team-mirai/policy/git/commits{/sha}",
        "comments_url": "https://api.github.com/repos/team-mirai/policy/comments{/number}",
        "issue_comment_url": "https://api.github.com/repos/team-mirai/policy/issues/comments{/number}",
        "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/{+path}",
        "compare_url": "https://api.github.com/repos/team-mirai/policy/compare/{base}...{head}",
        "merges_url": "https://api.github.com/repos/team-mirai/policy/merges",
        "archive_url": "https://api.github.com/repos/team-mirai/policy/{archive_format}{/ref}",
        "downloads_url": "https://api.github.com/repos/team-mirai/policy/downloads",
        "issues_url": "https://api.github.com/repos/team-mirai/policy/issues{/number}",
        "pulls_url": "https://api.github.com/repos/team-mirai/policy/pulls{/number}",
        "milestones_url": "https://api.github.com/repos/team-mirai/policy/milestones{/number}",
        "notifications_url": "https://api.github.com/repos/team-mirai/policy/notifications{?since,all,participating}",
        "labels_url": "https://api.github.com/repos/team-mirai/policy/labels{/name}",
        "releases_url": "https://api.github.com/repos/team-mirai/policy/releases{/id}",
        "deployments_url": "https://api.github.com/repos/team-mirai/policy/deployments",
        "created_at": "2025-05-15T04:00:23Z",
        "updated_at": "2025-05-24T15:59:18Z",
        "pushed_at": "2025-05-24T19:57:49Z",
        "git_url": "git://github.com/team-mirai/policy.git",
        "ssh_url": "git@github.com:team-mirai/policy.git",
        "clone_url": "https://github.com/team-mirai/policy.git",
        "svn_url": "https://github.com/team-mirai/policy",
        "homepage": null,
        "size": 6044,
        "stargazers_count": 129,
        "watchers_count": 129,
        "language": "TypeScript",
        "has_issues": true,
        "has_projects": true,
        "has_downloads": true,
        "has_wiki": true,
        "has_pages": false,
        "has_discussions": false,
        "forks_count": 38,
        "mirror_url": null,
        "archived": false,
        "disabled": false,
        "open_issues_count": 1553,
        "license": {
          "key": "cc-by-4.0",
          "name": "Creative Commons Attribution 4.0 International",
          "spdx_id": "CC-BY-4.0",
          "url": "https://api.github.com/licenses/cc-by-4.0",
          "node_id": "MDc6TGljZW5zZTI1"
        },
        "allow_forking": true,
        "is_template": false,
        "web_commit_signoff_required": false,
        "topics": [],
        "visibility": "public",
        "forks": 38,
        "open_issues": 1553,
        "watchers": 129,
        "default_branch": "main"
      }
    },
    "base": {
      "label": "team-mirai:main",
      "ref": "main",
      "sha": "172c39b3b74735a08df9da768d5042bd12222794",
      "user": {
        "login": "team-mirai",
        "id": 210232249,
        "node_id": "O_kgDODIfjuQ",
        "avatar_url": "https://avatars.githubusercontent.com/u/210232249?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/team-mirai",
        "html_url": "https://github.com/team-mirai",
        "followers_url": "https://api.github.com/users/team-mirai/followers",
        "following_url": "https://api.github.com/users/team-mirai/following{/other_user}",
        "gists_url": "https://api.github.com/users/team-mirai/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/team-mirai/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/team-mirai/subscriptions",
        "organizations_url": "https://api.github.com/users/team-mirai/orgs",
        "repos_url": "https://api.github.com/users/team-mirai/repos",
        "events_url": "https://api.github.com/users/team-mirai/events{/privacy}",
        "received_events_url": "https://api.github.com/users/team-mirai/received_events",
        "type": "Organization",
        "user_view_type": "public",
        "site_admin": false
      },
      "repo": {
        "id": 983878076,
        "node_id": "R_kgDOOqTJvA",
        "name": "policy",
        "full_name": "team-mirai/policy",
        "private": false,
        "owner": {
          "login": "team-mirai",
          "id": 210232249,
          "node_id": "O_kgDODIfjuQ",
          "avatar_url": "https://avatars.githubusercontent.com/u/210232249?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/team-mirai",
          "html_url": "https://github.com/team-mirai",
          "followers_url": "https://api.github.com/users/team-mirai/followers",
          "following_url": "https://api.github.com/users/team-mirai/following{/other_user}",
          "gists_url": "https://api.github.com/users/team-mirai/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/team-mirai/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/team-mirai/subscriptions",
          "organizations_url": "https://api.github.com/users/team-mirai/orgs",
          "repos_url": "https://api.github.com/users/team-mirai/repos",
          "events_url": "https://api.github.com/users/team-mirai/events{/privacy}",
          "received_events_url": "https://api.github.com/users/team-mirai/received_events",
          "type": "Organization",
          "user_view_type": "public",
          "site_admin": false
        },
        "html_url": "https://github.com/team-mirai/policy",
        "description": null,
        "fork": false,
        "url": "https://api.github.com/repos/team-mirai/policy",
        "forks_url": "https://api.github.com/repos/team-mirai/policy/forks",
        "keys_url": "https://api.github.com/repos/team-mirai/policy/keys{/key_id}",
        "collaborators_url": "https://api.github.com/repos/team-mirai/policy/collaborators{/collaborator}",
        "teams_url": "https://api.github.com/repos/team-mirai/policy/teams",
        "hooks_url": "https://api.github.com/repos/team-mirai/policy/hooks",
        "issue_events_url": "https://api.github.com/repos/team-mirai/policy/issues/events{/number}",
        "events_url": "https://api.github.com/repos/team-mirai/policy/events",
        "assignees_url": "https://api.github.com/repos/team-mirai/policy/assignees{/user}",
        "branches_url": "https://api.github.com/repos/team-mirai/policy/branches{/branch}",
        "tags_url": "https://api.github.com/repos/team-mirai/policy/tags",
        "blobs_url": "https://api.github.com/repos/team-mirai/policy/git/blobs{/sha}",
        "git_tags_url": "https://api.github.com/repos/team-mirai/policy/git/tags{/sha}",
        "git_refs_url": "https://api.github.com/repos/team-mirai/policy/git/refs{/sha}",
        "trees_url": "https://api.github.com/repos/team-mirai/policy/git/trees{/sha}",
        "statuses_url": "https://api.github.com/repos/team-mirai/policy/statuses/{sha}",
        "languages_url": "https://api.github.com/repos/team-mirai/policy/languages",
        "stargazers_url": "https://api.github.com/repos/team-mirai/policy/stargazers",
        "contributors_url": "https://api.github.com/repos/team-mirai/policy/contributors",
        "subscribers_url": "https://api.github.com/repos/team-mirai/policy/subscribers",
        "subscription_url": "https://api.github.com/repos/team-mirai/policy/subscription",
        "commits_url": "https://api.github.com/repos/team-mirai/policy/commits{/sha}",
        "git_commits_url": "https://api.github.com/repos/team-mirai/policy/git/commits{/sha}",
        "comments_url": "https://api.github.com/repos/team-mirai/policy/comments{/number}",
        "issue_comment_url": "https://api.github.com/repos/team-mirai/policy/issues/comments{/number}",
        "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/{+path}",
        "compare_url": "https://api.github.com/repos/team-mirai/policy/compare/{base}...{head}",
        "merges_url": "https://api.github.com/repos/team-mirai/policy/merges",
        "archive_url": "https://api.github.com/repos/team-mirai/policy/{archive_format}{/ref}",
        "downloads_url": "https://api.github.com/repos/team-mirai/policy/downloads",
        "issues_url": "https://api.github.com/repos/team-mirai/policy/issues{/number}",
        "pulls_url": "https://api.github.com/repos/team-mirai/policy/pulls{/number}",
        "milestones_url": "https://api.github.com/repos/team-mirai/policy/milestones{/number}",
        "notifications_url": "https://api.github.com/repos/team-mirai/policy/notifications{?since,all,participating}",
        "labels_url": "https://api.github.com/repos/team-mirai/policy/labels{/name}",
        "releases_url": "https://api.github.com/repos/team-mirai/policy/releases{/id}",
        "deployments_url": "https://api.github.com/repos/team-mirai/policy/deployments",
        "created_at": "2025-05-15T04:00:23Z",
        "updated_at": "2025-05-24T15:59:18Z",
        "pushed_at": "2025-05-24T19:57:49Z",
        "git_url": "git://github.com/team-mirai/policy.git",
        "ssh_url": "git@github.com:team-mirai/policy.git",
        "clone_url": "https://github.com/team-mirai/policy.git",
        "svn_url": "https://github.com/team-mirai/policy",
        "homepage": null,
        "size": 6044,
        "stargazers_count": 129,
        "watchers_count": 129,
        "language": "TypeScript",
        "has_issues": true,
        "has_projects": true,
        "has_downloads": true,
        "has_wiki": true,
        "has_pages": false,
        "has_discussions": false,
        "forks_count": 38,
        "mirror_url": null,
        "archived": false,
        "disabled": false,
        "open_issues_count": 1553,
        "license": {
          "key": "cc-by-4.0",
          "name": "Creative Commons Attribution 4.0 International",
          "spdx_id": "CC-BY-4.0",
          "url": "https://api.github.com/licenses/cc-by-4.0",
          "node_id": "MDc6TGljZW5zZTI1"
        },
        "allow_forking": true,
        "is_template": false,
        "web_commit_signoff_required": false,
        "topics": [],
        "visibility": "public",
        "forks": 38,
        "open_issues": 1553,
        "watchers": 129,
        "default_branch": "main"
      }
    },
    "_links": {
      "self": {
        "href": "https://api.github.com/repos/team-mirai/policy/pulls/1605"
      },
      "html": {
        "href": "https://github.com/team-mirai/policy/pull/1605"
      },
      "issue": {
        "href": "https://api.github.com/repos/team-mirai/policy/issues/1605"
      },
      "comments": {
        "href": "https://api.github.com/repos/team-mirai/policy/issues/1605/comments"
      },
      "review_comments": {
        "href": "https://api.github.com/repos/team-mirai/policy/pulls/1605/comments"
      },
      "review_comment": {
        "href": "https://api.github.com/repos/team-mirai/policy/pulls/comments{/number}"
      },
      "commits": {
        "href": "https://api.github.com/repos/team-mirai/policy/pulls/1605/commits"
      },
      "statuses": {
        "href": "https://api.github.com/repos/team-mirai/policy/statuses/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35"
      }
    },
    "author_association": "CONTRIBUTOR",
    "auto_merge": null,
    "active_lock_reason": null,
    "merged": false,
    "mergeable": true,
    "rebaseable": true,
    "mergeable_state": "clean",
    "merged_by": null,
    "comments": 2,
    "review_comments": 0,
    "maintainer_can_modify": false,
    "commits": 4,
    "additions": 2047,
    "deletions": 0,
    "changed_files": 6
  },
  "state": "open",
  "updated_at": "2025-05-24T19:54:12Z",
  "comments": [
    {
      "url": "https://api.github.com/repos/team-mirai/policy/issues/comments/2906972687",
      "html_url": "https://github.com/team-mirai/policy/pull/1605#issuecomment-2906972687",
      "issue_url": "https://api.github.com/repos/team-mirai/policy/issues/1605",
      "id": 2906972687,
      "node_id": "IC_kwDOOqTJvM6tROIP",
      "user": {
        "login": "devin-ai-integration[bot]",
        "id": 158243242,
        "node_id": "BOT_kgDOCW6Zqg",
        "avatar_url": "https://avatars.githubusercontent.com/in/811515?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D",
        "html_url": "https://github.com/apps/devin-ai-integration",
        "followers_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/followers",
        "following_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/following{/other_user}",
        "gists_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/subscriptions",
        "organizations_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/orgs",
        "repos_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/repos",
        "events_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/events{/privacy}",
        "received_events_url": "https://api.github.com/users/devin-ai-integration%5Bbot%5D/received_events",
        "type": "Bot",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2025-05-24T18:45:19Z",
      "updated_at": "2025-05-24T18:45:19Z",
      "author_association": "CONTRIBUTOR",
      "body": "### ğŸ¤– Devin AI Engineer\n\nI'll be helping with this pull request! Here's what you should know:\n\nâœ… I will automatically:\n- Address comments on this PR. Add '(aside)' to your comment to have me ignore it.\n- Look at CI failures and help fix them\n\nNote: I can only respond to comments from users who have write access to this repository.\n\nâš™ï¸ Control Options:\n- [ ] Disable automatic comment and CI monitoring\n",
      "reactions": {
        "url": "https://api.github.com/repos/team-mirai/policy/issues/comments/2906972687/reactions",
        "total_count": 0,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 0
      },
      "performed_via_github_app": {
        "id": 811515,
        "client_id": "Iv1.fffb955bc006997f",
        "slug": "devin-ai-integration",
        "node_id": "A_kwHOCQk3ds4ADGH7",
        "owner": {
          "login": "usacognition",
          "id": 151598966,
          "node_id": "O_kgDOCQk3dg",
          "avatar_url": "https://avatars.githubusercontent.com/u/151598966?v=4",
          "gravatar_id": "",
          "url": "https://api.github.com/users/usacognition",
          "html_url": "https://github.com/usacognition",
          "followers_url": "https://api.github.com/users/usacognition/followers",
          "following_url": "https://api.github.com/users/usacognition/following{/other_user}",
          "gists_url": "https://api.github.com/users/usacognition/gists{/gist_id}",
          "starred_url": "https://api.github.com/users/usacognition/starred{/owner}{/repo}",
          "subscriptions_url": "https://api.github.com/users/usacognition/subscriptions",
          "organizations_url": "https://api.github.com/users/usacognition/orgs",
          "repos_url": "https://api.github.com/users/usacognition/repos",
          "events_url": "https://api.github.com/users/usacognition/events{/privacy}",
          "received_events_url": "https://api.github.com/users/usacognition/received_events",
          "type": "Organization",
          "user_view_type": "public",
          "site_admin": false
        },
        "name": "Devin.ai Integration",
        "description": "This Github App integration allows Devin (devin.ai) to have limited authenticated access to your organization's code repositories.",
        "external_url": "https://devin.ai",
        "html_url": "https://github.com/apps/devin-ai-integration",
        "created_at": "2024-01-31T03:16:04Z",
        "updated_at": "2025-05-14T06:48:07Z",
        "permissions": {
          "actions": "read",
          "checks": "read",
          "contents": "write",
          "deployments": "read",
          "discussions": "write",
          "emails": "read",
          "issues": "write",
          "members": "read",
          "metadata": "read",
          "packages": "read",
          "pages": "read",
          "pull_requests": "write",
          "repository_advisories": "read",
          "repository_hooks": "read",
          "repository_projects": "read",
          "statuses": "read",
          "vulnerability_alerts": "read",
          "workflows": "write"
        },
        "events": [
          "check_run",
          "issue_comment",
          "pull_request",
          "pull_request_review",
          "pull_request_review_comment",
          "repository",
          "status"
        ]
      }
    },
    {
      "url": "https://api.github.com/repos/team-mirai/policy/issues/comments/2906997703",
      "html_url": "https://github.com/team-mirai/policy/pull/1605#issuecomment-2906997703",
      "issue_url": "https://api.github.com/repos/team-mirai/policy/issues/1605",
      "id": 2906997703,
      "node_id": "IC_kwDOOqTJvM6tRUPH",
      "user": {
        "login": "nishio",
        "id": 315198,
        "node_id": "MDQ6VXNlcjMxNTE5OA==",
        "avatar_url": "https://avatars.githubusercontent.com/u/315198?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/nishio",
        "html_url": "https://github.com/nishio",
        "followers_url": "https://api.github.com/users/nishio/followers",
        "following_url": "https://api.github.com/users/nishio/following{/other_user}",
        "gists_url": "https://api.github.com/users/nishio/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/nishio/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/nishio/subscriptions",
        "organizations_url": "https://api.github.com/users/nishio/orgs",
        "repos_url": "https://api.github.com/users/nishio/repos",
        "events_url": "https://api.github.com/users/nishio/events{/privacy}",
        "received_events_url": "https://api.github.com/users/nishio/received_events",
        "type": "User",
        "user_view_type": "public",
        "site_admin": false
      },
      "created_at": "2025-05-24T19:54:12Z",
      "updated_at": "2025-05-24T19:54:12Z",
      "author_association": "NONE",
      "body": "ã“ã‚Œã¯å¾Œã§random repoã«ç§»å‹•ã—ã¾ã™",
      "reactions": {
        "url": "https://api.github.com/repos/team-mirai/policy/issues/comments/2906997703/reactions",
        "total_count": 1,
        "+1": 0,
        "-1": 0,
        "laugh": 0,
        "hooray": 0,
        "confused": 0,
        "heart": 0,
        "rocket": 0,
        "eyes": 1
      },
      "performed_via_github_app": null
    }
  ],
  "review_comments": [],
  "files": [
    {
      "sha": "85c4313def35c635569fe7fd0e331fc7d0eaf486",
      "filename": "education_pr_analyzer.py",
      "status": "added",
      "additions": 373,
      "deletions": 0,
      "changes": 373,
      "blob_url": "https://github.com/team-mirai/policy/blob/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/education_pr_analyzer.py",
      "raw_url": "https://github.com/team-mirai/policy/raw/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/education_pr_analyzer.py",
      "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/education_pr_analyzer.py?ref=1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
      "patch": "@@ -0,0 +1,373 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Script to analyze PRs with the education label and generate a Markdown report\n+of which sections are modified by each PR.\n+\"\"\"\n+\n+import argparse\n+import json\n+import os\n+import re\n+import subprocess\n+import sys\n+from collections import defaultdict\n+from datetime import datetime\n+\n+def run_command(command):\n+    \"\"\"Run a shell command and return the output.\"\"\"\n+    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n+    if result.returncode != 0:\n+        print(f\"Error running command: {command}\")\n+        print(f\"Error: {result.stderr}\")\n+        return \"\"\n+    return result.stdout.strip()\n+\n+def get_education_prs():\n+    \"\"\"Get all PRs with the education label.\"\"\"\n+    open_prs_cmd = 'gh pr list --label \"æ•™è‚²\" --state open --json number,title,url'\n+    open_prs_output = run_command(open_prs_cmd)\n+    \n+    closed_prs_cmd = 'gh pr list --label \"æ•™è‚²\" --state closed --json number,title,url'\n+    closed_prs_output = run_command(closed_prs_cmd)\n+    \n+    open_prs = json.loads(open_prs_output) if open_prs_output else []\n+    closed_prs = json.loads(closed_prs_output) if closed_prs_output else []\n+    \n+    all_prs = open_prs + closed_prs\n+    \n+    return all_prs\n+\n+def extract_markdown_sections(file_path):\n+    \"\"\"Extract markdown sections from a file with improved Japanese section handling.\"\"\"\n+    if not os.path.exists(file_path):\n+        print(f\"File not found: {file_path}\")\n+        return {}\n+    \n+    with open(file_path, 'r', encoding='utf-8') as f:\n+        content = f.read()\n+    \n+    print(f\"Extracting sections from file with {len(content.split('\\n'))} lines\")\n+    \n+    headings = []\n+    section_content = {}\n+    current_section_start = None\n+    \n+    for i, line in enumerate(content.split('\\n')):\n+        heading_match = re.match(r'^(#+)\\s+(.+)$', line)\n+        \n+        jp_section_match = None\n+        if not heading_match:\n+            jp_section_match = re.match(r'^([ï¼-ï¼™]+ï¼‰|\\d+ï¼‰)\\s*(.+)$', line)\n+        \n+        jp_policy_match = None\n+        if not heading_match and not jp_section_match:\n+            jp_policy_match = re.match(r'^###\\s+(ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ|æ”¿ç­–æ¦‚è¦)$', line)\n+        \n+        if heading_match:\n+            level = len(heading_match.group(1))\n+            title = heading_match.group(2).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found heading at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_section_match:\n+            level = 2\n+            title = jp_section_match.group(0).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_policy_match:\n+            level = 3\n+            title = jp_policy_match.group(1).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP policy section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+    \n+    if current_section_start is not None and len(content.split('\\n')) > 0:\n+        section_content[current_section_start] = (current_section_start, len(content.split('\\n')))\n+    \n+    sections = {}\n+    \n+    sorted_headings = sorted(headings, key=lambda x: x[0])\n+    \n+    for i, (line_num, level, title) in enumerate(sorted_headings):\n+        parent_sections = []\n+        current_parent_level = 0\n+        \n+        for j in range(i-1, -1, -1):\n+            prev_line, prev_level, prev_title = sorted_headings[j]\n+            \n+            if prev_level < level and prev_level > current_parent_level:\n+                parent_sections.insert(0, prev_title)\n+                current_parent_level = prev_level\n+                \n+                if prev_level == 1:\n+                    break\n+        \n+        section_path = \" > \".join(parent_sections + [title])\n+        \n+        content_range = section_content.get(line_num, (line_num, line_num))\n+        \n+        sections[line_num] = {\n+            \"level\": level,\n+            \"title\": title,\n+            \"path\": section_path,\n+            \"parent_sections\": parent_sections,\n+            \"start_line\": content_range[0],\n+            \"end_line\": content_range[1]\n+        }\n+    \n+    return sections\n+\n+def find_section_for_line(sections, line_number):\n+    \"\"\"Find the section that contains a specific line.\"\"\"\n+    for section_line, section_info in sections.items():\n+        if section_info[\"start_line\"] <= line_number <= section_info[\"end_line\"]:\n+            return section_info\n+    \n+    section_lines = sorted(sections.keys())\n+    for i, section_line in enumerate(section_lines):\n+        if section_line > line_number:\n+            if i > 0:\n+                return sections[section_lines[i-1]]\n+            break\n+    \n+    if section_lines:\n+        return sections[section_lines[-1]]\n+    \n+    return None\n+\n+def get_file_diff(pr_number, file_path):\n+    \"\"\"Get the diff for a specific file in a PR using git commands.\"\"\"\n+    command = f\"gh pr view {pr_number} --json headRefName\"\n+    output = run_command(command)\n+    try:\n+        data = json.loads(output)\n+        branch_name = data.get('headRefName')\n+        if not branch_name:\n+            print(f\"Could not get branch name for PR #{pr_number}\")\n+            return \"\"\n+        \n+        fetch_cmd = f\"git fetch origin {branch_name}\"\n+        run_command(fetch_cmd)\n+        \n+        diff_cmd = f\"git diff main..origin/{branch_name} -- '{file_path}'\"\n+        return run_command(diff_cmd)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON for PR branch: {output}\")\n+        return \"\"\n+\n+def extract_line_numbers_from_diff(diff_text):\n+    \"\"\"Extract line numbers from a diff.\"\"\"\n+    line_numbers = []\n+    current_line = None\n+    \n+    for line in diff_text.split('\\n'):\n+        if line.startswith('@@'):\n+            match = re.search(r'@@ -\\d+(?:,\\d+)? \\+(\\d+)(?:,(\\d+))? @@', line)\n+            if match:\n+                current_line = int(match.group(1))\n+                line_count = int(match.group(2)) if match.group(2) else 1\n+        elif current_line is not None:\n+            if line.startswith('+') and not line.startswith('+++'):\n+                line_numbers.append(current_line)\n+                current_line += 1\n+            elif line.startswith('-'):\n+                pass\n+            else:\n+                current_line += 1\n+    \n+    return line_numbers\n+\n+def get_pr_details(pr_number):\n+    \"\"\"Get details for a specific PR.\"\"\"\n+    cmd = f'gh pr view {pr_number} --json number,title,url,body'\n+    output = run_command(cmd)\n+    if not output:\n+        return None\n+    \n+    pr_data = json.loads(output)\n+    return pr_data\n+\n+def get_pr_files(pr_number):\n+    \"\"\"Get the files changed in a PR.\"\"\"\n+    cmd = f'gh pr view {pr_number} --json files'\n+    output = run_command(cmd)\n+    if not output:\n+        return []\n+    \n+    pr_data = json.loads(output)\n+    return [file['path'] for file in pr_data.get('files', [])]\n+\n+def analyze_pr(pr_number):\n+    \"\"\"Analyze a PR and extract the sections it modifies.\"\"\"\n+    pr_details = get_pr_details(pr_number)\n+    if not pr_details:\n+        print(f\"Failed to get details for PR #{pr_number}\")\n+        return None\n+    \n+    pr_files = get_pr_files(pr_number)\n+    if not pr_files:\n+        print(f\"No files found for PR #{pr_number}\")\n+        return None\n+    \n+    education_files = [f for f in pr_files if f.endswith('.md') and (\n+        f.startswith('11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚²') or \n+        f.startswith('21_ã‚¹ãƒ†ãƒƒãƒ—ï¼’æ•™è‚²') or \n+        f.startswith('32_ã‚¹ãƒ†ãƒƒãƒ—ï¼“æ•™è‚²')\n+    )]\n+    \n+    if not education_files:\n+        print(f\"No education files found for PR #{pr_number}\")\n+        return None\n+    \n+    results = []\n+    \n+    for file_path in education_files:\n+        print(f\"Processing file: {file_path}\")\n+        full_path = os.path.join(os.getcwd(), file_path)\n+        \n+        if not os.path.exists(full_path):\n+            print(f\"Warning: File {full_path} does not exist, skipping\")\n+            continue\n+        \n+        file_diff = get_file_diff(pr_number, file_path)\n+        \n+        if not file_diff:\n+            print(f\"No diff found for file {file_path}\")\n+            continue\n+        \n+        print(f\"Found diff for file {file_path}, length: {len(file_diff)}\")\n+        \n+        line_numbers = extract_line_numbers_from_diff(file_diff)\n+        \n+        if not line_numbers:\n+            print(f\"No line numbers found in diff for file {file_path}\")\n+            continue\n+        \n+        print(f\"Affected line numbers: {line_numbers[:10]}...\")\n+        \n+        sections = extract_markdown_sections(full_path)\n+        print(f\"Found {len(sections)} sections in {file_path}\")\n+        \n+        if len(sections) > 0:\n+            print(f\"Section titles: {[s['title'] for s in sections.values()][:5]}\")\n+        \n+        affected_sections = set()\n+        for line_number in line_numbers:\n+            section = find_section_for_line(sections, line_number)\n+            if section:\n+                print(f\"Line {line_number} is in section: {section['title']}\")\n+                affected_sections.add((section['title'], section['path']))\n+        \n+        for section_title, section_path in affected_sections:\n+            results.append({\n+                'pr_number': pr_number,\n+                'pr_title': pr_details.get('title', ''),\n+                'pr_url': pr_details.get('url', ''),\n+                'file': file_path,\n+                'section': section_title,\n+                'section_path': section_path,\n+                'changes': [line for line in file_diff.split('\\n') if line.startswith('+') and not line.startswith('+++')]\n+            })\n+    \n+    print(f\"Found {len(results)} affected sections\")\n+    return results\n+\n+def generate_markdown_report(pr_analyses):\n+    \"\"\"Generate a Markdown report from the PR analyses.\"\"\"\n+    if not pr_analyses:\n+        return \"# æ•™è‚²é–¢é€£PRã®åˆ†æ\\n\\nåˆ†æå¯¾è±¡ã®PRãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\"\n+    \n+    sections_to_prs = defaultdict(list)\n+    for pr in pr_analyses:\n+        for result in pr.get(\"results\", []):\n+            section_key = f\"{result['file']}: {result['section_path']}\"\n+            sections_to_prs[section_key].append({\n+                \"number\": result['pr_number'],\n+                \"title\": result['pr_title'],\n+                \"url\": result['pr_url']\n+            })\n+    \n+    report = \"# æ•™è‚²é–¢é€£PRã®åˆ†æ\\n\\n\"\n+    report += f\"åˆ†ææ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n+    \n+    report += \"## ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥PRä¸€è¦§\\n\\n\"\n+    for section_key, prs in sections_to_prs.items():\n+        if len(prs) > 1:  # Only show sections with multiple PRs\n+            file_path, section_path = section_key.split(\": \", 1)\n+            report += f\"### {file_path}\\n\"\n+            report += f\"#### {section_path}\\n\"\n+            report += \"ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¤‰æ›´ã™ã‚‹PR:\\n\"\n+            for pr in prs:\n+                report += f\"- PR #{pr['number']}: [{pr['title']}]({pr['url']})\\n\"\n+            report += \"\\n\"\n+    \n+    report += \"## PRåˆ¥å¤‰æ›´ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§\\n\\n\"\n+    pr_to_sections = defaultdict(list)\n+    for pr in pr_analyses:\n+        for result in pr.get(\"results\", []):\n+            pr_key = f\"{result['pr_number']}:{result['pr_title']}\"\n+            section_info = {\n+                'file': result['file'],\n+                'section_path': result['section_path']\n+            }\n+            pr_to_sections[pr_key].append(section_info)\n+    \n+    for pr_key, sections in sorted(pr_to_sections.items()):\n+        pr_number, pr_title = pr_key.split(\":\", 1)\n+        report += f\"### PR #{pr_number}: {pr_title}\\n\"\n+        \n+        for section in sections:\n+            report += f\"- {section['file']}: {section['section_path']}\\n\"\n+        \n+        report += \"\\n\"\n+    \n+    return report\n+\n+def main():\n+    parser = argparse.ArgumentParser(description=\"Analyze PRs with the education label\")\n+    parser.add_argument(\"--output\", help=\"Output file for the Markdown report\")\n+    parser.add_argument(\"--limit\", type=int, default=20, help=\"Limit the number of PRs to analyze\")\n+    args = parser.parse_args()\n+    \n+    education_prs = get_education_prs()\n+    print(f\"Found {len(education_prs)} PRs with the education label\")\n+    \n+    education_prs = education_prs[:args.limit]\n+    \n+    pr_analyses = []\n+    for pr in education_prs:\n+        pr_number = pr[\"number\"]\n+        print(f\"\\nAnalyzing PR #{pr_number}: {pr['title']}\")\n+        results = analyze_pr(pr_number)\n+        if results:\n+            pr_analyses.append({\n+                \"pr_number\": pr_number,\n+                \"pr_title\": pr['title'],\n+                \"pr_url\": pr['url'],\n+                \"results\": results\n+            })\n+    \n+    report = generate_markdown_report(pr_analyses)\n+    \n+    if args.output:\n+        with open(args.output, \"w\", encoding=\"utf-8\") as f:\n+            f.write(report)\n+        print(f\"Report written to {args.output}\")\n+    else:\n+        print(\"\\n\" + \"=\"*80 + \"\\n\")\n+        print(report)\n+\n+if __name__ == \"__main__\":\n+    main()"
    },
    {
      "sha": "1c16ea171d0118085d39a0f00f0c57dc549979af",
      "filename": "education_pr_report_final.md",
      "status": "added",
      "additions": 88,
      "deletions": 0,
      "changes": 88,
      "blob_url": "https://github.com/team-mirai/policy/blob/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/education_pr_report_final.md",
      "raw_url": "https://github.com/team-mirai/policy/raw/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/education_pr_report_final.md",
      "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/education_pr_report_final.md?ref=1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
      "patch": "@@ -0,0 +1,88 @@\n+# æ•™è‚²é–¢é€£PRã®åˆ†æ\n+\n+åˆ†ææ—¥æ™‚: 2025-05-24 19:13:06\n+\n+## ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥PRä¸€è¦§\n+\n+### 32_ã‚¹ãƒ†ãƒƒãƒ—ï¼“æ•™è‚².md\n+#### ï¼“ï¼‰å­ã©ã‚‚ãŸã¡ã®å¥½å¥‡å¿ƒã¨ã€Œã¯ã˜ã‚ã‚‹åŠ›ã€ã‚’è‚²ã‚€ãŸã‚ã®æ•™è‚²ã«æŠ•è³‡ã—ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¤‰æ›´ã™ã‚‹PR:\n+- PR #1475: [æ”¿ç­–æ¡ˆã€Œï¼’ï¼æ•™è‚²ã€ã¸ã®å­ã©ã‚‚ã®æ¨©åˆ©å°Šé‡ã«é–¢ã™ã‚‹æ–°è¦ææ¡ˆ (IR)](https://github.com/team-mirai/policy/pull/1475)\n+- PR #1307: [å…¬è¨­æ°‘å–¶å‹æ–½è¨­ã®äº‹ä¾‹è¨˜è¿°ã‚’æœ€æ–°æƒ…å ±ã«åŸºã¥ãæ›´æ–°](https://github.com/team-mirai/policy/pull/1307)\n+\n+### 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md\n+#### ï¼”ï¼‰è²§å›°ä¸–å¸¯ã®å­ã©ã‚‚ãŸã¡ãƒ»ä¿è­·è€…ã®çš†æ§˜ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã¨AIã‚’é§†ä½¿ã—ã€ãƒ—ãƒƒã‚·ãƒ¥å‹ã®æ”¯æ´ã‚’å®Ÿç¾ã—ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¤‰æ›´ã™ã‚‹PR:\n+- PR #1462: [SAISã«ãŠã‘ã‚‹ç§‘å­¦æŠ€è¡“ã‚³ãƒ³ãƒ†ã‚¹ãƒˆå‚åŠ æ”¯æ´ã®æ˜è¨˜](https://github.com/team-mirai/policy/pull/1462)\n+- PR #1460: [æ•™è‚²æ”¿ç­–ã«ãŠã‘ã‚‹å€‹åˆ¥æœ€é©åŒ–ã¨ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ã®å¼·åŒ–ï¼ˆåŒ¿åãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆï¼‰](https://github.com/team-mirai/policy/pull/1460)\n+- PR #1431: [æ•™è‚²æ–¹é‡æ”¹å–„ï¼šå°‚ç§‘åˆ¶å°å…¥ã¨æ•™å“¡ã®å½¹å‰²åˆ†æ‹…ã«ã‚ˆã‚‹AIæ´»ç”¨ä¿ƒé€²ã¨åƒãæ–¹æ”¹é©](https://github.com/team-mirai/policy/pull/1431)\n+- PR #1385: [mayabreaã•ã‚“ææ¡ˆï¼šå°å­¦æ ¡ã«ãŠã‘ã‚‹ç”ŸæˆAIåˆ©ç”¨ã®å®‰å…¨æŠ€è¡“è¦ä»¶ã‚’å¼·åŒ–](https://github.com/team-mirai/policy/pull/1385)\n+- PR #1383: [æ•™è‚²æ”¿ç­–æ¡ˆã®æ”¹å–„ï¼šæ—¢å­˜æ•™è‚²ã‚¢ãƒ—ãƒªã®è¿…é€Ÿãªå°å…¥ã«ã‚ˆã‚‹å­¦ç¿’æ©Ÿä¼šã®æ‹¡å……ï¼ˆHimawaruwaruã•ã‚“ææ¡ˆï¼‰](https://github.com/team-mirai/policy/pull/1383)\n+- PR #1338: [æ•™å“¡ã®åƒãæ–¹æ”¹é©ã«é–¢ã™ã‚‹è¨˜è¿°ã‚’æ›´æ–°ï¼ˆåƒè‘‰ã•ã‚“ææ¡ˆï¼‰](https://github.com/team-mirai/policy/pull/1338)\n+- PR #1335: [æ•™è‚²æ”¿ç­–ã®æ”¹å–„æ¡ˆï¼šAIå®¶åº­æ•™å¸«ã®å¯¾è±¡æ‹¡å¤§ã¨ãƒ—ãƒƒã‚·ãƒ¥å‹æ”¯æ´ã®å¼·åŒ–ï¼ˆã™ã ã•ã‚“ã”ææ¡ˆï¼‰](https://github.com/team-mirai/policy/pull/1335)\n+- PR #1316: [ãƒ ãƒ©ã‚¤æ°ææ¡ˆï¼šå®Ÿè·µçš„ã‚¹ã‚­ãƒ«è‚²æˆã¨ç¤¾ä¼šãƒ‹ãƒ¼ã‚ºã‚’åæ˜ ã—ãŸæ•™è‚²ã¸ã®AIæ´»ç”¨å¼·åŒ–](https://github.com/team-mirai/policy/pull/1316)\n+\n+### 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md\n+#### ï¼‘ï¼æ•™è‚² > ãƒ“ã‚¸ãƒ§ãƒ³\n+ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¤‰æ›´ã™ã‚‹PR:\n+- PR #1460: [æ•™è‚²æ”¿ç­–ã«ãŠã‘ã‚‹å€‹åˆ¥æœ€é©åŒ–ã¨ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ã®å¼·åŒ–ï¼ˆåŒ¿åãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆï¼‰](https://github.com/team-mirai/policy/pull/1460)\n+- PR #1335: [æ•™è‚²æ”¿ç­–ã®æ”¹å–„æ¡ˆï¼šAIå®¶åº­æ•™å¸«ã®å¯¾è±¡æ‹¡å¤§ã¨ãƒ—ãƒƒã‚·ãƒ¥å‹æ”¯æ´ã®å¼·åŒ–ï¼ˆã™ã ã•ã‚“ã”ææ¡ˆï¼‰](https://github.com/team-mirai/policy/pull/1335)\n+- PR #1316: [ãƒ ãƒ©ã‚¤æ°ææ¡ˆï¼šå®Ÿè·µçš„ã‚¹ã‚­ãƒ«è‚²æˆã¨ç¤¾ä¼šãƒ‹ãƒ¼ã‚ºã‚’åæ˜ ã—ãŸæ•™è‚²ã¸ã®AIæ´»ç”¨å¼·åŒ–](https://github.com/team-mirai/policy/pull/1316)\n+\n+### 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md\n+#### ï¼‘ï¼‰ã™ã¹ã¦ã®å­ã©ã‚‚ã«ã€Œå°‚å±ã®AIå®¶åº­æ•™å¸«ã€ã‚’å±Šã‘ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¤‰æ›´ã™ã‚‹PR:\n+- PR #1431: [æ•™è‚²æ–¹é‡æ”¹å–„ï¼šå°‚ç§‘åˆ¶å°å…¥ã¨æ•™å“¡ã®å½¹å‰²åˆ†æ‹…ã«ã‚ˆã‚‹AIæ´»ç”¨ä¿ƒé€²ã¨åƒãæ–¹æ”¹é©](https://github.com/team-mirai/policy/pull/1431)\n+- PR #1383: [æ•™è‚²æ”¿ç­–æ¡ˆã®æ”¹å–„ï¼šæ—¢å­˜æ•™è‚²ã‚¢ãƒ—ãƒªã®è¿…é€Ÿãªå°å…¥ã«ã‚ˆã‚‹å­¦ç¿’æ©Ÿä¼šã®æ‹¡å……ï¼ˆHimawaruwaruã•ã‚“ææ¡ˆï¼‰](https://github.com/team-mirai/policy/pull/1383)\n+- PR #1335: [æ•™è‚²æ”¿ç­–ã®æ”¹å–„æ¡ˆï¼šAIå®¶åº­æ•™å¸«ã®å¯¾è±¡æ‹¡å¤§ã¨ãƒ—ãƒƒã‚·ãƒ¥å‹æ”¯æ´ã®å¼·åŒ–ï¼ˆã™ã ã•ã‚“ã”ææ¡ˆï¼‰](https://github.com/team-mirai/policy/pull/1335)\n+\n+### 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md\n+#### ï¼“ï¼‰ï¼šAIï¼†ITã«ã‚ˆã‚‹æ•™å“¡ã®åƒãæ–¹æ”¹é©ã‚’é€²ã‚ã‚‹ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¤‰æ›´ã™ã‚‹PR:\n+- PR #1431: [æ•™è‚²æ–¹é‡æ”¹å–„ï¼šå°‚ç§‘åˆ¶å°å…¥ã¨æ•™å“¡ã®å½¹å‰²åˆ†æ‹…ã«ã‚ˆã‚‹AIæ´»ç”¨ä¿ƒé€²ã¨åƒãæ–¹æ”¹é©](https://github.com/team-mirai/policy/pull/1431)\n+- PR #1338: [æ•™å“¡ã®åƒãæ–¹æ”¹é©ã«é–¢ã™ã‚‹è¨˜è¿°ã‚’æ›´æ–°ï¼ˆåƒè‘‰ã•ã‚“ææ¡ˆï¼‰](https://github.com/team-mirai/policy/pull/1338)\n+\n+## PRåˆ¥å¤‰æ›´ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä¸€è¦§\n+\n+### PR #1307: å…¬è¨­æ°‘å–¶å‹æ–½è¨­ã®äº‹ä¾‹è¨˜è¿°ã‚’æœ€æ–°æƒ…å ±ã«åŸºã¥ãæ›´æ–°\n+- 32_ã‚¹ãƒ†ãƒƒãƒ—ï¼“æ•™è‚².md: ï¼“ï¼‰å­ã©ã‚‚ãŸã¡ã®å¥½å¥‡å¿ƒã¨ã€Œã¯ã˜ã‚ã‚‹åŠ›ã€ã‚’è‚²ã‚€ãŸã‚ã®æ•™è‚²ã«æŠ•è³‡ã—ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+\n+### PR #1316: ãƒ ãƒ©ã‚¤æ°ææ¡ˆï¼šå®Ÿè·µçš„ã‚¹ã‚­ãƒ«è‚²æˆã¨ç¤¾ä¼šãƒ‹ãƒ¼ã‚ºã‚’åæ˜ ã—ãŸæ•™è‚²ã¸ã®AIæ´»ç”¨å¼·åŒ–\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼‘ï¼æ•™è‚² > ãƒ“ã‚¸ãƒ§ãƒ³\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼”ï¼‰è²§å›°ä¸–å¸¯ã®å­ã©ã‚‚ãŸã¡ãƒ»ä¿è­·è€…ã®çš†æ§˜ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã¨AIã‚’é§†ä½¿ã—ã€ãƒ—ãƒƒã‚·ãƒ¥å‹ã®æ”¯æ´ã‚’å®Ÿç¾ã—ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+\n+### PR #1335: æ•™è‚²æ”¿ç­–ã®æ”¹å–„æ¡ˆï¼šAIå®¶åº­æ•™å¸«ã®å¯¾è±¡æ‹¡å¤§ã¨ãƒ—ãƒƒã‚·ãƒ¥å‹æ”¯æ´ã®å¼·åŒ–ï¼ˆã™ã ã•ã‚“ã”ææ¡ˆï¼‰\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼‘ï¼æ•™è‚² > ãƒ“ã‚¸ãƒ§ãƒ³\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼”ï¼‰è²§å›°ä¸–å¸¯ã®å­ã©ã‚‚ãŸã¡ãƒ»ä¿è­·è€…ã®çš†æ§˜ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã¨AIã‚’é§†ä½¿ã—ã€ãƒ—ãƒƒã‚·ãƒ¥å‹ã®æ”¯æ´ã‚’å®Ÿç¾ã—ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼‘ï¼‰ã™ã¹ã¦ã®å­ã©ã‚‚ã«ã€Œå°‚å±ã®AIå®¶åº­æ•™å¸«ã€ã‚’å±Šã‘ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼‘ï¼æ•™è‚² > ï¼‘ï¼‰ã™ã¹ã¦ã®å­ã©ã‚‚ã«ã€Œå°‚å±ã®AIå®¶åº­æ•™å¸«ã€ã‚’å±Šã‘ã¾ã™\n+\n+### PR #1338: æ•™å“¡ã®åƒãæ–¹æ”¹é©ã«é–¢ã™ã‚‹è¨˜è¿°ã‚’æ›´æ–°ï¼ˆåƒè‘‰ã•ã‚“ææ¡ˆï¼‰\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼”ï¼‰è²§å›°ä¸–å¸¯ã®å­ã©ã‚‚ãŸã¡ãƒ»ä¿è­·è€…ã®çš†æ§˜ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã¨AIã‚’é§†ä½¿ã—ã€ãƒ—ãƒƒã‚·ãƒ¥å‹ã®æ”¯æ´ã‚’å®Ÿç¾ã—ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼“ï¼‰ï¼šAIï¼†ITã«ã‚ˆã‚‹æ•™å“¡ã®åƒãæ–¹æ”¹é©ã‚’é€²ã‚ã‚‹ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+\n+### PR #1383: æ•™è‚²æ”¿ç­–æ¡ˆã®æ”¹å–„ï¼šæ—¢å­˜æ•™è‚²ã‚¢ãƒ—ãƒªã®è¿…é€Ÿãªå°å…¥ã«ã‚ˆã‚‹å­¦ç¿’æ©Ÿä¼šã®æ‹¡å……ï¼ˆHimawaruwaruã•ã‚“ææ¡ˆï¼‰\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼‘ï¼‰ã™ã¹ã¦ã®å­ã©ã‚‚ã«ã€Œå°‚å±ã®AIå®¶åº­æ•™å¸«ã€ã‚’å±Šã‘ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼”ï¼‰è²§å›°ä¸–å¸¯ã®å­ã©ã‚‚ãŸã¡ãƒ»ä¿è­·è€…ã®çš†æ§˜ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã¨AIã‚’é§†ä½¿ã—ã€ãƒ—ãƒƒã‚·ãƒ¥å‹ã®æ”¯æ´ã‚’å®Ÿç¾ã—ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+\n+### PR #1385: mayabreaã•ã‚“ææ¡ˆï¼šå°å­¦æ ¡ã«ãŠã‘ã‚‹ç”ŸæˆAIåˆ©ç”¨ã®å®‰å…¨æŠ€è¡“è¦ä»¶ã‚’å¼·åŒ–\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼’ï¼‰å­ã©ã‚‚ã®AIãƒªãƒ†ãƒ©ã‚·ãƒ¼ã‚’è‚²ã¿ã€AIã¨å…±ç”Ÿã™ã‚‹æœªæ¥ã‚’åˆ‡ã‚Šæ‹“ãã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼”ï¼‰è²§å›°ä¸–å¸¯ã®å­ã©ã‚‚ãŸã¡ãƒ»ä¿è­·è€…ã®çš†æ§˜ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã¨AIã‚’é§†ä½¿ã—ã€ãƒ—ãƒƒã‚·ãƒ¥å‹ã®æ”¯æ´ã‚’å®Ÿç¾ã—ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+\n+### PR #1431: æ•™è‚²æ–¹é‡æ”¹å–„ï¼šå°‚ç§‘åˆ¶å°å…¥ã¨æ•™å“¡ã®å½¹å‰²åˆ†æ‹…ã«ã‚ˆã‚‹AIæ´»ç”¨ä¿ƒé€²ã¨åƒãæ–¹æ”¹é©\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼‘ï¼‰ã™ã¹ã¦ã®å­ã©ã‚‚ã«ã€Œå°‚å±ã®AIå®¶åº­æ•™å¸«ã€ã‚’å±Šã‘ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼“ï¼‰ï¼šAIï¼†ITã«ã‚ˆã‚‹æ•™å“¡ã®åƒãæ–¹æ”¹é©ã‚’é€²ã‚ã‚‹ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼”ï¼‰è²§å›°ä¸–å¸¯ã®å­ã©ã‚‚ãŸã¡ãƒ»ä¿è­·è€…ã®çš†æ§˜ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã¨AIã‚’é§†ä½¿ã—ã€ãƒ—ãƒƒã‚·ãƒ¥å‹ã®æ”¯æ´ã‚’å®Ÿç¾ã—ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+\n+### PR #1460: æ•™è‚²æ”¿ç­–ã«ãŠã‘ã‚‹å€‹åˆ¥æœ€é©åŒ–ã¨ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ã®å¼·åŒ–ï¼ˆåŒ¿åãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆï¼‰\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼‘ï¼æ•™è‚² > ãƒ“ã‚¸ãƒ§ãƒ³\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼”ï¼‰è²§å›°ä¸–å¸¯ã®å­ã©ã‚‚ãŸã¡ãƒ»ä¿è­·è€…ã®çš†æ§˜ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã¨AIã‚’é§†ä½¿ã—ã€ãƒ—ãƒƒã‚·ãƒ¥å‹ã®æ”¯æ´ã‚’å®Ÿç¾ã—ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+\n+### PR #1462: SAISã«ãŠã‘ã‚‹ç§‘å­¦æŠ€è¡“ã‚³ãƒ³ãƒ†ã‚¹ãƒˆå‚åŠ æ”¯æ´ã®æ˜è¨˜\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼”ï¼‰è²§å›°ä¸–å¸¯ã®å­ã©ã‚‚ãŸã¡ãƒ»ä¿è­·è€…ã®çš†æ§˜ã‚’æ”¯æ´ã™ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã¨AIã‚’é§†ä½¿ã—ã€ãƒ—ãƒƒã‚·ãƒ¥å‹ã®æ”¯æ´ã‚’å®Ÿç¾ã—ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+- 11_ã‚¹ãƒ†ãƒƒãƒ—ï¼‘æ•™è‚².md: ï¼‘ï¼æ•™è‚² > ï¼“ï¼‰ï¼šAIï¼†ITã«ã‚ˆã‚‹æ•™å“¡ã®åƒãæ–¹æ”¹é©ã‚’é€²ã‚ã‚‹\n+\n+### PR #1475: æ”¿ç­–æ¡ˆã€Œï¼’ï¼æ•™è‚²ã€ã¸ã®å­ã©ã‚‚ã®æ¨©åˆ©å°Šé‡ã«é–¢ã™ã‚‹æ–°è¦ææ¡ˆ (IR)\n+- 32_ã‚¹ãƒ†ãƒƒãƒ—ï¼“æ•™è‚².md: ï¼“ï¼‰å­ã©ã‚‚ãŸã¡ã®å¥½å¥‡å¿ƒã¨ã€Œã¯ã˜ã‚ã‚‹åŠ›ã€ã‚’è‚²ã‚€ãŸã‚ã®æ•™è‚²ã«æŠ•è³‡ã—ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+- 32_ã‚¹ãƒ†ãƒƒãƒ—ï¼“æ•™è‚².md: ï¼‘ï¼‰æ”¿åºœå…¨ä½“ã§ã€æ•™è‚²ã¸ã®æŠ•è³‡äºˆç®—ã‚’ç¢ºä¿ã—ã¾ã™ > ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ\n+"
    },
    {
      "sha": "c98dacfacf04cbebb19abf181a0f40a44efd24f7",
      "filename": "pr_section_analyzer.py",
      "status": "added",
      "additions": 420,
      "deletions": 0,
      "changes": 420,
      "blob_url": "https://github.com/team-mirai/policy/blob/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer.py",
      "raw_url": "https://github.com/team-mirai/policy/raw/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer.py",
      "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/pr_section_analyzer.py?ref=1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
      "patch": "@@ -0,0 +1,420 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Script to analyze PRs in the policy repository and extract the specific sections \n+of the manifest that are being modified. This script can identify which PRs modify\n+the same sections of the manifest.\n+\"\"\"\n+\n+import json\n+import os\n+import re\n+import subprocess\n+import sys\n+import argparse\n+from collections import defaultdict\n+\n+def run_command(command):\n+    \"\"\"Run a shell command and return the output.\"\"\"\n+    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n+    if result.returncode != 0:\n+        print(f\"Error running command: {command}\")\n+        print(f\"Error: {result.stderr}\")\n+        return \"\"\n+    return result.stdout.strip()\n+\n+def get_pr_list(limit=100):\n+    \"\"\"Get a list of PRs from the repository.\"\"\"\n+    command = f\"gh pr list --limit {limit} --json number,title,headRefName,state,url\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR list: {output}\")\n+        return []\n+\n+def get_pr_details(pr_number):\n+    \"\"\"Get detailed information about a PR.\"\"\"\n+    command = f\"gh pr view {pr_number} --json number,title,headRefName,state,url,body\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR details: {output}\")\n+        return {}\n+\n+def get_pr_files(pr_number):\n+    \"\"\"Get the list of files changed in a PR.\"\"\"\n+    command = f\"gh pr view {pr_number} --json files\"\n+    output = run_command(command)\n+    try:\n+        data = json.loads(output)\n+        return data.get(\"files\", [])\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR files: {output}\")\n+        return []\n+\n+def get_pr_diff(pr_number):\n+    \"\"\"Get the diff for a PR.\"\"\"\n+    command = f\"git fetch origin pull/{pr_number}/head:pr-{pr_number}-temp && git diff main..pr-{pr_number}-temp\"\n+    diff = run_command(command)\n+    if not diff:\n+        print(f\"DEBUG: Failed to get diff using git fetch/diff, trying alternative method\")\n+        command = f\"git fetch && git diff origin/main...origin/pr-{pr_number}-head\"\n+        diff = run_command(command)\n+    return diff\n+\n+def extract_markdown_sections(file_path):\n+    \"\"\"Extract the markdown sections from a file with improved Japanese section handling.\"\"\"\n+    if not os.path.exists(file_path):\n+        print(f\"File not found: {file_path}\")\n+        return {}\n+    \n+    with open(file_path, 'r', encoding='utf-8') as f:\n+        content = f.read()\n+    \n+    headings = []\n+    section_content = {}\n+    current_section_start = None\n+    \n+    for i, line in enumerate(content.split('\\n')):\n+        heading_match = re.match(r'^(#+)\\s+(.+)$', line)\n+        \n+        jp_section_match = None\n+        if not heading_match:\n+            jp_section_match = re.match(r'^([ï¼-ï¼™]+ï¼‰|\\d+ï¼‰)\\s*(.+)$', line)\n+        \n+        if heading_match:\n+            level = len(heading_match.group(1))\n+            title = heading_match.group(2).strip()\n+            headings.append((i+1, level, title))\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_section_match:\n+            level = 3\n+            title = jp_section_match.group(0).strip()\n+            headings.append((i+1, level, title))\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+    \n+    if current_section_start is not None and len(content.split('\\n')) > 0:\n+        section_content[current_section_start] = (current_section_start, len(content.split('\\n')))\n+    \n+    sections = {}\n+    for i, (line_num, level, title) in enumerate(headings):\n+        parent_sections = []\n+        for prev_line, prev_level, prev_title in reversed(headings[:i]):\n+            if prev_level < level:\n+                parent_sections.insert(0, prev_title)\n+                if prev_level == 1:  # Stop at top level\n+                    break\n+        \n+        section_path = \" > \".join(parent_sections + [title])\n+        \n+        content_range = section_content.get(line_num, (line_num, line_num))\n+        \n+        sections[line_num] = {\n+            \"level\": level,\n+            \"title\": title,\n+            \"path\": section_path,\n+            \"parent_sections\": parent_sections,\n+            \"start_line\": content_range[0],\n+            \"end_line\": content_range[1]\n+        }\n+    \n+    return sections\n+\n+def find_section_for_line(sections, line_number):\n+    \"\"\"Find the section that contains a specific line.\"\"\"\n+    for section_line, section_info in sections.items():\n+        if section_info[\"start_line\"] <= line_number <= section_info[\"end_line\"]:\n+            return section_info\n+    \n+    section_lines = sorted(sections.keys())\n+    for i, section_line in enumerate(section_lines):\n+        if section_line > line_number:\n+            if i > 0:\n+                return sections[section_lines[i-1]]\n+            break\n+    \n+    if section_lines:\n+        return sections[section_lines[-1]]\n+    \n+    return None\n+\n+def parse_diff_hunks(diff_text):\n+    \"\"\"Parse the diff hunks to extract line numbers and changes with improved accuracy.\"\"\"\n+    hunks = []\n+    current_file = None\n+    in_file_header = False\n+    \n+    lines = diff_text.split('\\n')\n+    i = 0\n+    \n+    while i < len(lines):\n+        line = lines[i]\n+        \n+        if line.startswith('diff --git'):\n+            in_file_header = True\n+            match = re.search(r'b/(.+?)(\"?)$', line)\n+            if match:\n+                current_file = match.group(1)\n+                if current_file.endswith('\"'):\n+                    current_file = current_file[:-1]\n+                current_file = current_file.replace('\\\\', '')\n+                print(f\"DEBUG: Extracted file path: {current_file}\")\n+        elif line.startswith('+++'):\n+            in_file_header = False\n+            match = re.search(r'\\+\\+\\+ b/(.+?)(\"?)$', line)\n+            if match:\n+                new_file = match.group(1)\n+                if new_file.endswith('\"'):\n+                    new_file = new_file[:-1]\n+                new_file = new_file.replace('\\\\', '')\n+                print(f\"DEBUG: Updated file path from +++ line: {new_file}\")\n+                current_file = new_file\n+        elif line.startswith('@@') and not in_file_header:\n+            match = re.search(r'@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@', line)\n+            if match and current_file:\n+                old_start = int(match.group(1))\n+                old_count = int(match.group(2)) if match.group(2) else 1\n+                new_start = int(match.group(3))\n+                new_count = int(match.group(4)) if match.group(4) else 1\n+                \n+                hunk = {\n+                    'file': current_file,\n+                    'old_start': old_start,\n+                    'old_count': old_count,\n+                    'new_start': new_start,\n+                    'new_count': new_count,\n+                    'changes': [],\n+                    'context_before': [],\n+                    'context_after': []\n+                }\n+                \n+                j = i + 1\n+                while j < len(lines) and j < i + 4 and not lines[j].startswith(('+', '-', '@@')):\n+                    hunk['context_before'].append(lines[j])\n+                    j += 1\n+                \n+                current_old_line = old_start\n+                current_new_line = new_start\n+                \n+                j = i + 1\n+                while j < len(lines) and not lines[j].startswith('@@'):\n+                    if lines[j].startswith('+'):\n+                        hunk['changes'].append({\n+                            'type': 'add',\n+                            'line_num': current_new_line,\n+                            'content': lines[j][1:]\n+                        })\n+                        current_new_line += 1\n+                    elif lines[j].startswith('-'):\n+                        hunk['changes'].append({\n+                            'type': 'remove',\n+                            'line_num': current_old_line,\n+                            'content': lines[j][1:]\n+                        })\n+                        current_old_line += 1\n+                    else:\n+                        current_old_line += 1\n+                        current_new_line += 1\n+                    j += 1\n+                \n+                hunks.append(hunk)\n+        \n+        i += 1\n+    \n+    return hunks\n+\n+def analyze_pr(pr_number):\n+    \"\"\"Analyze a PR and extract the sections being modified.\"\"\"\n+    pr_details = get_pr_details(pr_number)\n+    diff_text = get_pr_diff(pr_number)\n+    \n+    print(f\"DEBUG: Analyzing PR #{pr_number}\")\n+    \n+    if not diff_text:\n+        print(\"DEBUG: No diff text found\")\n+        return None\n+    \n+    print(f\"DEBUG: Diff text length: {len(diff_text)} characters\")\n+    print(f\"DEBUG: First 100 chars of diff: {diff_text[:100]}\")\n+    \n+    hunks = parse_diff_hunks(diff_text)\n+    print(f\"DEBUG: Found {len(hunks)} hunks in diff\")\n+    \n+    results = []\n+    for i, hunk in enumerate(hunks):\n+        file_path = hunk['file']\n+        print(f\"DEBUG: Hunk {i+1} - File: {file_path}\")\n+        \n+        is_markdown = False\n+        if file_path.endswith('.md'):\n+            is_markdown = True\n+        elif file_path.lower().endswith('.md\"'):\n+            is_markdown = True\n+            file_path = file_path[:-1]  # Remove trailing quote\n+        elif '.' in file_path and file_path.split('.')[-1].lower() == 'md':\n+            is_markdown = True\n+        \n+        if not is_markdown:\n+            print(f\"DEBUG: Skipping non-markdown file: {file_path}\")\n+            continue\n+        \n+        full_path = os.path.join(os.getcwd(), file_path)\n+        print(f\"DEBUG: Full path: {full_path}\")\n+        \n+        if not os.path.exists(full_path):\n+            print(f\"Warning: File {full_path} does not exist, skipping\")\n+            continue\n+        \n+        sections = extract_markdown_sections(full_path)\n+        print(f\"DEBUG: Found {len(sections)} sections in {file_path}\")\n+        if len(sections) > 0:\n+            print(f\"DEBUG: Section titles: {[s['title'] for s in sections.values()][:5]}\")\n+        \n+        affected_sections = set()\n+        for j, change in enumerate(hunk['changes']):\n+            line_number = change['line_num']\n+            print(f\"DEBUG: Change {j+1} - Line: {line_number}, Content: {change['content'][:30]}...\")\n+            \n+            section = find_section_for_line(sections, line_number)\n+            \n+            if section:\n+                print(f\"DEBUG: Found section: {section['title']}\")\n+                affected_sections.add((section['title'], section['path']))\n+            else:\n+                print(f\"DEBUG: No section found for line {line_number}\")\n+        \n+        for section_title, section_path in affected_sections:\n+            results.append({\n+                'pr_number': pr_number,\n+                'pr_title': pr_details.get('title', ''),\n+                'pr_url': pr_details.get('url', ''),\n+                'file': file_path,\n+                'section': section_title,\n+                'section_path': section_path,\n+                'changes': [c['content'] for c in hunk['changes']]\n+            })\n+    \n+    print(f\"DEBUG: Found {len(results)} affected sections\")\n+    return results\n+\n+def analyze_all_prs(limit=100):\n+    \"\"\"Analyze all PRs and group them by the sections they modify.\"\"\"\n+    prs = get_pr_list(limit)\n+    \n+    all_results = []\n+    section_to_prs = defaultdict(list)\n+    \n+    for pr in prs:\n+        pr_number = pr['number']\n+        print(f\"Analyzing PR #{pr_number}: {pr['title']}\")\n+        \n+        results = analyze_pr(pr_number)\n+        if results:\n+            all_results.extend(results)\n+            \n+            for result in results:\n+                section_key = f\"{result['file']}:{result['section_path']}\"\n+                section_to_prs[section_key].append({\n+                    'pr_number': pr_number,\n+                    'pr_title': pr['title'],\n+                    'pr_url': pr['url']\n+                })\n+    \n+    return all_results, section_to_prs\n+\n+def generate_report(all_results, section_to_prs):\n+    \"\"\"Generate a report of the analysis results.\"\"\"\n+    report = []\n+    \n+    report.append(\"# PRs Grouped by Section\\n\")\n+    \n+    for section_key, prs in sorted(section_to_prs.items()):\n+        if len(prs) > 1:  # Only show sections with multiple PRs\n+            file_path, section_path = section_key.split(':', 1)\n+            report.append(f\"## {file_path}\\n\")\n+            report.append(f\"### {section_path}\\n\")\n+            report.append(\"PRs modifying this section:\\n\")\n+            \n+            for pr in prs:\n+                report.append(f\"- PR #{pr['pr_number']}: {pr['pr_title']} ({pr['pr_url']})\\n\")\n+            \n+            report.append(\"\\n\")\n+    \n+    report.append(\"# Sections Modified by Each PR\\n\")\n+    \n+    pr_to_sections = defaultdict(list)\n+    for result in all_results:\n+        pr_key = f\"{result['pr_number']}:{result['pr_title']}\"\n+        section_info = {\n+            'file': result['file'],\n+            'section_path': result['section_path']\n+        }\n+        pr_to_sections[pr_key].append(section_info)\n+    \n+    for pr_key, sections in sorted(pr_to_sections.items()):\n+        pr_number, pr_title = pr_key.split(':', 1)\n+        report.append(f\"## PR #{pr_number}: {pr_title}\\n\")\n+        \n+        for section in sections:\n+            report.append(f\"- {section['file']}: {section['section_path']}\\n\")\n+        \n+        report.append(\"\\n\")\n+    \n+    return \"\".join(report)\n+\n+def main():\n+    \"\"\"Main function with improved command-line interface.\"\"\"\n+    parser = argparse.ArgumentParser(description='Analyze PRs in the policy repository and extract modified sections.')\n+    group = parser.add_mutually_exclusive_group(required=True)\n+    group.add_argument('--pr', type=str, help='PR number to analyze')\n+    group.add_argument('--all', action='store_true', help='Analyze all PRs')\n+    parser.add_argument('--limit', type=int, default=100, help='Limit the number of PRs to analyze (default: 100)')\n+    parser.add_argument('--output', type=str, help='Output file for the report (default: stdout)')\n+    parser.add_argument('--format', choices=['text', 'json'], default='text', help='Output format (default: text)')\n+    \n+    args = parser.parse_args()\n+    \n+    if args.pr:\n+        results = analyze_pr(args.pr)\n+        \n+        if results:\n+            if args.format == 'json':\n+                output = json.dumps(results, indent=2, ensure_ascii=False)\n+            else:\n+                output = f\"\\nAnalysis of PR #{args.pr}:\\n\"\n+                for result in results:\n+                    output += f\"\\nFile: {result['file']}\\n\"\n+                    output += f\"Section: {result['section_path']}\\n\"\n+                    output += \"Changes:\\n\"\n+                    for change in result['changes']:\n+                        output += f\"  {change}\\n\"\n+        else:\n+            output = f\"No markdown sections found in PR #{args.pr}\"\n+    else:  # --all\n+        all_results, section_to_prs = analyze_all_prs(args.limit)\n+        \n+        if args.format == 'json':\n+            output = json.dumps({\n+                'results': all_results,\n+                'section_to_prs': {k: v for k, v in section_to_prs.items()}\n+            }, indent=2, ensure_ascii=False)\n+        else:\n+            output = generate_report(all_results, section_to_prs)\n+    \n+    if args.output:\n+        with open(args.output, 'w', encoding='utf-8') as f:\n+            f.write(output)\n+        print(f\"Report written to {args.output}\")\n+    else:\n+        print(output)\n+\n+if __name__ == \"__main__\":\n+    main()"
    },
    {
      "sha": "a1a964cd88275570738c78f209c59f246f07a092",
      "filename": "pr_section_analyzer_final.py",
      "status": "added",
      "additions": 390,
      "deletions": 0,
      "changes": 390,
      "blob_url": "https://github.com/team-mirai/policy/blob/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer_final.py",
      "raw_url": "https://github.com/team-mirai/policy/raw/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer_final.py",
      "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/pr_section_analyzer_final.py?ref=1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
      "patch": "@@ -0,0 +1,390 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Script to analyze PRs in the policy repository and extract the specific sections \n+of the manifest that are being modified. This script can identify which PRs modify\n+the same sections of the manifest.\n+\"\"\"\n+\n+import json\n+import os\n+import re\n+import subprocess\n+import sys\n+import argparse\n+from collections import defaultdict\n+\n+def run_command(command):\n+    \"\"\"Run a shell command and return the output.\"\"\"\n+    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n+    if result.returncode != 0:\n+        print(f\"Error running command: {command}\")\n+        print(f\"Error: {result.stderr}\")\n+        return \"\"\n+    return result.stdout.strip()\n+\n+def get_pr_list(limit=100):\n+    \"\"\"Get a list of PRs from the repository.\"\"\"\n+    command = f\"gh pr list --limit {limit} --json number,title,headRefName,state,url\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR list: {output}\")\n+        return []\n+\n+def get_pr_details(pr_number):\n+    \"\"\"Get detailed information about a PR.\"\"\"\n+    command = f\"gh pr view {pr_number} --json number,title,headRefName,state,url,body\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR details: {output}\")\n+        return {}\n+\n+def get_pr_files(pr_number):\n+    \"\"\"Get the list of files changed in a PR using GitHub API.\"\"\"\n+    command = f\"gh pr view {pr_number} --json files\"\n+    output = run_command(command)\n+    try:\n+        data = json.loads(output)\n+        files = [file.get('path') for file in data.get('files', [])]\n+        print(f\"Changed files from GitHub API: {files}\")\n+        return files\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR files: {output}\")\n+        return []\n+\n+def get_file_diff(pr_number, file_path):\n+    \"\"\"Get the diff for a specific file in a PR.\"\"\"\n+    command = f\"gh pr view {pr_number} --json headRefName\"\n+    output = run_command(command)\n+    try:\n+        data = json.loads(output)\n+        branch_name = data.get('headRefName')\n+        if not branch_name:\n+            print(f\"Could not get branch name for PR #{pr_number}\")\n+            return \"\"\n+        \n+        fetch_cmd = f\"git fetch origin {branch_name}\"\n+        run_command(fetch_cmd)\n+        \n+        diff_cmd = f\"git diff main..origin/{branch_name} -- '{file_path}'\"\n+        return run_command(diff_cmd)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON for PR branch: {output}\")\n+        return \"\"\n+\n+def extract_markdown_sections(file_path):\n+    \"\"\"Extract the markdown sections from a file with improved Japanese section handling.\"\"\"\n+    if not os.path.exists(file_path):\n+        print(f\"File not found: {file_path}\")\n+        return {}\n+    \n+    with open(file_path, 'r', encoding='utf-8') as f:\n+        content = f.read()\n+    \n+    print(f\"Extracting sections from file with {len(content.split('\\n'))} lines\")\n+    \n+    headings = []\n+    section_content = {}\n+    current_section_start = None\n+    \n+    for i, line in enumerate(content.split('\\n')):\n+        heading_match = re.match(r'^(#+)\\s+(.+)$', line)\n+        \n+        jp_section_match = None\n+        if not heading_match:\n+            jp_section_match = re.match(r'^([ï¼-ï¼™]+ï¼‰|\\d+ï¼‰)\\s*(.+)$', line)\n+        \n+        jp_policy_match = None\n+        if not heading_match and not jp_section_match:\n+            jp_policy_match = re.match(r'^###\\s+(ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ|æ”¿ç­–æ¦‚è¦)$', line)\n+        \n+        if heading_match:\n+            level = len(heading_match.group(1))\n+            title = heading_match.group(2).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found heading at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_section_match:\n+            level = 2\n+            title = jp_section_match.group(0).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_policy_match:\n+            level = 3\n+            title = jp_policy_match.group(1).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP policy section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+    \n+    if current_section_start is not None and len(content.split('\\n')) > 0:\n+        section_content[current_section_start] = (current_section_start, len(content.split('\\n')))\n+    \n+    sections = {}\n+    \n+    sorted_headings = sorted(headings, key=lambda x: x[0])\n+    \n+    for i, (line_num, level, title) in enumerate(sorted_headings):\n+        parent_sections = []\n+        current_parent_level = 0\n+        \n+        for j in range(i-1, -1, -1):\n+            prev_line, prev_level, prev_title = sorted_headings[j]\n+            \n+            if prev_level < level and prev_level > current_parent_level:\n+                parent_sections.insert(0, prev_title)\n+                current_parent_level = prev_level\n+                \n+                if prev_level == 1:\n+                    break\n+        \n+        section_path = \" > \".join(parent_sections + [title])\n+        \n+        content_range = section_content.get(line_num, (line_num, line_num))\n+        \n+        sections[line_num] = {\n+            \"level\": level,\n+            \"title\": title,\n+            \"path\": section_path,\n+            \"parent_sections\": parent_sections,\n+            \"start_line\": content_range[0],\n+            \"end_line\": content_range[1]\n+        }\n+    \n+    return sections\n+\n+def find_section_for_line(sections, line_number):\n+    \"\"\"Find the section that contains a specific line.\"\"\"\n+    for section_line, section_info in sections.items():\n+        if section_info[\"start_line\"] <= line_number <= section_info[\"end_line\"]:\n+            return section_info\n+    \n+    section_lines = sorted(sections.keys())\n+    for i, section_line in enumerate(section_lines):\n+        if section_line > line_number:\n+            if i > 0:\n+                return sections[section_lines[i-1]]\n+            break\n+    \n+    if section_lines:\n+        return sections[section_lines[-1]]\n+    \n+    return None\n+\n+def extract_line_numbers_from_diff(diff_text):\n+    \"\"\"Extract line numbers from a diff.\"\"\"\n+    line_numbers = []\n+    current_line = None\n+    \n+    for line in diff_text.split('\\n'):\n+        if line.startswith('@@'):\n+            match = re.search(r'@@ -\\d+(?:,\\d+)? \\+(\\d+)(?:,(\\d+))? @@', line)\n+            if match:\n+                current_line = int(match.group(1))\n+                line_count = int(match.group(2)) if match.group(2) else 1\n+        elif current_line is not None:\n+            if line.startswith('+') and not line.startswith('+++'):\n+                line_numbers.append(current_line)\n+                current_line += 1\n+            elif line.startswith('-'):\n+                pass\n+            else:\n+                current_line += 1\n+    \n+    return line_numbers\n+\n+def analyze_pr(pr_number):\n+    \"\"\"Analyze a PR and extract the sections being modified.\"\"\"\n+    pr_details = get_pr_details(pr_number)\n+    changed_files = get_pr_files(pr_number)\n+    \n+    print(f\"Analyzing PR #{pr_number}\")\n+    \n+    if not changed_files:\n+        print(\"No changed files found\")\n+        return None\n+    \n+    results = []\n+    \n+    for file_path in changed_files:\n+        if not file_path.endswith('.md'):\n+            print(f\"Skipping non-markdown file: {file_path}\")\n+            continue\n+        \n+        print(f\"Processing file: {file_path}\")\n+        full_path = os.path.join(os.getcwd(), file_path)\n+        \n+        if not os.path.exists(full_path):\n+            print(f\"Warning: File {full_path} does not exist, skipping\")\n+            continue\n+        \n+        file_diff = get_file_diff(pr_number, file_path)\n+        \n+        if not file_diff:\n+            print(f\"No diff found for file {file_path}\")\n+            continue\n+        \n+        print(f\"Found diff for file {file_path}, length: {len(file_diff)}\")\n+        \n+        line_numbers = extract_line_numbers_from_diff(file_diff)\n+        \n+        if not line_numbers:\n+            print(f\"No line numbers found in diff for file {file_path}\")\n+            continue\n+        \n+        print(f\"Affected line numbers: {line_numbers[:10]}...\")\n+        \n+        sections = extract_markdown_sections(full_path)\n+        print(f\"Found {len(sections)} sections in {file_path}\")\n+        \n+        if len(sections) > 0:\n+            print(f\"Section titles: {[s['title'] for s in sections.values()][:5]}\")\n+        \n+        affected_sections = set()\n+        for line_number in line_numbers:\n+            section = find_section_for_line(sections, line_number)\n+            if section:\n+                print(f\"Line {line_number} is in section: {section['title']}\")\n+                affected_sections.add((section['title'], section['path']))\n+        \n+        for section_title, section_path in affected_sections:\n+            results.append({\n+                'pr_number': pr_number,\n+                'pr_title': pr_details.get('title', ''),\n+                'pr_url': pr_details.get('url', ''),\n+                'file': file_path,\n+                'section': section_title,\n+                'section_path': section_path,\n+                'changes': [line for line in file_diff.split('\\n') if line.startswith('+') and not line.startswith('+++')]\n+            })\n+    \n+    print(f\"Found {len(results)} affected sections\")\n+    return results\n+\n+def analyze_all_prs(limit=100):\n+    \"\"\"Analyze all PRs and group them by the sections they modify.\"\"\"\n+    prs = get_pr_list(limit)\n+    \n+    all_results = []\n+    section_to_prs = defaultdict(list)\n+    \n+    for pr in prs:\n+        pr_number = pr['number']\n+        print(f\"Analyzing PR #{pr_number}: {pr['title']}\")\n+        \n+        results = analyze_pr(pr_number)\n+        if results:\n+            all_results.extend(results)\n+            \n+            for result in results:\n+                section_key = f\"{result['file']}:{result['section_path']}\"\n+                section_to_prs[section_key].append({\n+                    'pr_number': pr_number,\n+                    'pr_title': pr['title'],\n+                    'pr_url': pr['url']\n+                })\n+    \n+    return all_results, section_to_prs\n+\n+def generate_report(all_results, section_to_prs):\n+    \"\"\"Generate a report of the analysis results.\"\"\"\n+    report = []\n+    \n+    report.append(\"# PRs Grouped by Section\\n\")\n+    \n+    for section_key, prs in sorted(section_to_prs.items()):\n+        if len(prs) > 1:  # Only show sections with multiple PRs\n+            file_path, section_path = section_key.split(':', 1)\n+            report.append(f\"## {file_path}\\n\")\n+            report.append(f\"### {section_path}\\n\")\n+            report.append(\"PRs modifying this section:\\n\")\n+            \n+            for pr in prs:\n+                report.append(f\"- PR #{pr['pr_number']}: {pr['pr_title']} ({pr['pr_url']})\\n\")\n+            \n+            report.append(\"\\n\")\n+    \n+    report.append(\"# Sections Modified by Each PR\\n\")\n+    \n+    pr_to_sections = defaultdict(list)\n+    for result in all_results:\n+        pr_key = f\"{result['pr_number']}:{result['pr_title']}\"\n+        section_info = {\n+            'file': result['file'],\n+            'section_path': result['section_path']\n+        }\n+        pr_to_sections[pr_key].append(section_info)\n+    \n+    for pr_key, sections in sorted(pr_to_sections.items()):\n+        pr_number, pr_title = pr_key.split(':', 1)\n+        report.append(f\"## PR #{pr_number}: {pr_title}\\n\")\n+        \n+        for section in sections:\n+            report.append(f\"- {section['file']}: {section['section_path']}\\n\")\n+        \n+        report.append(\"\\n\")\n+    \n+    return \"\".join(report)\n+\n+def main():\n+    \"\"\"Main function with improved command-line interface.\"\"\"\n+    parser = argparse.ArgumentParser(description='Analyze PRs in the policy repository and extract modified sections.')\n+    group = parser.add_mutually_exclusive_group(required=True)\n+    group.add_argument('--pr', type=str, help='PR number to analyze')\n+    group.add_argument('--all', action='store_true', help='Analyze all PRs')\n+    parser.add_argument('--limit', type=int, default=100, help='Limit the number of PRs to analyze (default: 100)')\n+    parser.add_argument('--output', type=str, help='Output file for the report (default: stdout)')\n+    parser.add_argument('--format', choices=['text', 'json'], default='text', help='Output format (default: text)')\n+    \n+    args = parser.parse_args()\n+    \n+    if args.pr:\n+        results = analyze_pr(args.pr)\n+        \n+        if results:\n+            if args.format == 'json':\n+                output = json.dumps(results, indent=2, ensure_ascii=False)\n+            else:\n+                output = f\"\\nAnalysis of PR #{args.pr}:\\n\"\n+                for result in results:\n+                    output += f\"\\nFile: {result['file']}\\n\"\n+                    output += f\"Section: {result['section_path']}\\n\"\n+                    output += \"Changes:\\n\"\n+                    for change in result['changes']:\n+                        output += f\"  {change}\\n\"\n+        else:\n+            output = f\"No markdown sections found in PR #{args.pr}\"\n+    else:  # --all\n+        all_results, section_to_prs = analyze_all_prs(args.limit)\n+        \n+        if args.format == 'json':\n+            output = json.dumps({\n+                'results': all_results,\n+                'section_to_prs': {k: v for k, v in section_to_prs.items()}\n+            }, indent=2, ensure_ascii=False)\n+        else:\n+            output = generate_report(all_results, section_to_prs)\n+    \n+    if args.output:\n+        with open(args.output, 'w', encoding='utf-8') as f:\n+            f.write(output)\n+        print(f\"Report written to {args.output}\")\n+    else:\n+        print(output)\n+\n+if __name__ == \"__main__\":\n+    main()"
    },
    {
      "sha": "29dd289128d3622917579d7305d95eea19622db4",
      "filename": "pr_section_analyzer_new.py",
      "status": "added",
      "additions": 395,
      "deletions": 0,
      "changes": 395,
      "blob_url": "https://github.com/team-mirai/policy/blob/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer_new.py",
      "raw_url": "https://github.com/team-mirai/policy/raw/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer_new.py",
      "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/pr_section_analyzer_new.py?ref=1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
      "patch": "@@ -0,0 +1,395 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Script to analyze PRs in the policy repository and extract the specific sections \n+of the manifest that are being modified. This script can identify which PRs modify\n+the same sections of the manifest.\n+\"\"\"\n+\n+import json\n+import os\n+import re\n+import subprocess\n+import sys\n+import argparse\n+from collections import defaultdict\n+\n+def run_command(command):\n+    \"\"\"Run a shell command and return the output.\"\"\"\n+    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n+    if result.returncode != 0:\n+        print(f\"Error running command: {command}\")\n+        print(f\"Error: {result.stderr}\")\n+        return \"\"\n+    return result.stdout.strip()\n+\n+def get_pr_list(limit=100):\n+    \"\"\"Get a list of PRs from the repository.\"\"\"\n+    command = f\"gh pr list --limit {limit} --json number,title,headRefName,state,url\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR list: {output}\")\n+        return []\n+\n+def get_pr_details(pr_number):\n+    \"\"\"Get detailed information about a PR.\"\"\"\n+    command = f\"gh pr view {pr_number} --json number,title,headRefName,state,url,body\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR details: {output}\")\n+        return {}\n+\n+def get_pr_changed_files(pr_number):\n+    \"\"\"Get the list of files changed in a PR using git directly.\"\"\"\n+    fetch_cmd = f\"git fetch origin pull/{pr_number}/head:pr-{pr_number}-temp\"\n+    run_command(fetch_cmd)\n+    \n+    files_cmd = f\"git ls-tree -r --name-only pr-{pr_number}-temp\"\n+    files_output = run_command(files_cmd)\n+    \n+    if not files_output:\n+        print(f\"No changed files found for PR #{pr_number}\")\n+        return []\n+    \n+    all_files = files_output.split('\\n')\n+    \n+    diff_cmd = f\"git diff --name-only main..pr-{pr_number}-temp\"\n+    diff_output = run_command(diff_cmd)\n+    \n+    if not diff_output:\n+        print(f\"No changed files found in diff for PR #{pr_number}\")\n+        return []\n+    \n+    changed_files_escaped = diff_output.split('\\n')\n+    \n+    changed_files = []\n+    for file_path in all_files:\n+        for escaped_path in changed_files_escaped:\n+            if escaped_path.startswith('\"') and escaped_path.endswith('\"'):\n+                escaped_path = escaped_path[1:-1]\n+            \n+            try:\n+                decoded_path = bytes(escaped_path, 'utf-8').decode('unicode_escape')\n+                if decoded_path == file_path or escaped_path == file_path:\n+                    changed_files.append(file_path)\n+                    break\n+            except:\n+                if escaped_path == file_path:\n+                    changed_files.append(file_path)\n+                    break\n+    \n+    print(f\"Changed files (actual): {changed_files}\")\n+    return changed_files\n+\n+def get_file_diff(pr_number, file_path):\n+    \"\"\"Get the diff for a specific file in a PR.\"\"\"\n+    command = f\"git diff main..pr-{pr_number}-temp -- '{file_path}'\"\n+    return run_command(command)\n+\n+def extract_markdown_sections(file_path):\n+    \"\"\"Extract the markdown sections from a file with improved Japanese section handling.\"\"\"\n+    if not os.path.exists(file_path):\n+        print(f\"File not found: {file_path}\")\n+        return {}\n+    \n+    with open(file_path, 'r', encoding='utf-8') as f:\n+        content = f.read()\n+    \n+    print(f\"Extracting sections from file with {len(content.split('\\n'))} lines\")\n+    \n+    headings = []\n+    section_content = {}\n+    current_section_start = None\n+    \n+    for i, line in enumerate(content.split('\\n')):\n+        heading_match = re.match(r'^(#+)\\s+(.+)$', line)\n+        \n+        jp_section_match = None\n+        if not heading_match:\n+            jp_section_match = re.match(r'^([ï¼-ï¼™]+ï¼‰|\\d+ï¼‰)\\s*(.+)$', line)\n+        \n+        jp_policy_match = None\n+        if not heading_match and not jp_section_match:\n+            jp_policy_match = re.match(r'^###\\s+(ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ|æ”¿ç­–æ¦‚è¦)$', line)\n+        \n+        if heading_match:\n+            level = len(heading_match.group(1))\n+            title = heading_match.group(2).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found heading at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_section_match:\n+            level = 3\n+            title = jp_section_match.group(0).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_policy_match:\n+            level = 3\n+            title = jp_policy_match.group(1).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP policy section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+    \n+    if current_section_start is not None and len(content.split('\\n')) > 0:\n+        section_content[current_section_start] = (current_section_start, len(content.split('\\n')))\n+    \n+    sections = {}\n+    for i, (line_num, level, title) in enumerate(headings):\n+        parent_sections = []\n+        for prev_line, prev_level, prev_title in reversed(headings[:i]):\n+            if prev_level < level:\n+                parent_sections.insert(0, prev_title)\n+                if prev_level == 1:  # Stop at top level\n+                    break\n+        \n+        section_path = \" > \".join(parent_sections + [title])\n+        \n+        content_range = section_content.get(line_num, (line_num, line_num))\n+        \n+        sections[line_num] = {\n+            \"level\": level,\n+            \"title\": title,\n+            \"path\": section_path,\n+            \"parent_sections\": parent_sections,\n+            \"start_line\": content_range[0],\n+            \"end_line\": content_range[1]\n+        }\n+    \n+    return sections\n+\n+def find_section_for_line(sections, line_number):\n+    \"\"\"Find the section that contains a specific line.\"\"\"\n+    for section_line, section_info in sections.items():\n+        if section_info[\"start_line\"] <= line_number <= section_info[\"end_line\"]:\n+            return section_info\n+    \n+    section_lines = sorted(sections.keys())\n+    for i, section_line in enumerate(section_lines):\n+        if section_line > line_number:\n+            if i > 0:\n+                return sections[section_lines[i-1]]\n+            break\n+    \n+    if section_lines:\n+        return sections[section_lines[-1]]\n+    \n+    return None\n+\n+def extract_line_numbers_from_diff(diff_text):\n+    \"\"\"Extract line numbers from a diff.\"\"\"\n+    line_numbers = []\n+    current_line = None\n+    \n+    for line in diff_text.split('\\n'):\n+        if line.startswith('@@'):\n+            match = re.search(r'@@ -\\d+(?:,\\d+)? \\+(\\d+)(?:,(\\d+))? @@', line)\n+            if match:\n+                current_line = int(match.group(1))\n+                line_count = int(match.group(2)) if match.group(2) else 1\n+        elif current_line is not None:\n+            if line.startswith('+') and not line.startswith('+++'):\n+                line_numbers.append(current_line)\n+                current_line += 1\n+            elif line.startswith('-'):\n+                pass\n+            else:\n+                current_line += 1\n+    \n+    return line_numbers\n+\n+def analyze_pr(pr_number):\n+    \"\"\"Analyze a PR and extract the sections being modified.\"\"\"\n+    pr_details = get_pr_details(pr_number)\n+    changed_files = get_pr_changed_files(pr_number)\n+    \n+    print(f\"Analyzing PR #{pr_number}\")\n+    \n+    if not changed_files:\n+        print(\"No changed files found\")\n+        return None\n+    \n+    results = []\n+    \n+    for file_path in changed_files:\n+        if not file_path.endswith('.md'):\n+            print(f\"Skipping non-markdown file: {file_path}\")\n+            continue\n+        \n+        print(f\"Processing file: {file_path}\")\n+        full_path = os.path.join(os.getcwd(), file_path)\n+        \n+        if not os.path.exists(full_path):\n+            print(f\"Warning: File {full_path} does not exist, skipping\")\n+            continue\n+        \n+        file_diff = get_file_diff(pr_number, file_path)\n+        \n+        if not file_diff:\n+            print(f\"No diff found for file {file_path}\")\n+            continue\n+        \n+        print(f\"Found diff for file {file_path}, length: {len(file_diff)}\")\n+        \n+        line_numbers = extract_line_numbers_from_diff(file_diff)\n+        \n+        if not line_numbers:\n+            print(f\"No line numbers found in diff for file {file_path}\")\n+            continue\n+        \n+        print(f\"Affected line numbers: {line_numbers[:10]}...\")\n+        \n+        sections = extract_markdown_sections(full_path)\n+        print(f\"Found {len(sections)} sections in {file_path}\")\n+        \n+        if len(sections) > 0:\n+            print(f\"Section titles: {[s['title'] for s in sections.values()][:5]}\")\n+        \n+        affected_sections = set()\n+        for line_number in line_numbers:\n+            section = find_section_for_line(sections, line_number)\n+            if section:\n+                print(f\"Line {line_number} is in section: {section['title']}\")\n+                affected_sections.add((section['title'], section['path']))\n+        \n+        for section_title, section_path in affected_sections:\n+            results.append({\n+                'pr_number': pr_number,\n+                'pr_title': pr_details.get('title', ''),\n+                'pr_url': pr_details.get('url', ''),\n+                'file': file_path,\n+                'section': section_title,\n+                'section_path': section_path,\n+                'changes': [line for line in file_diff.split('\\n') if line.startswith('+') and not line.startswith('+++')]\n+            })\n+    \n+    print(f\"Found {len(results)} affected sections\")\n+    return results\n+\n+def analyze_all_prs(limit=100):\n+    \"\"\"Analyze all PRs and group them by the sections they modify.\"\"\"\n+    prs = get_pr_list(limit)\n+    \n+    all_results = []\n+    section_to_prs = defaultdict(list)\n+    \n+    for pr in prs:\n+        pr_number = pr['number']\n+        print(f\"Analyzing PR #{pr_number}: {pr['title']}\")\n+        \n+        results = analyze_pr(pr_number)\n+        if results:\n+            all_results.extend(results)\n+            \n+            for result in results:\n+                section_key = f\"{result['file']}:{result['section_path']}\"\n+                section_to_prs[section_key].append({\n+                    'pr_number': pr_number,\n+                    'pr_title': pr['title'],\n+                    'pr_url': pr['url']\n+                })\n+    \n+    return all_results, section_to_prs\n+\n+def generate_report(all_results, section_to_prs):\n+    \"\"\"Generate a report of the analysis results.\"\"\"\n+    report = []\n+    \n+    report.append(\"# PRs Grouped by Section\\n\")\n+    \n+    for section_key, prs in sorted(section_to_prs.items()):\n+        if len(prs) > 1:  # Only show sections with multiple PRs\n+            file_path, section_path = section_key.split(':', 1)\n+            report.append(f\"## {file_path}\\n\")\n+            report.append(f\"### {section_path}\\n\")\n+            report.append(\"PRs modifying this section:\\n\")\n+            \n+            for pr in prs:\n+                report.append(f\"- PR #{pr['pr_number']}: {pr['pr_title']} ({pr['pr_url']})\\n\")\n+            \n+            report.append(\"\\n\")\n+    \n+    report.append(\"# Sections Modified by Each PR\\n\")\n+    \n+    pr_to_sections = defaultdict(list)\n+    for result in all_results:\n+        pr_key = f\"{result['pr_number']}:{result['pr_title']}\"\n+        section_info = {\n+            'file': result['file'],\n+            'section_path': result['section_path']\n+        }\n+        pr_to_sections[pr_key].append(section_info)\n+    \n+    for pr_key, sections in sorted(pr_to_sections.items()):\n+        pr_number, pr_title = pr_key.split(':', 1)\n+        report.append(f\"## PR #{pr_number}: {pr_title}\\n\")\n+        \n+        for section in sections:\n+            report.append(f\"- {section['file']}: {section['section_path']}\\n\")\n+        \n+        report.append(\"\\n\")\n+    \n+    return \"\".join(report)\n+\n+def main():\n+    \"\"\"Main function with improved command-line interface.\"\"\"\n+    parser = argparse.ArgumentParser(description='Analyze PRs in the policy repository and extract modified sections.')\n+    group = parser.add_mutually_exclusive_group(required=True)\n+    group.add_argument('--pr', type=str, help='PR number to analyze')\n+    group.add_argument('--all', action='store_true', help='Analyze all PRs')\n+    parser.add_argument('--limit', type=int, default=100, help='Limit the number of PRs to analyze (default: 100)')\n+    parser.add_argument('--output', type=str, help='Output file for the report (default: stdout)')\n+    parser.add_argument('--format', choices=['text', 'json'], default='text', help='Output format (default: text)')\n+    \n+    args = parser.parse_args()\n+    \n+    if args.pr:\n+        results = analyze_pr(args.pr)\n+        \n+        if results:\n+            if args.format == 'json':\n+                output = json.dumps(results, indent=2, ensure_ascii=False)\n+            else:\n+                output = f\"\\nAnalysis of PR #{args.pr}:\\n\"\n+                for result in results:\n+                    output += f\"\\nFile: {result['file']}\\n\"\n+                    output += f\"Section: {result['section_path']}\\n\"\n+                    output += \"Changes:\\n\"\n+                    for change in result['changes']:\n+                        output += f\"  {change}\\n\"\n+        else:\n+            output = f\"No markdown sections found in PR #{args.pr}\"\n+    else:  # --all\n+        all_results, section_to_prs = analyze_all_prs(args.limit)\n+        \n+        if args.format == 'json':\n+            output = json.dumps({\n+                'results': all_results,\n+                'section_to_prs': {k: v for k, v in section_to_prs.items()}\n+            }, indent=2, ensure_ascii=False)\n+        else:\n+            output = generate_report(all_results, section_to_prs)\n+    \n+    if args.output:\n+        with open(args.output, 'w', encoding='utf-8') as f:\n+            f.write(output)\n+        print(f\"Report written to {args.output}\")\n+    else:\n+        print(output)\n+\n+if __name__ == \"__main__\":\n+    main()"
    },
    {
      "sha": "a5f4987ed94c6ced36ef9c83baafad95ca4fe49e",
      "filename": "pr_section_analyzer_v2.py",
      "status": "added",
      "additions": 381,
      "deletions": 0,
      "changes": 381,
      "blob_url": "https://github.com/team-mirai/policy/blob/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer_v2.py",
      "raw_url": "https://github.com/team-mirai/policy/raw/1b8ba495519c7d26f21fdfdd5c0f8375e3031a35/pr_section_analyzer_v2.py",
      "contents_url": "https://api.github.com/repos/team-mirai/policy/contents/pr_section_analyzer_v2.py?ref=1b8ba495519c7d26f21fdfdd5c0f8375e3031a35",
      "patch": "@@ -0,0 +1,381 @@\n+#!/usr/bin/env python3\n+\"\"\"\n+Script to analyze PRs in the policy repository and extract the specific sections \n+of the manifest that are being modified. This script can identify which PRs modify\n+the same sections of the manifest.\n+\"\"\"\n+\n+import json\n+import os\n+import re\n+import subprocess\n+import sys\n+import argparse\n+from collections import defaultdict\n+\n+def run_command(command):\n+    \"\"\"Run a shell command and return the output.\"\"\"\n+    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n+    if result.returncode != 0:\n+        print(f\"Error running command: {command}\")\n+        print(f\"Error: {result.stderr}\")\n+        return \"\"\n+    return result.stdout.strip()\n+\n+def get_pr_list(limit=100):\n+    \"\"\"Get a list of PRs from the repository.\"\"\"\n+    command = f\"gh pr list --limit {limit} --json number,title,headRefName,state,url\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR list: {output}\")\n+        return []\n+\n+def get_pr_details(pr_number):\n+    \"\"\"Get detailed information about a PR.\"\"\"\n+    command = f\"gh pr view {pr_number} --json number,title,headRefName,state,url,body\"\n+    output = run_command(command)\n+    try:\n+        return json.loads(output)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR details: {output}\")\n+        return {}\n+\n+def get_pr_files(pr_number):\n+    \"\"\"Get the list of files changed in a PR using GitHub API.\"\"\"\n+    command = f\"gh pr view {pr_number} --json files\"\n+    output = run_command(command)\n+    try:\n+        data = json.loads(output)\n+        files = [file.get('path') for file in data.get('files', [])]\n+        print(f\"Changed files from GitHub API: {files}\")\n+        return files\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON from PR files: {output}\")\n+        return []\n+\n+def get_file_diff(pr_number, file_path):\n+    \"\"\"Get the diff for a specific file in a PR.\"\"\"\n+    command = f\"gh pr view {pr_number} --json headRefName\"\n+    output = run_command(command)\n+    try:\n+        data = json.loads(output)\n+        branch_name = data.get('headRefName')\n+        if not branch_name:\n+            print(f\"Could not get branch name for PR #{pr_number}\")\n+            return \"\"\n+        \n+        fetch_cmd = f\"git fetch origin {branch_name}\"\n+        run_command(fetch_cmd)\n+        \n+        diff_cmd = f\"git diff main..origin/{branch_name} -- '{file_path}'\"\n+        return run_command(diff_cmd)\n+    except json.JSONDecodeError:\n+        print(f\"Error parsing JSON for PR branch: {output}\")\n+        return \"\"\n+\n+def extract_markdown_sections(file_path):\n+    \"\"\"Extract the markdown sections from a file with improved Japanese section handling.\"\"\"\n+    if not os.path.exists(file_path):\n+        print(f\"File not found: {file_path}\")\n+        return {}\n+    \n+    with open(file_path, 'r', encoding='utf-8') as f:\n+        content = f.read()\n+    \n+    print(f\"Extracting sections from file with {len(content.split('\\n'))} lines\")\n+    \n+    headings = []\n+    section_content = {}\n+    current_section_start = None\n+    \n+    for i, line in enumerate(content.split('\\n')):\n+        heading_match = re.match(r'^(#+)\\s+(.+)$', line)\n+        \n+        jp_section_match = None\n+        if not heading_match:\n+            jp_section_match = re.match(r'^([ï¼-ï¼™]+ï¼‰|\\d+ï¼‰)\\s*(.+)$', line)\n+        \n+        jp_policy_match = None\n+        if not heading_match and not jp_section_match:\n+            jp_policy_match = re.match(r'^###\\s+(ç¾çŠ¶èªè­˜ãƒ»èª²é¡Œåˆ†æ|æ”¿ç­–æ¦‚è¦)$', line)\n+        \n+        if heading_match:\n+            level = len(heading_match.group(1))\n+            title = heading_match.group(2).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found heading at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_section_match:\n+            level = 3\n+            title = jp_section_match.group(0).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+            \n+        elif jp_policy_match:\n+            level = 3\n+            title = jp_policy_match.group(1).strip()\n+            headings.append((i+1, level, title))\n+            print(f\"Found JP policy section at line {i+1}: {title} (level {level})\")\n+            \n+            if current_section_start is not None:\n+                section_content[current_section_start] = (current_section_start, i)\n+            current_section_start = i+1\n+    \n+    if current_section_start is not None and len(content.split('\\n')) > 0:\n+        section_content[current_section_start] = (current_section_start, len(content.split('\\n')))\n+    \n+    sections = {}\n+    for i, (line_num, level, title) in enumerate(headings):\n+        parent_sections = []\n+        for prev_line, prev_level, prev_title in reversed(headings[:i]):\n+            if prev_level < level:\n+                parent_sections.insert(0, prev_title)\n+                if prev_level == 1:  # Stop at top level\n+                    break\n+        \n+        section_path = \" > \".join(parent_sections + [title])\n+        \n+        content_range = section_content.get(line_num, (line_num, line_num))\n+        \n+        sections[line_num] = {\n+            \"level\": level,\n+            \"title\": title,\n+            \"path\": section_path,\n+            \"parent_sections\": parent_sections,\n+            \"start_line\": content_range[0],\n+            \"end_line\": content_range[1]\n+        }\n+    \n+    return sections\n+\n+def find_section_for_line(sections, line_number):\n+    \"\"\"Find the section that contains a specific line.\"\"\"\n+    for section_line, section_info in sections.items():\n+        if section_info[\"start_line\"] <= line_number <= section_info[\"end_line\"]:\n+            return section_info\n+    \n+    section_lines = sorted(sections.keys())\n+    for i, section_line in enumerate(section_lines):\n+        if section_line > line_number:\n+            if i > 0:\n+                return sections[section_lines[i-1]]\n+            break\n+    \n+    if section_lines:\n+        return sections[section_lines[-1]]\n+    \n+    return None\n+\n+def extract_line_numbers_from_diff(diff_text):\n+    \"\"\"Extract line numbers from a diff.\"\"\"\n+    line_numbers = []\n+    current_line = None\n+    \n+    for line in diff_text.split('\\n'):\n+        if line.startswith('@@'):\n+            match = re.search(r'@@ -\\d+(?:,\\d+)? \\+(\\d+)(?:,(\\d+))? @@', line)\n+            if match:\n+                current_line = int(match.group(1))\n+                line_count = int(match.group(2)) if match.group(2) else 1\n+        elif current_line is not None:\n+            if line.startswith('+') and not line.startswith('+++'):\n+                line_numbers.append(current_line)\n+                current_line += 1\n+            elif line.startswith('-'):\n+                pass\n+            else:\n+                current_line += 1\n+    \n+    return line_numbers\n+\n+def analyze_pr(pr_number):\n+    \"\"\"Analyze a PR and extract the sections being modified.\"\"\"\n+    pr_details = get_pr_details(pr_number)\n+    changed_files = get_pr_files(pr_number)\n+    \n+    print(f\"Analyzing PR #{pr_number}\")\n+    \n+    if not changed_files:\n+        print(\"No changed files found\")\n+        return None\n+    \n+    results = []\n+    \n+    for file_path in changed_files:\n+        if not file_path.endswith('.md'):\n+            print(f\"Skipping non-markdown file: {file_path}\")\n+            continue\n+        \n+        print(f\"Processing file: {file_path}\")\n+        full_path = os.path.join(os.getcwd(), file_path)\n+        \n+        if not os.path.exists(full_path):\n+            print(f\"Warning: File {full_path} does not exist, skipping\")\n+            continue\n+        \n+        file_diff = get_file_diff(pr_number, file_path)\n+        \n+        if not file_diff:\n+            print(f\"No diff found for file {file_path}\")\n+            continue\n+        \n+        print(f\"Found diff for file {file_path}, length: {len(file_diff)}\")\n+        \n+        line_numbers = extract_line_numbers_from_diff(file_diff)\n+        \n+        if not line_numbers:\n+            print(f\"No line numbers found in diff for file {file_path}\")\n+            continue\n+        \n+        print(f\"Affected line numbers: {line_numbers[:10]}...\")\n+        \n+        sections = extract_markdown_sections(full_path)\n+        print(f\"Found {len(sections)} sections in {file_path}\")\n+        \n+        if len(sections) > 0:\n+            print(f\"Section titles: {[s['title'] for s in sections.values()][:5]}\")\n+        \n+        affected_sections = set()\n+        for line_number in line_numbers:\n+            section = find_section_for_line(sections, line_number)\n+            if section:\n+                print(f\"Line {line_number} is in section: {section['title']}\")\n+                affected_sections.add((section['title'], section['path']))\n+        \n+        for section_title, section_path in affected_sections:\n+            results.append({\n+                'pr_number': pr_number,\n+                'pr_title': pr_details.get('title', ''),\n+                'pr_url': pr_details.get('url', ''),\n+                'file': file_path,\n+                'section': section_title,\n+                'section_path': section_path,\n+                'changes': [line for line in file_diff.split('\\n') if line.startswith('+') and not line.startswith('+++')]\n+            })\n+    \n+    print(f\"Found {len(results)} affected sections\")\n+    return results\n+\n+def analyze_all_prs(limit=100):\n+    \"\"\"Analyze all PRs and group them by the sections they modify.\"\"\"\n+    prs = get_pr_list(limit)\n+    \n+    all_results = []\n+    section_to_prs = defaultdict(list)\n+    \n+    for pr in prs:\n+        pr_number = pr['number']\n+        print(f\"Analyzing PR #{pr_number}: {pr['title']}\")\n+        \n+        results = analyze_pr(pr_number)\n+        if results:\n+            all_results.extend(results)\n+            \n+            for result in results:\n+                section_key = f\"{result['file']}:{result['section_path']}\"\n+                section_to_prs[section_key].append({\n+                    'pr_number': pr_number,\n+                    'pr_title': pr['title'],\n+                    'pr_url': pr['url']\n+                })\n+    \n+    return all_results, section_to_prs\n+\n+def generate_report(all_results, section_to_prs):\n+    \"\"\"Generate a report of the analysis results.\"\"\"\n+    report = []\n+    \n+    report.append(\"# PRs Grouped by Section\\n\")\n+    \n+    for section_key, prs in sorted(section_to_prs.items()):\n+        if len(prs) > 1:  # Only show sections with multiple PRs\n+            file_path, section_path = section_key.split(':', 1)\n+            report.append(f\"## {file_path}\\n\")\n+            report.append(f\"### {section_path}\\n\")\n+            report.append(\"PRs modifying this section:\\n\")\n+            \n+            for pr in prs:\n+                report.append(f\"- PR #{pr['pr_number']}: {pr['pr_title']} ({pr['pr_url']})\\n\")\n+            \n+            report.append(\"\\n\")\n+    \n+    report.append(\"# Sections Modified by Each PR\\n\")\n+    \n+    pr_to_sections = defaultdict(list)\n+    for result in all_results:\n+        pr_key = f\"{result['pr_number']}:{result['pr_title']}\"\n+        section_info = {\n+            'file': result['file'],\n+            'section_path': result['section_path']\n+        }\n+        pr_to_sections[pr_key].append(section_info)\n+    \n+    for pr_key, sections in sorted(pr_to_sections.items()):\n+        pr_number, pr_title = pr_key.split(':', 1)\n+        report.append(f\"## PR #{pr_number}: {pr_title}\\n\")\n+        \n+        for section in sections:\n+            report.append(f\"- {section['file']}: {section['section_path']}\\n\")\n+        \n+        report.append(\"\\n\")\n+    \n+    return \"\".join(report)\n+\n+def main():\n+    \"\"\"Main function with improved command-line interface.\"\"\"\n+    parser = argparse.ArgumentParser(description='Analyze PRs in the policy repository and extract modified sections.')\n+    group = parser.add_mutually_exclusive_group(required=True)\n+    group.add_argument('--pr', type=str, help='PR number to analyze')\n+    group.add_argument('--all', action='store_true', help='Analyze all PRs')\n+    parser.add_argument('--limit', type=int, default=100, help='Limit the number of PRs to analyze (default: 100)')\n+    parser.add_argument('--output', type=str, help='Output file for the report (default: stdout)')\n+    parser.add_argument('--format', choices=['text', 'json'], default='text', help='Output format (default: text)')\n+    \n+    args = parser.parse_args()\n+    \n+    if args.pr:\n+        results = analyze_pr(args.pr)\n+        \n+        if results:\n+            if args.format == 'json':\n+                output = json.dumps(results, indent=2, ensure_ascii=False)\n+            else:\n+                output = f\"\\nAnalysis of PR #{args.pr}:\\n\"\n+                for result in results:\n+                    output += f\"\\nFile: {result['file']}\\n\"\n+                    output += f\"Section: {result['section_path']}\\n\"\n+                    output += \"Changes:\\n\"\n+                    for change in result['changes']:\n+                        output += f\"  {change}\\n\"\n+        else:\n+            output = f\"No markdown sections found in PR #{args.pr}\"\n+    else:  # --all\n+        all_results, section_to_prs = analyze_all_prs(args.limit)\n+        \n+        if args.format == 'json':\n+            output = json.dumps({\n+                'results': all_results,\n+                'section_to_prs': {k: v for k, v in section_to_prs.items()}\n+            }, indent=2, ensure_ascii=False)\n+        else:\n+            output = generate_report(all_results, section_to_prs)\n+    \n+    if args.output:\n+        with open(args.output, 'w', encoding='utf-8') as f:\n+            f.write(output)\n+        print(f\"Report written to {args.output}\")\n+    else:\n+        print(output)\n+\n+if __name__ == \"__main__\":\n+    main()"
    }
  ]
}